time=2025-11-09T23:11:09.031-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:11:09.032-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:11:09.032-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:11:09.032-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-09T23:11:09.032-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:11:09.032-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34983"
time=2025-11-09T23:11:09.213-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32845"
time=2025-11-09T23:11:09.394-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40859"
time=2025-11-09T23:11:09.394-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45713"
time=2025-11-09T23:11:09.602-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:11:14 | 200 |      33.904µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:11:14 | 404 |      144.19µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:11:14 | 200 |      25.688µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:11:14.551-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:11:17.768-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:11:18.983-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:11:20.212-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:11:21.434-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:11:22.654-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:11:23 | 200 |  9.837836447s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:06 | 200 |     374.082µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:24:26.953-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33733"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11461/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:24:28.510-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11461/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 33623"
time=2025-11-09T23:24:28.511-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="101.9 GiB" free_swap="0 B"
time=2025-11-09T23:24:28.511-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11461/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:24:28.512-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[20.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:24:28.526-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:24:29.515-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:24:29.516-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:33623"
time=2025-11-09T23:24:29.526-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:24:29.527-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:29.527-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 21635072000 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 20632 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11461/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:24:34.307-07:00 level=INFO source=server.go:1289 msg="llama runner started in 5.80 seconds"
time=2025-11-09T23:24:34.308-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:24:34.308-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:34.308-07:00 level=INFO source=server.go:1289 msg="llama runner started in 5.80 seconds"
[GIN] 2025/11/09 - 23:24:34 | 200 |  793.012086ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:34 | 200 |  7.751863114s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:35 | 200 |  490.636376ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:35 | 200 |  1.001938244s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:35 | 200 |  965.155748ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  788.649264ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  884.067506ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  883.417347ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  584.606974ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |  419.902994ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |  132.519074ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |   899.93174ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |  669.577461ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |    96.22771ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |  121.000886ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |  430.896565ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |  371.909511ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:44 | 200 |  175.469545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:44 | 200 |  156.030255ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:45 | 200 |   140.54247ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |   129.99363ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  145.521443ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:50 | 200 |  462.533881ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:51 | 200 |  764.653901ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:52 | 200 |  364.497177ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:52 | 200 |  247.802625ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:53 | 200 |  229.521458ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:58 | 200 |  469.204008ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:59 | 200 |  366.783956ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:26 | 200 |  438.801621ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:26 | 200 |  176.767476ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:27 | 200 |  234.328043ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:27 | 200 |  232.519941ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:27 | 200 |    151.7082ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:28 | 200 |  155.978362ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:35 | 200 |  109.112243ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:36 | 200 |  154.484973ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:49 | 200 |  200.936567ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:50 | 200 |  143.446759ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:50 | 200 |  445.276665ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:58 | 200 |  469.404237ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:59 | 200 |  192.354671ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:59 | 200 |   75.606764ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:00 | 200 |     131.629ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:01 | 200 |  403.194729ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:03 | 200 |  217.216723ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:03 | 200 |   69.110605ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:03 | 200 |   68.439977ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:28 | 200 |  155.087299ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:29 | 200 |   81.387137ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:29 | 200 |   87.216365ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:30 | 200 |  240.328088ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:30 | 200 |   86.313851ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:30 | 200 |  118.409818ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:34 | 200 |  262.687878ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:35 | 200 |  511.965393ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |  673.833169ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:40 | 200 |  151.084463ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:40 | 200 |  118.728729ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:47 | 200 |  736.375782ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:48 | 200 |  102.024597ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:48 | 200 |  113.232879ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:53 | 200 |  433.358835ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:53 | 200 |   69.723245ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:53 | 200 |   67.687084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:02 | 200 |  161.480993ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:02 | 200 |  139.276084ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:05 | 200 |  157.278267ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:06 | 200 |   76.137545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:06 | 200 |  101.978395ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:08 | 200 |  150.169799ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:08 | 200 |  172.776071ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:21 | 200 |  821.246957ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:22 | 200 |   422.39272ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:25 | 200 |   534.78136ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:26 | 200 |  1.340947968s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:27 | 200 |  310.316756ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:29 | 200 |  247.337802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:29 | 200 |  117.741352ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:30 | 200 |  364.169137ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:30 | 200 |  150.264656ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:31 | 200 |  850.846976ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 |  144.214195ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 |  249.903802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:38 | 200 |   99.570917ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  159.279907ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:40 | 200 |  149.735326ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  165.613655ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  166.317104ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:28 | 200 |  533.898046ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:29 | 200 |  749.313135ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:30 | 200 |  341.171881ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:32 | 200 |  366.577423ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:33 | 200 |  445.630896ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:33 | 200 |  201.181287ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:34 | 200 |  398.063425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:35 | 200 |  1.366257427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |  2.458683637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  828.376632ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  1.244137907s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  1.149669312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  880.130975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |   264.42377ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  590.100648ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  626.881597ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  494.940134ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:40 | 200 |  320.699797ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |  1.035972677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |  678.752149ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  1.050972571s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  925.008256ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  926.527296ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  535.614074ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  515.837979ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |  510.239543ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |    601.5005ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:45 | 200 |  481.391052ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |   124.98502ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |  688.996731ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |  728.826894ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:52 | 200 |   341.28467ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:52 | 200 |  313.324014ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:10 | 200 |  529.699957ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  469.808201ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  399.967312ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  174.856837ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  523.154894ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  559.994326ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  380.777226ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  606.630976ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  725.335086ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  438.469656ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  229.384878ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |   450.76288ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:20 | 200 |   97.139947ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:20 | 200 |  107.365269ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:35 | 200 |  124.129888ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:35 | 200 |  177.402132ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:42 | 200 |  289.577557ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:42 | 200 |  347.439338ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:42 | 200 |  207.309349ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:36:49.651-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:36:49.651-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:36:49.651-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:36:49.651-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-09T23:36:49.651-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:36:49.652-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46353"
time=2025-11-09T23:36:49.842-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34023"
time=2025-11-09T23:36:50.045-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42777"
time=2025-11-09T23:36:50.045-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34655"
time=2025-11-09T23:36:50.524-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:36:54 | 200 |      30.337µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:36:54 | 404 |     350.619µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:36:54 | 200 |        21.4µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:36:55.167-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:36:59.386-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:37:00.610-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:37:01.840-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:37:03.062-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:37:04 | 200 |   9.62650152s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     414.638µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:49:28.790-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:49:28.791-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:49:28.791-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:49:28.791-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-09T23:49:28.791-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:49:28.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35137"
time=2025-11-09T23:49:28.992-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36061"
time=2025-11-09T23:49:29.439-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34181"
time=2025-11-09T23:49:29.440-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42545"
time=2025-11-09T23:49:29.654-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:49:33 | 200 |      51.196µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:49:33 | 200 |   44.504592ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     533.953µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:57:35.956-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:57:35.956-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:57:35.957-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:57:35.957-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-09T23:57:35.957-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:57:35.958-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40691"
time=2025-11-09T23:57:36.145-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42191"
time=2025-11-09T23:57:36.339-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40137"
time=2025-11-09T23:57:36.339-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41977"
time=2025-11-09T23:57:36.558-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:57:40 | 200 |      65.683µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:57:41 | 200 |   46.996273ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     629.533µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:04:55.146-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:04:55.146-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:04:55.147-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:04:55.147-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:04:55.147-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:04:55.148-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38351"
time=2025-11-10T00:04:55.348-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38333"
time=2025-11-10T00:04:55.543-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43119"
time=2025-11-10T00:04:55.543-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35021"
time=2025-11-10T00:04:56.037-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:05:00 | 200 |      76.554µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:05:00 | 200 |   47.542432ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:51 | 200 |     274.977µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:11:09.940-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:11:09.941-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:11:09.941-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:11:09.941-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:11:09.941-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:11:09.942-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40027"
time=2025-11-10T00:11:10.145-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45809"
time=2025-11-10T00:11:10.336-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39533"
time=2025-11-10T00:11:10.336-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45131"
time=2025-11-10T00:11:10.561-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:11:14 | 200 |      35.667µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:11:14 | 200 |   47.456547ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:06 | 200 |     626.467µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:17:47.951-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:17:47.952-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:17:47.952-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:17:47.952-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:17:47.953-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:17:47.954-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39427"
time=2025-11-10T00:17:48.173-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37921"
time=2025-11-10T00:17:48.401-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35303"
time=2025-11-10T00:17:48.401-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39021"
time=2025-11-10T00:17:48.852-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:17:52 | 200 |      60.493µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:17:53 | 200 |   58.237554ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     435.968µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:26:11.242-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:26:11.242-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:26:11.243-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:26:11.243-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:26:11.243-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:26:11.244-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46725"
time=2025-11-10T00:26:11.624-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35475"
time=2025-11-10T00:26:11.840-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35461"
time=2025-11-10T00:26:11.840-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38081"
time=2025-11-10T00:26:12.085-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:26:16 | 200 |      51.697µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:26:16 | 200 |   58.332542ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     524.265µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:37:09.368-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:37:09.369-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:37:09.369-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:37:09.370-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:37:09.370-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:37:09.371-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37349"
time=2025-11-10T00:37:09.596-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39245"
time=2025-11-10T00:37:09.822-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43029"
time=2025-11-10T00:37:09.822-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40569"
time=2025-11-10T00:37:10.071-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:37:14 | 200 |      33.924µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:37:14 | 200 |   50.336608ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     333.426µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:47:43.674-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:47:43.675-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:47:43.675-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:47:43.675-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T00:47:43.675-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:47:43.676-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39169"
time=2025-11-10T00:47:43.873-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41411"
time=2025-11-10T00:47:44.061-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46373"
time=2025-11-10T00:47:44.061-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42801"
time=2025-11-10T00:47:44.563-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:47:48 | 200 |      29.215µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:47:48 | 200 |   47.134541ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     534.925µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:07:43.155-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:07:43.155-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:07:43.155-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:07:43.156-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T01:07:43.156-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:07:43.157-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35315"
time=2025-11-10T01:07:43.358-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43701"
time=2025-11-10T01:07:43.545-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36135"
time=2025-11-10T01:07:43.545-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36063"
time=2025-11-10T01:07:44.028-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:07:48 | 200 |      28.393µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:07:48 | 200 |   45.795865ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |      526.98µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:14:54.103-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:14:54.104-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:14:54.104-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:14:54.104-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T01:14:54.104-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:14:54.105-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41237"
time=2025-11-10T01:14:54.295-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38167"
time=2025-11-10T01:14:54.485-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38609"
time=2025-11-10T01:14:54.485-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46411"
time=2025-11-10T01:14:54.700-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:14:59 | 200 |      35.116µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:14:59 | 200 |   48.402493ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:50 | 200 |     299.453µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:39:35.216-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:39:35.217-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:39:35.217-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:39:35.217-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T01:39:35.217-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:39:35.217-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46845"
time=2025-11-10T01:39:35.419-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44887"
time=2025-11-10T01:39:35.643-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40209"
time=2025-11-10T01:39:35.643-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42771"
time=2025-11-10T01:39:35.862-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:39:37 | 200 |      27.531µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:39:37 | 200 |   44.334559ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:23 | 200 |     573.458µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:51.588-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46823"
time=2025-11-10T01:40:51.936-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:40:51.937-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11461/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 35819"
time=2025-11-10T01:40:51.938-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:40:51.938-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="104.2 GiB" free_swap="0 B"
time=2025-11-10T01:40:51.938-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="21.3 GiB" free="21.7 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:40:51.961-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:40:51.962-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:35819"
time=2025-11-10T01:40:51.974-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:52.025-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:40:52.612-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:40:53.766-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:40:53.941-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:40:53.942-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:40:53.942-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:40:54.443-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.51 seconds"
[GIN] 2025/11/10 - 01:40:55 | 200 |  3.613101474s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:40:56 | 200 |  974.547226ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:06 | 200 |  803.289467ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:07 | 200 |  1.243092907s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:45 | 200 |   4.53702301s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:20 | 200 |  6.032423131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:24 | 200 |  7.979444528s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:29 | 200 | 13.363532447s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:34 | 200 | 16.980389468s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:40 | 200 | 21.159404996s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:45 | 200 | 22.819183959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:51 | 200 | 21.813497886s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:57 | 200 | 25.925619837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:00 | 200 | 23.749858141s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:03 | 200 | 21.580257874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:09 | 200 |  21.13847568s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:15 | 200 | 20.501410504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:19 | 200 |  18.28528021s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:21 | 200 | 17.427911281s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:23 | 200 | 17.019080206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:26 | 200 |  13.31713138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:29 | 200 | 11.765107453s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:35 | 200 | 11.540946427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:38 | 200 |  8.825993333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:42 | 200 | 12.075364147s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:45 | 200 |  8.589317938s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:48 | 200 |  4.729055844s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:52 | 200 |  6.557684206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:58 | 200 |  9.383741145s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:04 | 200 | 13.833588319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:09 | 200 | 12.975154824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:13 | 200 | 10.724289039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:17 | 200 |  8.939000385s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:21 | 200 | 12.002853652s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:28 | 200 | 14.717063842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:32 | 200 | 16.988306004s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:35 | 200 |  17.86206811s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:39 | 200 | 16.626512214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 | 12.631639386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:46 | 200 | 13.707400476s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:49 | 200 | 14.592012615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:55 | 200 | 16.555820664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:59 | 200 | 17.429557114s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:03 | 200 | 22.026555617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:08 | 200 | 21.050622071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:12 | 200 | 18.487938474s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:18 | 200 | 18.999927351s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:22 | 200 | 21.933833995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:26 | 200 | 21.347156317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:33 | 200 | 24.475271993s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:37 | 200 | 24.529718809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:42 | 200 | 23.057223144s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:46 | 200 | 23.318298699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:51 | 200 | 23.733514022s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:54 | 200 | 21.293830097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:59 | 200 | 21.680991645s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:03 | 200 | 20.987633589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:06 | 200 | 17.752286335s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:08 | 200 | 13.285012432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:11 | 200 | 16.725290409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:15 | 200 | 13.956681883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:19 | 200 |  11.40882535s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:21 | 200 |  8.710993226s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:26 | 200 |  11.56254509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:32 | 200 | 12.656517854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:38 | 200 | 16.255821325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:42 | 200 | 15.671411866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:49 | 200 | 22.049030338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:52 | 200 | 18.013878731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:56 | 200 | 15.920985342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:00 | 200 | 13.052013555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:02 | 200 | 10.062035765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:05 | 200 |  8.614112864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:31 | 200 |  6.906266311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:36 | 200 | 11.553717563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:42 | 200 | 17.929697984s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:45 | 200 | 19.879385196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:48 | 200 | 23.129869495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:52 | 200 | 20.625182477s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:57 | 200 | 20.515386628s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:02 | 200 | 20.340093861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:08 | 200 | 22.107143988s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:12 | 200 |  23.55177416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:17 | 200 | 24.157068021s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:21 | 200 | 23.498765549s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:25 | 200 | 22.534823697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:30 | 200 | 21.759899384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:36 | 200 | 23.293230477s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:39 | 200 | 21.997637464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:43 | 200 | 21.151253139s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:46 | 200 | 20.572691691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:52 | 200 | 22.288466055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:57 | 200 | 19.968039307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:01 | 200 | 21.343045076s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:05 | 200 | 21.398431086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:10 | 200 | 23.135982942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:15 | 200 | 22.244956005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:21 | 200 | 23.596618509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:27 | 200 | 25.606941293s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:31 | 200 | 26.191791919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:36 | 200 | 25.868161236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:40 | 200 | 24.257072595s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:44 | 200 | 22.517682776s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:46 | 200 | 19.122852894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:52 | 200 | 20.120653422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:55 | 200 | 18.607249454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:59 | 200 | 18.444842785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:02 | 200 |  15.55228991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:05 | 200 | 18.760008215s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:09 | 200 | 17.007155333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:14 | 200 | 15.118996634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:18 | 200 | 16.355826978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:23 | 200 | 17.763526349s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:27 | 200 | 21.622037853s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:33 | 200 | 21.819089267s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:37 | 200 | 18.199949589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:43 | 200 | 18.072141197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:48 | 200 | 18.253612525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 |  16.62636333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:58 | 200 |  15.38193375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:03 | 200 | 13.600819225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:07 | 200 | 12.125308377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:10 | 200 |  9.942214994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:17 | 200 | 13.220458142s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:22 | 200 | 14.517695621s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:27 | 200 | 13.713658419s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:31 | 200 | 11.529675491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:40 | 200 | 14.389584692s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:45 | 200 | 12.706506254s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:50 | 200 | 11.087190457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:52 | 200 |  6.729041794s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:59 | 200 |  6.336842915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:05 | 200 |  6.292291444s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:11 | 200 |  5.964085346s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:20 | 200 |  7.505287441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:24 | 200 |   8.14608659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:28 | 200 |  9.060501064s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:31 | 200 |  6.497807104s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:37 | 200 |  10.96603957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:41 | 200 | 12.685642079s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:43 | 200 |  9.014550109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:49 | 200 |  8.137295398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:53 | 200 |  9.994275585s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:58 | 200 | 12.211194817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:04 | 200 | 15.170192027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:10 | 200 | 16.526113614s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:17 | 200 | 15.183695515s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:26 | 200 | 17.357086365s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:31 | 200 | 14.714582169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:34 | 200 | 12.138582041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:37 | 200 |  8.888747903s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:43 | 200 |  7.176516165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:48 | 200 | 11.001615879s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:52 | 200 |  8.967090318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:56 | 200 |  6.139644914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:01 | 200 |  4.419618345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:07 | 200 |  5.017533082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:10 | 200 |  5.340526804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:17 | 200 |  5.632644733s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:23 | 200 |  6.018245039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:27 | 200 |  9.020283431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:31 | 200 |  4.836259627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:37 | 200 |  9.327966462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:40 | 200 |  6.067816308s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:44 | 200 |   4.25221775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:52 | 200 |  4.877247538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:59 | 200 |  4.919450844s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:05 | 200 |  5.410951543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:13 | 200 |  8.367811804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:17 | 200 | 11.061024425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:19 | 200 |  5.357501584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:22 | 200 |  8.202223103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:26 | 200 |  9.015549534s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:32 | 200 | 10.877459169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:35 | 200 | 12.647563586s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:37 | 200 |  8.901074841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:41 | 200 |   6.06148886s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:44 | 200 |  3.081495473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:55 | 200 |  6.838407421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:00 | 200 |   6.86996174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:06 | 200 |  6.708090307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:10 | 200 |   5.51761342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:18 | 200 |  6.665239613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:24 | 200 | 13.390698207s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:32 | 200 | 15.080228657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:36 | 200 | 12.958104662s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:42 | 200 | 12.440461723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:45 | 200 |  8.860939386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:48 | 200 | 11.094505527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:51 | 200 |  9.064176642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:55 | 200 | 12.605839227s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:00 | 200 | 10.646411376s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:05 | 200 | 10.375347738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:09 | 200 |  7.801588258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:16 | 200 |  6.868975779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:18 | 200 |  8.280476571s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:22 | 200 |  7.011823951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:27 | 200 |  6.220785296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:34 | 200 | 11.979459508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:40 | 200 | 12.835997808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:45 | 200 | 11.266614861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:51 | 200 | 16.732493724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:54 | 200 | 14.874159565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:00 | 200 | 13.866511374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:06 | 200 | 14.153707173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:12 | 200 | 12.651020163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:16 | 200 | 10.636473183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:21 | 200 | 10.304066372s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:26 | 200 | 13.503175246s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:32 | 200 |  15.38228066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:37 | 200 | 19.624116162s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:43 | 200 | 18.717020214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:46 | 200 | 15.320218953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:52 | 200 | 14.132506674s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:58 | 200 | 14.477916361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:03 | 200 | 16.721937613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:07 | 200 | 16.947742196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:14 | 200 | 17.669435107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:18 | 200 | 14.452069387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:21 | 200 |  12.07492258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:23 | 200 |  7.565225458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:27 | 200 |  8.119449637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:32 | 200 | 10.216334324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:37 | 200 | 15.185139439s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:41 | 200 | 13.028364495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:46 | 200 | 11.830827937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:51 | 200 |  9.321480006s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:54 | 200 |  7.271852901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:59 | 200 | 10.481731783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:05 | 200 | 10.222430552s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:11 | 200 | 15.312920837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:13 | 200 | 12.669142573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:16 | 200 | 10.096122013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:21 | 200 |  7.901349471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:26 | 200 |  8.135632784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:31 | 200 |  6.894760136s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:38 | 200 |  7.602770802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:44 | 200 | 12.556642768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:51 | 200 | 12.734050462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:57 | 200 | 13.458638071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:01 | 200 | 11.013605544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:05 | 200 |  8.972872949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:12 | 200 |  8.991501743s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:17 | 200 | 10.912462833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:22 | 200 | 12.901758645s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:25 | 200 | 10.533700126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:30 | 200 |  7.873738561s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:34 | 200 |  8.841361779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:38 | 200 |  9.781575781s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:44 | 200 |  9.288771533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:49 | 200 |  4.796365784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:28 | 200 | 10.331660529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:33 | 200 | 14.196067067s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:36 | 200 | 15.919697959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:39 | 200 | 19.326938858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:42 | 200 | 22.515961988s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:44 | 200 | 16.038658826s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:48 | 200 | 14.287476398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:52 | 200 | 15.636242401s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:57 | 200 | 17.291463117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:01 | 200 |  18.13452394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:04 | 200 | 16.331031105s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:07 | 200 | 18.732547835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:11 | 200 | 16.844623125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:20 | 200 | 18.727919407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:26 | 200 |  24.60200025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:30 | 200 |  24.15173999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:35 | 200 | 23.276995364s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:39 | 200 | 21.429636548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:44 | 200 | 18.568856749s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:49 | 200 | 17.137355786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:53 | 200 | 15.887416097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:58 | 200 | 14.072206715s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:04 | 200 | 14.443260627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:07 | 200 | 16.212271469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:11 | 200 |  17.70871406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:15 | 200 | 18.331319096s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:19 | 200 | 16.654469596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:23 | 200 |  14.46930217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:28 | 200 |  14.48324482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:31 | 200 |  15.99271201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:34 | 200 | 13.737263929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:36 | 200 | 13.439644976s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:40 | 200 | 13.442124813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:43 | 200 | 15.286437852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:48 | 200 | 15.956025375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:50 | 200 | 11.494394313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:52 | 200 |  6.850595899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:56 | 200 |  5.486305464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:00 | 200 |   4.32112046s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:09 | 200 |  6.538512246s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:14 | 200 |  5.358750889s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:21 | 200 |  7.017109363s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:25 | 200 |  9.896451716s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:29 | 200 |  7.611974072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:35 | 200 |  6.928518668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:41 | 200 |  7.367701442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:46 | 200 |  6.328747326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:53 | 200 |   6.36339566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:57 | 200 |  3.781460832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:02 | 200 |  4.849235412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:05 | 200 |  4.193346667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:11 | 200 |  6.120288681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:14 | 200 |  8.357137828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:20 | 200 |  6.301859108s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:26 | 200 | 10.610800573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:31 | 200 |  11.64765896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:37 | 200 | 17.111481845s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:42 | 200 |  16.02547691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:48 | 200 | 15.017986994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:54 | 200 | 15.144072919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:58 | 200 | 12.985617915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 500 |  8.322366983s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.693-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:08:00.243-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11461 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11461 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:00.244-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:00.244-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:00.244-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11461 (version 0.12.9)"
time=2025-11-10T02:08:00.244-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:00.245-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42701"
time=2025-11-10T02:08:00.440-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38777"
time=2025-11-10T02:08:00.904-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45553"
time=2025-11-10T02:08:00.904-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40463"
time=2025-11-10T02:08:01.128-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:02 | 200 |      33.002µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:02 | 200 |   46.850257ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:48 | 200 |     467.888µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:17.798-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40599"
time=2025-11-10T02:09:18.417-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:18.417-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11461/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 35455"
time=2025-11-10T02:09:18.418-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:18.418-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="103.7 GiB" free_swap="0 B"
time=2025-11-10T02:09:18.418-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="22.2 GiB" free="22.6 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:18.436-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:18.437-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:35455"
time=2025-11-10T02:09:18.440-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:18.480-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:18.960-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:19.683-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:19.926-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:19.926-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:19.925-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:19.925-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:19.925-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:19.925-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:19.927-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:20.432-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.01 seconds"
[GIN] 2025/11/10 - 02:09:21 | 200 |  3.380846314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:22 | 200 |  1.174123359s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:31 | 200 |  756.619675ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:33 | 200 |  1.142231641s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:16 | 200 |  2.481967715s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:19 | 200 |  2.175131319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:21 | 200 |  2.230167038s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:11 | 200 |  1.808104243s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:24 | 200 | 11.912897306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:33 | 200 | 20.042428122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:40 | 200 | 25.788177231s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:44 | 200 | 29.889674569s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:49 | 200 | 32.880623769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:53 | 200 | 36.929265563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:57 | 200 | 38.817952467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:03 | 200 | 43.346505214s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:06.040-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:06 | 500 | 45.111359622s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 46.278382281s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:07 | 500 | 45.097430012s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:09.143-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:12 | 200 | 45.713955837s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:12.292-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:12 | 500 | 45.047250088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:12 | 500 | 45.048666124s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:13.963-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:13 | 500 | 45.048290238s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:14.707-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.056960894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:20 | 200 | 46.403494075s |       127.0.0.1 | POST     "/api/generate"
Error #01: write tcp 127.0.0.1:11461->127.0.0.1:39424: write: broken pipe
[GIN] 2025/11/10 - 02:13:25 | 200 | 44.402596973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:29 | 200 | 44.306042892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:33 | 200 | 43.218179309s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:37 | 200 | 43.807083717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:41 | 200 | 43.558412563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:45 | 200 | 42.231507389s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:52 | 200 |  46.17041677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:58 | 200 | 32.424698338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:04 | 200 | 34.832790025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:08 | 200 |   35.2880198s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:10 | 200 | 33.189072823s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:13 | 200 |  31.93625648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:18 | 200 | 33.392044028s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.446-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 31.407251493s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.361611259s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.456-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 29.653029415s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.456-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 29.085164647s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  2.376891905s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 30.225235433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 15.090284708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 |  7.647700447s |       127.0.0.1 | POST     "/api/generate"
