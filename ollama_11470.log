time=2025-11-09T23:13:25.378-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:13:25.378-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:13:25.378-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:13:25.378-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-09T23:13:25.379-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:13:25.379-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45577"
time=2025-11-09T23:13:25.559-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33627"
time=2025-11-09T23:13:25.748-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34779"
time=2025-11-09T23:13:25.748-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36195"
time=2025-11-09T23:13:26.197-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:13:30 | 200 |      58.179µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:13:30 | 404 |     268.795µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:13:30 | 200 |      16.771µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:13:30.890-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:13:34.112-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:13:35.336-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:13:36.559-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:13:37.784-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:13:39.002-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:13:40 | 200 |  9.832830186s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |      234.68µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:24:26.799-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34613"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11470/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:24:28.449-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11470/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 41975"
time=2025-11-09T23:24:28.450-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="101.8 GiB" free_swap="0 B"
time=2025-11-09T23:24:28.450-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11470/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:24:28.450-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[20.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:24:28.463-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:24:28.968-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:24:28.968-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:41975"
time=2025-11-09T23:24:28.971-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:24:28.971-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:28.971-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 22333685760 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 21299 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11470/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:24:32.732-07:00 level=INFO source=server.go:1289 msg="llama runner started in 4.28 seconds"
time=2025-11-09T23:24:32.732-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:24:32.733-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:32.733-07:00 level=INFO source=server.go:1289 msg="llama runner started in 4.28 seconds"
[GIN] 2025/11/09 - 23:24:32 | 200 |  6.174418018s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:33 | 200 |  1.719204513s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:33 | 200 |  637.999554ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:33 | 200 |  134.141425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:33 | 200 |  125.841114ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:38 | 200 |  1.069447796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |  606.274689ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:42 | 200 |  198.608324ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:42 | 200 |    167.3389ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:42 | 200 |  174.400708ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:42 | 200 |  282.869204ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |  435.586353ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |   460.85478ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |  406.000839ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:43 | 200 |  141.631371ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:45 | 200 |  1.176469123s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:47 | 200 |  137.332609ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  160.486969ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  145.355762ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |   163.76549ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:52 | 200 |  400.056601ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:52 | 200 |  155.945111ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:52 | 200 |  186.252069ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:53 | 200 |  229.107191ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:53 | 200 |  138.463393ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:07 | 200 |  259.778076ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:07 | 200 |  219.393378ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:08 | 200 |  1.108121588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:08 | 200 |   1.05009966s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:10 | 200 |  1.387867975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:11 | 200 |  305.538884ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:11 | 200 |  348.742332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:12 | 200 |  331.280441ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  270.089997ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  766.422279ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:21 | 200 |   124.30913ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:21 | 200 |   96.528999ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:35 | 200 |  238.865563ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:36 | 200 |  224.055606ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:39 | 200 |   116.27353ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:39 | 200 |  117.970305ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:56 | 200 |  114.731103ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:56 | 200 |  130.868352ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:03 | 200 |  137.697016ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:03 | 200 |  102.266627ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:04 | 200 |  270.631008ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:35 | 200 |  219.722346ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:35 | 200 |  381.933687ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:35 | 200 |  242.436618ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |  566.386081ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:36 | 500 |          1m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 | 48.334169301s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 | 47.694824545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 |  1.018246321s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 |   540.70888ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:37 | 200 |   479.57199ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:38 | 200 |   711.59651ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:38 | 200 |  698.501176ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:38 | 200 |  588.543068ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  756.043822ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  776.825983ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  765.119134ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  756.157858ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  650.481534ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:39 | 200 |  461.623651ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:41 | 200 |  1.628552735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:41 | 200 |  1.656665953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  2.067710087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  456.516219ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  453.968247ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  143.466774ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  291.559227ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  324.076229ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  152.159376ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  275.186431ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  243.456406ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:42 | 200 |  380.388335ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  1.263584897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  1.457536874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |   1.66041391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  547.262288ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  352.827146ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  161.999733ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:45 | 200 |  691.058663ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:45 | 200 |  657.873295ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:45 | 200 |  628.394104ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:46 | 200 |   672.52921ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:46 | 200 |  880.775043ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:46 | 200 |    926.4301ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:46 | 200 |   76.144345ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:38:53.651-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:38:53.651-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:38:53.651-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:38:53.651-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-09T23:38:53.651-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:38:53.652-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36083"
time=2025-11-09T23:38:53.861-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38915"
time=2025-11-09T23:38:54.063-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33607"
time=2025-11-09T23:38:54.063-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38603"
time=2025-11-09T23:38:54.297-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:38:58 | 200 |      39.594µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:38:58 | 404 |       287.3µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:38:58 | 200 |      19.858µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:38:59.146-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:39:02.357-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:39:03.569-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:39:04.794-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:39:06.004-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:39:07 | 200 |  8.580356961s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     348.745µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:50:14.369-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:50:14.370-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:50:14.370-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:50:14.370-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-09T23:50:14.370-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:50:14.371-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42079"
time=2025-11-09T23:50:14.574-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33131"
time=2025-11-09T23:50:14.761-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33847"
time=2025-11-09T23:50:14.761-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43499"
time=2025-11-09T23:50:14.996-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:50:19 | 200 |      93.826µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:50:19 | 200 |   45.249158ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     526.669µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:58:21.541-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:58:21.542-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:58:21.542-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:58:21.542-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-09T23:58:21.542-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:58:21.543-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42275"
time=2025-11-09T23:58:21.748-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40395"
time=2025-11-09T23:58:21.948-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43561"
time=2025-11-09T23:58:21.948-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41745"
time=2025-11-09T23:58:22.169-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:58:26 | 200 |      66.174µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:58:26 | 200 |   45.153962ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     301.025µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:05:40.739-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:05:40.740-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:05:40.740-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:05:40.740-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:05:40.741-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:05:40.741-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41427"
time=2025-11-10T00:05:40.938-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40177"
time=2025-11-10T00:05:41.153-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34805"
time=2025-11-10T00:05:41.153-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45573"
time=2025-11-10T00:05:41.378-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:05:45 | 200 |      35.457µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:05:45 | 200 |   45.978273ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |      564.32µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:11:55.574-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:11:55.574-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:11:55.574-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:11:55.575-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:11:55.575-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:11:55.575-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39941"
time=2025-11-10T00:11:55.826-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40751"
time=2025-11-10T00:11:56.038-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42569"
time=2025-11-10T00:11:56.038-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37677"
time=2025-11-10T00:11:56.282-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:12:00 | 200 |      29.916µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:12:00 | 200 |   47.952379ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:06 | 200 |     346.772µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:18:33.641-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:18:33.641-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:18:33.641-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:18:33.641-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:18:33.642-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:18:33.642-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36357"
time=2025-11-10T00:18:33.868-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35013"
time=2025-11-10T00:18:34.083-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39383"
time=2025-11-10T00:18:34.083-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43307"
time=2025-11-10T00:18:34.335-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:18:38 | 200 |      43.892µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:18:38 | 200 |   51.543131ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     334.699µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:26:56.924-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:26:56.925-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:26:56.925-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:26:56.925-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:26:56.925-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:26:56.926-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39587"
time=2025-11-10T00:26:57.146-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40297"
time=2025-11-10T00:26:57.362-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33193"
time=2025-11-10T00:26:57.362-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44087"
time=2025-11-10T00:26:57.614-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:27:01 | 200 |      32.161µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:27:01 | 200 |   54.549415ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     390.614µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:37:54.964-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:37:54.965-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:37:54.965-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:37:54.965-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:37:54.965-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:37:54.966-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36837"
time=2025-11-10T00:37:55.421-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39419"
time=2025-11-10T00:37:55.629-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45375"
time=2025-11-10T00:37:55.629-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36187"
time=2025-11-10T00:37:55.866-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:37:59 | 200 |      55.014µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:38:00 | 200 |    48.53924ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     323.618µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:48:29.274-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:48:29.275-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:48:29.275-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:48:29.275-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T00:48:29.275-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:48:29.275-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43241"
time=2025-11-10T00:48:29.475-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45185"
time=2025-11-10T00:48:29.665-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33615"
time=2025-11-10T00:48:29.665-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40131"
time=2025-11-10T00:48:29.915-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:48:34 | 200 |       42.57µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:48:34 | 200 |   51.101163ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     351.991µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:08:28.790-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:08:28.790-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:08:28.790-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:08:28.791-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T01:08:28.791-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:08:28.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39229"
time=2025-11-10T01:08:29.019-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36631"
time=2025-11-10T01:08:29.238-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45751"
time=2025-11-10T01:08:29.238-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45049"
time=2025-11-10T01:08:29.491-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:08:33 | 200 |      44.123µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:08:33 | 200 |   53.758184ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |      564.38µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:15:39.724-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:15:39.724-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:15:39.724-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:15:39.724-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T01:15:39.725-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:15:39.725-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44197"
time=2025-11-10T01:15:39.933-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42229"
time=2025-11-10T01:15:40.134-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34757"
time=2025-11-10T01:15:40.134-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45833"
time=2025-11-10T01:15:40.368-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:15:44 | 200 |      40.877µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:15:44 | 200 |   49.236701ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:51 | 200 |     571.484µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:39:53.804-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:39:53.804-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:39:53.804-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:39:53.804-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T01:39:53.804-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:39:53.805-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43841"
time=2025-11-10T01:39:54.057-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39807"
time=2025-11-10T01:39:54.273-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33313"
time=2025-11-10T01:39:54.273-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34891"
time=2025-11-10T01:39:54.496-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:39:55 | 200 |      61.556µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:39:55 | 200 |   45.340258ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     345.369µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:14.827-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43345"
time=2025-11-10T01:41:15.256-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:15.256-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11470/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 39367"
time=2025-11-10T01:41:15.259-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:15.259-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="87.3 GiB" free_swap="0 B"
time=2025-11-10T01:41:15.259-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="14.2 GiB" free="14.7 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:15.279-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:15.280-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:39367"
time=2025-11-10T01:41:15.287-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:15.344-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:15.471-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:15.869-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:15.985-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:15.985-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:15.986-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:16.487-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.23 seconds"
[GIN] 2025/11/10 - 01:41:16 | 200 |  2.314774371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:17 | 200 |  363.726936ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:00 | 200 |  2.541755354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:03 | 200 |  2.437371403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:12 | 200 |  3.789456403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:22 | 200 | 10.004760779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:27 | 200 |  12.48677964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:33 | 200 | 17.148862772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:36 | 200 | 17.936204434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:41 | 200 |  22.00278101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:46 | 200 | 22.746761613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:51 | 200 | 24.250534714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:55 | 200 | 22.401233274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:00 | 200 | 23.753734057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:03 | 200 | 21.304859631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:06 | 200 | 20.186283524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:09 | 200 | 17.292065086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:12 | 200 | 16.736999176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:16 | 200 | 15.576216001s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:20 | 200 | 16.605525797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:26 | 200 | 19.496415623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:31 | 200 | 21.508448948s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:38 | 200 | 24.740055051s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:41 | 200 |  25.15075695s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:45 | 200 | 24.989846789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:48 | 200 | 21.242661433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:52 | 200 | 20.754702433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:56 | 200 | 17.759682643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:00 | 200 | 16.624216285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:05 | 200 | 17.006172833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:09 | 200 | 17.926978007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:13 | 200 | 16.219699593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:20 | 200 | 20.047792657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:23 | 200 | 19.662407024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:26 | 200 | 16.258137198s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:30 | 200 | 13.491457692s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:35 | 200 | 11.797417764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 |  17.10216139s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:46 | 200 | 19.063550547s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:49 | 200 | 19.425823204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:54 | 200 | 17.569291283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:57 | 200 | 14.140391427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:00 | 200 |  14.01167296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:04 | 200 | 15.292068589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:09 | 200 | 19.907282333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:12 | 200 |  17.86935174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:17 | 200 | 16.140183843s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:21 | 200 | 14.451650275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:26 | 200 | 13.456503042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:30 | 200 | 17.348682675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:35 | 200 | 15.528938708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:39 | 200 | 17.991250464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:45 | 200 | 19.123398973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:49 | 200 | 17.892852897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:53 | 200 | 21.591310682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:57 | 200 | 20.059071625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:00 | 200 |   16.3833915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:05 | 200 | 15.025057542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:13 | 200 | 16.787092671s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:18 | 200 | 15.981083876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:23 | 200 | 17.108487591s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:25 | 200 | 16.429583734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:29 | 200 | 14.599022231s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:33 | 200 | 12.828926013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:40 | 200 | 14.828150118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:45 | 200 | 17.683793362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:50 | 200 | 20.275822833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:54 | 200 | 20.089971808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:58 | 200 |  23.33138888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:01 | 200 |  19.74109171s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:34 | 200 | 10.075483366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:40 | 200 | 14.481409668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:44 | 200 | 19.710663719s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:48 | 200 | 22.588965894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:52 | 200 | 27.097743858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:56 | 200 | 22.440520974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:00 | 200 | 19.827016088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:05 | 200 | 21.348095524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:10 | 200 | 20.413095251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:14 | 200 | 21.797603355s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:19 | 200 | 22.237074324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:24 | 200 | 23.092721043s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:28 | 200 | 21.283680122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:31 | 200 | 20.984957874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:35 | 200 | 20.948344451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:37 | 200 | 18.430828305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:41 | 200 | 16.766507677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:45 | 200 | 17.380363415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:50 | 200 | 18.296188852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:54 | 200 | 16.875935544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:58 | 200 | 20.073233722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:01 | 200 | 16.592986056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:03 | 200 |  13.21806471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:09 | 200 | 13.198030297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:14 | 200 | 13.501225191s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:18 | 200 | 14.263291992s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:21 | 200 | 11.859076327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:25 | 200 |  8.627349611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:30 | 200 |  7.427571254s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:33 | 200 |  4.295896607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:43 | 200 |  8.606093396s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:48 | 200 |  7.409556314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:52 | 200 |  5.104710323s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:59 | 200 |  6.005032907s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:07 | 200 |  6.314413557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:12 | 200 |  6.082331969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:18 | 200 |  4.931220326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:22 | 200 |  3.856157839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:25 | 200 |  5.108147311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:32 | 200 |  5.932497401s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:37 | 200 |    4.7655489s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:44 | 200 |  6.530173094s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:49 | 200 | 11.286952422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 |  9.433018486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:58 | 200 |  8.584583251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:03 | 200 |  7.710123983s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:08 | 200 |  10.02140547s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:11 | 200 |  9.563534534s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:15 | 200 |  6.720988514s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:20 | 200 |  7.935155596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:24 | 200 |  9.102813206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:29 | 200 | 13.322356826s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:34 | 200 | 12.648693524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:39 | 200 | 11.467901832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:42 | 200 |  8.521342677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:47 | 200 |  7.858538414s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:49 | 200 |   10.0182777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:53 | 200 | 11.534310276s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:58 | 200 | 11.015461797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:03 | 200 | 13.664465809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:14 | 200 | 19.731326683s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:18 | 200 | 18.405566307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:25 | 200 | 18.533735527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:29 | 200 |  15.48121559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:34 | 200 | 14.343733486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:40 | 200 | 11.148125547s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:45 | 200 |  9.921625056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:49 | 200 |  8.770777945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:54 | 200 | 12.932006916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:00 | 200 | 11.498078443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:03 | 200 |   8.13552415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:11 | 200 | 10.926611981s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:14 | 200 | 11.214041031s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:20 | 200 | 10.291199964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:29 | 200 | 17.662421792s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:35 | 200 | 20.541695974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:39 | 200 | 22.631065671s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:43 | 200 | 19.893393391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:49 | 200 | 18.858829329s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:51 | 200 | 14.520582128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:59 | 200 | 18.768701849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:05 | 200 | 20.666654184s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:11 | 200 | 22.250058578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:16 | 200 | 23.775756588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:21 | 200 | 28.918691732s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:27 | 200 | 27.827494137s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:34 | 200 | 28.267288179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:41 | 200 | 29.529633995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:45 | 200 | 29.036875554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:48 | 200 | 26.460377939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:53 | 200 | 25.521035475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:56 | 200 |  21.86938342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:00 | 200 | 19.261528759s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:07 | 200 | 21.645189922s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:13 | 200 | 24.759466175s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:16 | 200 | 22.639183735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:20 | 200 | 23.302466684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:25 | 200 | 23.703257661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:29 | 200 | 21.433047874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:35 | 200 | 21.268031421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:39 | 200 | 23.457448589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:42 | 200 | 20.991251358s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:48 | 200 | 23.304179417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:52 | 200 | 22.144152975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:56 | 200 | 21.386400068s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:00 | 200 |  20.40874431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:06 | 200 |  23.58747527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:10 | 200 | 21.554847034s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:16 | 200 | 23.154536496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:19 | 200 | 22.427213214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:21 | 200 | 20.451659668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:24 | 200 | 17.658742264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:28 | 200 |  15.65310137s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:30 | 200 | 11.572707012s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:34 | 200 | 13.240313866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:41 | 200 | 16.652602261s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:45 | 200 | 14.313902968s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:50 | 200 | 13.399224577s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:55 | 200 | 11.081472513s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:02 | 200 | 12.261613433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:04 | 200 |  8.599847451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:06 | 200 |  4.103015873s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:08 | 200 |  3.306525237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:14 | 200 |  4.049632086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:19 | 200 |  4.785952863s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:24 | 200 |  7.956632356s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:26 | 200 |  4.683795873s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:34 | 200 |  7.839363949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:40 | 200 | 12.179860973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:45 | 200 | 10.815084329s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:49 | 200 |  8.217955754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:52 | 200 |  4.467845286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:00 | 200 |  6.293509325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:04 | 200 |  4.586039737s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:09 | 200 |  3.962517668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:11 | 200 |   5.59022559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:15 | 200 |  3.142509533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:24 | 200 |  9.382016296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:31 | 200 | 12.148570588s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:37 | 200 |  11.25199492s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:41 | 200 |  8.728926766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:47 | 200 |  8.808630644s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:51 | 200 |  8.926982374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:55 | 200 |  9.167598556s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:58 | 200 |  10.37985712s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:00 | 200 |  9.269303889s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:03 | 200 |   5.25703119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:08 | 200 |  9.304228707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:14 | 200 | 13.541298362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:21 | 200 | 16.295797905s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:25 | 200 | 15.116913265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:29 | 200 | 11.823762629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:33 | 200 |  9.469360487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:40 | 200 | 10.179591979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:43 | 200 | 13.095196714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:46 | 200 | 10.235412407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:49 | 200 |  9.259673059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:52 | 200 |  9.975763219s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:56 | 200 | 13.160547126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:02 | 200 | 15.523683403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:05 | 200 | 15.973313696s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:10 | 200 | 16.690554043s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:13 | 200 | 16.937083502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:15 | 200 | 13.724911047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:20 | 200 | 11.869723682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:24 | 200 | 10.288459638s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:29 | 200 | 14.169300632s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:35 | 200 | 14.720446119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:39 | 200 | 13.563038278s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:42 | 200 | 10.016955872s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:46 | 200 |  7.185659099s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:50 | 200 |   8.07861044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:55 | 200 | 10.269821079s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:01 | 200 |  9.823970092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:04 | 200 |  6.383669249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:10 | 200 |  5.350650365s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:16 | 200 |  5.925824799s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:21 | 200 |  5.450776581s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:25 | 200 |  8.513892809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:28 | 200 |  4.681261971s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:37 | 200 |   6.29014015s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:41 | 200 |  4.767900196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:47 | 200 |  5.509679024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:23 | 200 |  5.642001139s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:28 | 200 |  7.811260355s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:30 | 200 |  9.488794245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:34 | 200 | 14.245010557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:37 | 200 | 17.159898713s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:43 | 200 | 18.992426158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:48 | 200 | 20.410935027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:55 | 200 | 24.660997678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:58 | 200 | 24.247266959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:02 | 200 | 24.814150119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:07 | 200 | 22.736218548s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:11 | 200 | 21.212997686s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:16 | 200 | 20.506436127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:19 | 200 | 17.339957524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:23 | 200 | 15.691958851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:29 | 200 | 21.776249269s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:32 | 200 | 18.196169756s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:36 | 200 | 17.587585582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:40 | 200 | 19.423481374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:43 | 200 | 16.783906629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:48 | 200 | 16.814810104s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:53 | 200 | 21.155111649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:58 | 200 | 19.726485296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:01 | 200 | 20.946274203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:06 | 200 | 22.368849507s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:11 | 200 | 21.896125558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:17 | 200 | 23.085571345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:20 | 200 | 22.225495662s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:23 | 200 | 21.301559483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:27 | 200 | 20.174640597s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:30 | 200 | 18.834169306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:33 | 200 | 15.603949454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:38 | 200 | 17.058913837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:44 | 200 | 16.563717569s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:50 | 200 | 19.221689691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:55 | 200 | 21.953828284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:02 | 200 | 23.471949006s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:07 | 200 | 27.869508581s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:11 | 200 | 26.667893864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:14 | 200 | 23.815466378s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:18 | 200 |  22.57845429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:25 | 200 | 21.837471887s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:29 | 200 | 21.492970188s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:35 | 200 | 23.588815618s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:42 | 200 | 27.648614779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:46 | 200 | 27.428164736s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:49 | 200 | 24.491522237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:53 | 200 | 23.762405409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:56 | 200 | 21.184754497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:00 | 200 | 17.897031205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:06 | 200 | 19.422177723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:10 | 200 |  20.27673799s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:15 | 200 | 21.978128969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:18 | 200 | 21.502753675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:22 | 200 | 21.310592081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:27 | 200 |   20.6541796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:32 | 200 | 21.756492134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:35 | 200 | 20.032041134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:40 | 200 | 21.841510131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:46 | 200 | 23.420359619s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:49 | 200 | 21.166489426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:53 | 200 | 20.938941152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:58 | 200 | 22.400214451s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.994-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 |   9.90261633s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.994-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 |  5.596997491s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.995-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 | 13.457167119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 200 | 19.394105453s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:18.822-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11470 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11470 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:18.823-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:18.823-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:18.823-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11470 (version 0.12.9)"
time=2025-11-10T02:08:18.824-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:18.824-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45617"
time=2025-11-10T02:08:19.018-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36567"
time=2025-11-10T02:08:19.499-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39511"
time=2025-11-10T02:08:19.499-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44129"
time=2025-11-10T02:08:19.727-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:20 | 200 |      39.273µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:20 | 200 |   45.698828ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     547.497µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:38.903-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43523"
time=2025-11-10T02:09:39.371-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:39.371-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11470/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 44177"
time=2025-11-10T02:09:39.373-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:39.373-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="88.6 GiB" free_swap="0 B"
time=2025-11-10T02:09:39.373-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="15.1 GiB" free="15.6 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:39.391-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:39.391-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:44177"
time=2025-11-10T02:09:39.399-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:39.459-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:39.594-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:40.075-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:40.269-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:40.269-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:40.270-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:40.773-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.40 seconds"
[GIN] 2025/11/10 - 02:09:41 | 200 |  2.533293825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:41 | 200 |  412.740886ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:15 | 200 |  1.313870605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:17 | 200 |  2.123549814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:16 | 200 |  1.940507722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:19 | 200 |   2.86755388s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:21 | 200 |  4.572062769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:23 | 200 |  5.993538537s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:25 | 200 |  5.804976992s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:26 | 200 |  5.365398857s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:29 | 200 |  5.330483036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:30 | 200 |  3.757154283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:32 | 200 |  3.488985092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:26 | 200 | 10.229371369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:40 | 200 |  24.69863404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:45 | 200 | 28.426645666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:50 | 200 | 30.966309717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:55 | 200 | 35.537684914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:59 | 200 | 38.047201888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:02 | 200 | 40.735361676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 42.855471487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:11 | 200 | 44.816646122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:14 | 500 | 45.767227868s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:15.002-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.045355754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.095319589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.036999653s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:17.364-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:18.549-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:25 | 200 | 44.760379258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:29 | 200 | 43.434679376s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:34 | 200 | 43.159368974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:38 | 200 | 42.454871384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:41 | 200 | 41.954306512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:45 | 200 | 42.391995747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:48 | 200 | 41.997286874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:54 | 200 | 46.889900824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:54 | 500 | 45.052423975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:54 | 500 | 45.044738774s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:55.297-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:56.559-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:57 | 500 | 45.064215475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:00 | 200 | 45.215117528s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:00 | 500 | 45.159635735s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:02.411-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:08 | 200 |  42.77492389s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:11 | 200 | 41.274458214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:15 | 200 | 40.889335904s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:19 | 200 | 41.091109135s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.443-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 12.379682415s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.451-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 26.007824385s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.451-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.352127743s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 39.563698962s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.452-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  1.414202212s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  28.73893723s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  9.531463643s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.151976967s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.612120095s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.458-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  6.015501535s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.468-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 19.999938868s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.469-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 23.968332144s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.469-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |   20.8268464s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.470-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 26.479528517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.069596052s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:22.147-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:22.675-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:22.677-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
