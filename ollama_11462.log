time=2025-11-09T23:11:23.895-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:11:23.895-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:11:23.895-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:11:23.895-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-09T23:11:23.896-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:11:23.896-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40947"
time=2025-11-09T23:11:24.078-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39619"
time=2025-11-09T23:11:24.531-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39969"
time=2025-11-09T23:11:24.531-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46753"
time=2025-11-09T23:11:24.739-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:11:28 | 200 |      30.097µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:11:28 | 404 |     150.322µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:11:28 | 200 |      18.555µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:11:29.460-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:11:34.690-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:11:35.962-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:11:37.216-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:11:38.451-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:11:39.673-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:11:40 | 200 | 11.987146589s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:06 | 200 |     316.043µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:24:33.298-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33587"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11462/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:24:34.426-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11462/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 35507"
time=2025-11-09T23:24:34.427-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="95.6 GiB" free_swap="0 B"
time=2025-11-09T23:24:34.427-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11462/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:24:34.427-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[13.1 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:24:34.457-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:24:34.623-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:24:34.624-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:35507"
time=2025-11-09T23:24:34.636-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:34.634-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:24:34.636-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 13783597056 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 13145 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11462/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:24:36.645-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.22 seconds"
time=2025-11-09T23:24:36.645-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:24:36.645-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:36.645-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.22 seconds"
[GIN] 2025/11/09 - 23:24:36 | 200 |  3.711687861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:37 | 200 |  143.731148ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  397.701975ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  145.414631ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  256.563898ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |   130.00379ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:49 | 200 |  129.489526ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:49 | 200 |  109.923958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:01 | 200 |  379.702336ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:02 | 200 |  198.066862ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:02 | 200 |  128.423632ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:40 | 200 |  398.785149ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:41 | 200 |  288.625992ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:41 | 200 |  127.822921ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:44 | 200 |  113.290316ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:44 | 200 |   93.461039ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:45 | 200 |  269.826894ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:21 | 200 |  149.473362ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:22 | 200 |  313.595181ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:22 | 200 |   85.518628ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:34 | 200 |  247.714812ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:34 | 200 |   84.172104ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:34 | 200 |   93.942958ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:37 | 200 |  126.243479ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:37 | 200 |     81.7877ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:40 | 200 |  334.692757ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:41 | 200 |   142.59867ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:41 | 200 |  105.457306ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:45 | 200 |  353.867332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:45 | 200 |  143.766283ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:45 | 200 |  118.368636ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:51 | 200 |  381.532731ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:51 | 200 |  855.399337ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:53 | 200 |  1.150497347s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:12 | 200 |   70.889297ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:12 | 200 |   79.915445ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |   288.35374ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |  110.300545ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |  414.874283ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:16 | 200 |  487.903608ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:16 | 200 |  516.141579ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:41 | 200 |  223.890097ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:41 | 200 |  248.567918ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:29 | 200 |  670.047703ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:29 | 200 |  525.284722ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:30 | 200 |  433.600746ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:31 | 200 |  608.916911ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:32 | 200 |  190.275538ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:32 | 200 |  174.980594ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:35 | 200 |  626.853468ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:36 | 200 |  987.099264ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  1.103737568s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |   671.68425ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  216.447219ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  413.303703ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |   145.54667ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |  166.025735ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  230.560895ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  374.884402ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:59 | 200 |  221.373315ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:59 | 200 |  148.804647ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:00 | 200 |  168.691292ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:03 | 200 |  336.373832ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:04 | 200 |   742.39515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:06 | 200 |   176.80416ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:06 | 200 |  159.254302ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:07 | 200 |  666.204038ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:07 | 200 |  111.547024ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:08 | 200 |  167.995699ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:17 | 200 |  204.008142ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:19 | 200 |  1.471164341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:19 | 200 |  980.710544ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:19 | 200 |  125.856273ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  480.176034ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  293.363941ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  108.158081ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:41 | 200 |  214.830995ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:41 | 200 |  267.824128ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:42 | 200 |   336.79544ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:37:04.310-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:37:04.310-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:37:04.310-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:37:04.310-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-09T23:37:04.310-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:37:04.311-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37133"
time=2025-11-09T23:37:04.494-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34985"
time=2025-11-09T23:37:04.950-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40241"
time=2025-11-09T23:37:04.950-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45249"
time=2025-11-09T23:37:05.174-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:37:09 | 200 |      84.058µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:37:09 | 404 |     187.442µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:37:09 | 200 |      16.721µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:37:09.853-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:37:13.100-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:37:14.321-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:37:15.545-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:37:16.761-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:37:17 | 200 |  8.672159007s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     472.948µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:49:33.852-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:49:33.853-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:49:33.853-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:49:33.853-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-09T23:49:33.853-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:49:33.855-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37195"
time=2025-11-09T23:49:34.053-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44691"
time=2025-11-09T23:49:34.240-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41689"
time=2025-11-09T23:49:34.240-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42935"
time=2025-11-09T23:49:34.742-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:49:38 | 200 |       58.95µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:49:38 | 200 |   44.700459ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     554.792µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:57:41.021-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:57:41.022-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:57:41.022-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:57:41.022-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-09T23:57:41.022-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:57:41.025-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38071"
time=2025-11-09T23:57:41.225-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42787"
time=2025-11-09T23:57:41.414-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43703"
time=2025-11-09T23:57:41.414-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36483"
time=2025-11-09T23:57:41.629-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:57:46 | 200 |      29.125µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:57:46 | 200 |    49.84866ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     519.546µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:05:00.214-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:05:00.214-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:05:00.214-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:05:00.214-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:05:00.215-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:05:00.215-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39351"
time=2025-11-10T00:05:00.414-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43603"
time=2025-11-10T00:05:00.628-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46279"
time=2025-11-10T00:05:00.628-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33769"
time=2025-11-10T00:05:00.857-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:05:05 | 200 |      54.843µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:05:05 | 200 |   47.261073ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:51 | 200 |     602.532µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:11:15.002-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:11:15.003-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:11:15.003-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:11:15.003-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:11:15.003-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:11:15.004-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39299"
time=2025-11-10T00:11:15.200-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36443"
time=2025-11-10T00:11:15.427-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33649"
time=2025-11-10T00:11:15.427-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42169"
time=2025-11-10T00:11:15.652-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:11:20 | 200 |   10.753395ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:11:20 | 200 |   51.360219ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:06 | 200 |     583.977µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:17:53.029-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:17:53.030-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:17:53.030-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:17:53.030-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:17:53.030-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:17:53.031-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40865"
time=2025-11-10T00:17:53.243-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37077"
time=2025-11-10T00:17:53.459-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33753"
time=2025-11-10T00:17:53.459-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42127"
time=2025-11-10T00:17:53.699-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:17:58 | 200 |      38.272µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:17:58 | 200 |   56.222258ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     473.529µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:26:16.320-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:26:16.320-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:26:16.320-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:26:16.320-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:26:16.321-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:26:16.322-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43973"
time=2025-11-10T00:26:16.556-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35947"
time=2025-11-10T00:26:16.981-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42649"
time=2025-11-10T00:26:16.981-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39625"
time=2025-11-10T00:26:17.236-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:26:21 | 200 |      31.889µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:26:21 | 200 |   56.625933ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     621.238µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:37:14.435-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:37:14.435-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:37:14.435-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:37:14.435-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:37:14.436-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:37:14.436-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35657"
time=2025-11-10T00:37:14.649-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46259"
time=2025-11-10T00:37:14.853-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46721"
time=2025-11-10T00:37:14.853-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37329"
time=2025-11-10T00:37:15.087-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:37:19 | 200 |      46.116µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:37:19 | 200 |   47.999447ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     437.181µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:47:48.741-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:47:48.741-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:47:48.741-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:47:48.742-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T00:47:48.742-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:47:48.743-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36909"
time=2025-11-10T00:47:48.948-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42633"
time=2025-11-10T00:47:49.170-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33841"
time=2025-11-10T00:47:49.170-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41987"
time=2025-11-10T00:47:49.387-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:47:53 | 200 |      28.334µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:47:53 | 200 |   46.088196ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     533.272µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:07:48.218-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:07:48.218-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:07:48.218-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:07:48.219-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T01:07:48.219-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:07:48.219-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38047"
time=2025-11-10T01:07:48.424-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46215"
time=2025-11-10T01:07:48.609-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39927"
time=2025-11-10T01:07:48.609-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36543"
time=2025-11-10T01:07:48.827-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:07:53 | 200 |      29.645µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:07:53 | 200 |   66.441446ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     428.194µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:14:59.173-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:14:59.173-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:14:59.173-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:14:59.174-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T01:14:59.174-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:14:59.175-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39343"
time=2025-11-10T01:14:59.370-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46439"
time=2025-11-10T01:14:59.565-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45281"
time=2025-11-10T01:14:59.565-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44581"
time=2025-11-10T01:14:59.781-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:15:04 | 200 |       53.39µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:15:04 | 200 |   52.688216ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:50 | 200 |     295.705µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:39:37.280-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:39:37.281-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:39:37.281-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:39:37.281-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T01:39:37.281-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:39:37.282-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32973"
time=2025-11-10T01:39:37.478-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37901"
time=2025-11-10T01:39:37.972-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34737"
time=2025-11-10T01:39:37.972-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45143"
time=2025-11-10T01:39:38.200-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:39:39 | 200 |      28.313µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:39:39 | 200 |   52.237971ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:23 | 200 |     277.441µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:40.336-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36929"
time=2025-11-10T01:40:40.813-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:40:40.813-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11462/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 41241"
time=2025-11-10T01:40:40.813-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:40:40.813-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="107.5 GiB" free_swap="0 B"
time=2025-11-10T01:40:40.813-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="23.0 GiB" free="23.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:40:40.824-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:40:40.825-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:41241"
time=2025-11-10T01:40:40.836-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:40.868-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:40:40.992-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:40:41.603-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:41.671-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:40:41.672-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:40:41.672-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:40:42.174-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.36 seconds"
[GIN] 2025/11/10 - 01:40:43 | 200 |  2.764275667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:40:43 | 200 |  848.802201ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:40:44 | 200 |  819.020369ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:26 | 200 |  1.659934861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:29 | 200 |  2.223040211s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:30 | 200 |  2.622503112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:32 | 200 |  3.121142487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:33 | 200 |    3.1074042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:37 | 200 |  1.600527582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:38 | 200 |  1.235941304s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:12 | 200 |  3.086995726s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:17 | 200 |  7.224390961s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:24 | 200 |  9.450694701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:27 | 200 | 10.639300223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:30 | 200 | 11.217187289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:34 | 200 | 11.683281506s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:40 | 200 | 12.690066819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:42 | 200 | 11.656240697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:48 | 200 | 11.660426167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:52 | 200 |  9.839788787s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:55 | 200 | 12.471524467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:58 | 200 | 10.074759139s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:02 | 200 |  8.042358217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:10 | 200 | 11.107048892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:14 | 200 | 13.732519856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:17 | 200 | 11.155583412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:22 | 200 |   9.98958124s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:28 | 200 | 12.692763931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:32 | 200 | 14.737482794s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:38 | 200 | 20.872506084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:42 | 200 | 17.842489581s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:47 | 200 | 18.012597796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:51 | 200 | 18.389023727s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:56 | 200 | 19.645191215s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:01 | 200 | 22.417811322s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:05 | 200 | 23.122260353s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:09 | 200 | 21.300923284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:14 | 200 |  22.63101741s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:17 | 200 | 20.578483129s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:22 | 200 | 20.248636417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:26 | 200 | 20.558741048s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:28 | 200 | 18.070218995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:32 | 200 |  17.20020594s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:37 | 200 | 19.325476555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 | 18.540346075s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:45 | 200 | 18.360033777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:49 | 200 | 21.170722985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:57 | 200 | 24.834991117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:02 | 200 | 24.636757688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:05 | 200 | 23.834827707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:09 | 200 | 23.872010502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:11 | 200 |  21.65893987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:14 | 200 | 16.888396652s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:19 | 200 | 16.036894017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:23 | 200 | 17.373813566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:29 | 200 | 19.830459362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:33 | 200 | 21.527287187s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:36 | 200 | 20.982775725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:40 | 200 | 21.625352427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:43 | 200 | 19.711101867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:46 | 200 | 17.347917491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:52 | 200 | 18.725883786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:57 | 200 | 21.658017505s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:03 | 200 | 21.849213516s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:07 | 200 | 23.871267271s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:13 | 200 | 25.556051056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:17 | 200 | 22.501126647s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:19 | 200 | 18.773596112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:25 | 200 | 17.525645661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:29 | 200 | 21.838899614s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:33 | 200 | 18.764638498s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:36 | 200 | 17.002514054s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:43 | 200 | 16.572950964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:49 | 200 | 18.545627465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:51 | 200 | 17.338674418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:55 | 200 | 14.596521434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:58 | 200 | 11.671314982s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:01 | 200 | 12.276894658s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:04 | 200 | 12.124962296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:06 | 200 | 10.239150879s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:31 | 200 |  8.837524769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:35 | 200 | 12.078229604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:42 | 200 | 16.427352074s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:45 | 200 | 20.437149325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:51 | 200 | 26.115736391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:56 | 200 | 21.034781222s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:01 | 200 | 25.389210669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:09 | 200 | 26.992492213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:15 | 200 | 29.223357453s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:18 | 200 | 26.297773804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:23 | 200 | 26.447693162s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:28 | 200 | 25.888516454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:34 | 200 | 24.552479146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:39 | 200 | 23.348224774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:46 | 200 | 28.071690728s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:52 | 200 | 28.212847307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:55 | 200 | 26.673435338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:00 | 200 | 24.721289321s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:03 | 200 | 23.511531492s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:06 | 200 | 19.737242753s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:10 | 200 |  18.51293565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:16 | 200 | 20.545259523s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:25 | 200 | 24.518954339s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:29 | 200 | 25.183255272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:35 | 200 | 28.014223126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:39 | 200 | 27.731188481s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:42 | 200 | 26.595948684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:46 | 200 | 20.790245819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:48 | 200 | 18.490380242s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:53 | 200 | 18.143866711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:57 | 200 | 17.837510403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:02 | 200 | 19.516008127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:06 | 200 | 19.648841239s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:11 | 200 | 22.916265368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:17 | 200 | 23.893793152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:23 | 200 | 26.054809284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:29 | 200 | 25.885713561s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:35 | 200 | 29.011396462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:38 | 200 | 26.598749977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:42 | 200 | 24.935686978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:45 | 200 | 21.388866041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:47 | 200 | 17.916002946s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:50 | 200 | 14.385695928s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 | 15.269090325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:57 | 200 | 14.253760705s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:04 | 200 |  19.00480217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:11 | 200 | 22.650048815s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:16 | 200 | 26.258377967s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:22 | 200 |  27.56312784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:26 | 200 |   27.8737803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:33 | 200 | 28.657421536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:36 | 200 | 25.136318283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:38 | 200 | 21.295327205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:42 | 200 | 20.321661625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:47 | 200 | 20.446355832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:50 | 200 | 16.361070425s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:54 | 200 | 17.214995865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:58 | 200 | 19.395165915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:01 | 200 | 18.407358897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:04 | 200 | 16.882015098s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:08 | 200 | 17.884798126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:11 | 200 | 16.204183991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:14 | 200 | 15.312494492s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:18 | 200 | 16.484195103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:21 | 200 | 16.890123395s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:26 | 200 | 17.524183855s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:30 | 200 | 18.545989813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:34 | 200 | 20.128114685s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:40 | 200 | 21.949133084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:44 | 200 | 21.877644456s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:48 | 200 | 21.476640609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:54 | 200 |  23.76476334s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:00 | 200 |   24.6585504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:05 | 200 | 24.766836426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:09 | 200 | 24.979166319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:14 | 200 | 25.497700008s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:18 | 200 |  24.03825373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:27 | 200 | 26.375272081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:50 | 500 |   45.0529555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:55 | 500 | 45.044749991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:00 | 200 | 44.686431806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:04 | 500 | 45.071455072s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:06 | 200 | 39.023584483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:12 | 200 | 21.238695883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:16 | 200 | 20.697050791s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:21 | 200 | 21.691867044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:30 | 200 | 25.764316604s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:34 | 200 | 27.571196985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:37 | 200 | 25.107903467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:44 | 200 | 27.599723492s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:54:48.441-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 01:54:50 | 200 | 28.241788498s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:51 | 200 | 20.984332631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:55 | 200 | 20.509312828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:59 | 200 | 22.040335851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:02 | 200 | 18.011369288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:07 | 200 | 16.502735851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:11 | 200 |   19.0931449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:13 | 200 | 18.022777776s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:16 | 200 |  16.78656101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:20 | 200 | 17.577528662s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:25 | 200 | 18.087251377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:29 | 200 | 18.108899679s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:34 | 200 | 20.348161497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:39 | 200 | 23.032544205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:45 | 200 | 24.619952007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:50 | 200 | 24.835882171s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:56 | 200 | 26.411525217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:02 | 200 | 27.343758178s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:06 | 200 | 26.559058894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:15 | 200 | 29.350370062s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:17 | 200 | 26.382217878s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:21 | 200 | 25.080879479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:26 | 200 | 23.540801897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:29 | 200 | 22.205654365s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:35 | 200 | 19.630421672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:40 | 200 | 22.124957267s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:43 | 200 | 21.340792216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:50 | 200 | 23.253362726s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:53 | 200 | 24.224291852s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:00 | 200 | 24.822630985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:05 | 200 | 24.478495596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:07 | 200 | 24.405826863s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:10 | 200 | 19.819917502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:12 | 200 | 17.781290067s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:16 | 200 | 15.633318822s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:21 | 200 | 15.234723042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:25 | 200 | 17.306228663s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:31 | 200 |  21.03738483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:38 | 200 | 26.355206788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:44 | 200 | 27.556589156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:49 | 200 | 27.908051119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:55 | 200 | 29.659126008s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:01 | 200 | 29.536660959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:06 | 200 | 27.963514684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:12 | 200 | 27.962299146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:15 | 200 |  24.87680194s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:17 | 200 | 22.131139589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:21 | 200 | 19.512252087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:26 | 200 | 18.979849721s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:29 | 200 | 16.551773849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:34 | 200 | 19.255041672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:38 | 200 | 20.161590575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:42 | 200 | 20.798330957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:45 | 200 | 19.034883413s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:51 | 200 | 20.768141474s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:55 | 200 | 20.810468597s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:00 | 200 | 21.727082557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:04 | 200 |  21.50737118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:10 | 200 | 23.580036345s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:15 | 200 | 23.564775934s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:19 | 200 | 22.672328592s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:23 | 200 | 22.051273686s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:26 | 200 | 21.806384426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:33 | 200 |  22.85202565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:35 | 200 | 20.147051783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:38 | 200 | 18.880182627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:43 | 200 | 19.991983098s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:47 | 200 | 19.778181526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:51 | 200 | 17.584400918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:58 | 200 |  21.80462686s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:05 | 200 |  26.29696639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:10 | 200 | 26.372435785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:15 | 200 | 28.511486872s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:19 | 200 | 28.367006623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:26 | 200 | 27.243513808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:30 | 200 | 24.854827167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:35 | 200 | 24.638380112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:41 | 200 | 25.671339995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:44 | 200 | 24.794624035s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:50 | 200 | 24.509987752s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:56 | 200 | 25.211410156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:02 | 200 | 26.263274322s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:07 | 200 | 25.270983899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:10 | 200 | 24.904902265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:13 | 200 | 22.722773432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:17 | 200 | 21.509948487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:21 | 200 | 18.446404005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:23 | 200 | 15.452033225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:25 | 200 | 14.558097211s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:29 | 200 | 15.172239362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:34 | 200 | 16.462767338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:38 | 200 | 15.708670882s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:45 | 200 | 19.524402622s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:47 | 200 | 18.358323544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:51 | 200 | 15.904743527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:54 | 200 |  6.423509829s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:56 | 200 |  4.726779308s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:58 | 200 |  2.149514185s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:29 | 200 |  9.575654392s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:34 | 200 | 15.070245805s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:39 | 200 | 19.036182609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:44 | 200 | 23.721825258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:47 | 200 | 26.792279287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:51 | 200 |  22.40576708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:56 | 200 | 20.309350629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:00 | 200 | 21.235884214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:05 | 200 | 20.407289461s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:08 | 200 | 20.312331867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:11 | 200 | 16.609280062s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:14 | 200 | 12.957471109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:18 | 200 | 11.412032428s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:24 | 200 | 11.923237476s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:28 | 200 |  16.14440643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:31 | 200 | 12.940805778s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:38 | 200 | 19.364525725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:43 | 200 | 17.697807232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:49 | 200 | 17.196623052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:53 | 200 | 16.090217403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:58 | 200 | 13.141072306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:03 | 200 |  13.57457375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:08 | 200 | 17.438521616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:14 | 200 | 17.810553865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:22 | 200 | 19.213480012s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:25 | 200 | 16.566539475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:29 | 200 | 15.669194424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:32 | 200 | 10.882981201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:36 | 200 | 10.141466953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:40 | 200 |  8.072865806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:43 | 200 |  8.322546174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:48 | 200 | 11.281919746s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:52 | 200 | 13.291934117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:56 | 200 |  13.10836724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:59 | 200 | 13.901151617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:02 | 200 | 12.295654814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:05 | 200 |  8.817853109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:11 | 200 |  11.15624648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:14 | 200 | 12.167245037s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:16 | 200 | 13.793610755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:19 | 200 | 13.528947386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:23 | 200 | 14.130457277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:27 | 200 | 10.986337546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:30 | 200 |   8.53585364s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:34 | 200 |  9.868286929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:39 | 200 | 11.927872952s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:45 | 200 |  16.27640979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:50 | 200 | 15.715954231s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:24 | 200 | 44.356398488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:28 | 200 | 40.757583628s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:33 | 200 | 39.468844209s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:38 | 200 | 37.317057573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:42 | 200 | 35.393916175s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:45 | 200 | 20.687462801s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:48 | 200 | 19.744910214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:53 | 200 | 20.493044744s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.989-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 |  14.37837665s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.995-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 |  6.011633881s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.998-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 | 10.927865636s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.025-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:07:00 | 500 | 17.535650087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 200 | 21.425761757s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:02.310-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11462 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11462 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:02.311-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:02.311-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:02.311-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11462 (version 0.12.9)"
time=2025-11-10T02:08:02.311-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:02.312-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35759"
time=2025-11-10T02:08:02.513-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34951"
time=2025-11-10T02:08:02.729-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45535"
time=2025-11-10T02:08:02.729-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40721"
time=2025-11-10T02:08:02.951-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:04 | 200 |      55.093µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:04 | 200 |    47.00101ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:48 | 200 |      537.97µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:06.430-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37323"
time=2025-11-10T02:09:06.731-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:06.731-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11462/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 34585"
time=2025-11-10T02:09:06.731-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:06.731-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="107.9 GiB" free_swap="0 B"
time=2025-11-10T02:09:06.731-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="23.1 GiB" free="23.5 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:06.743-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:06.743-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:34585"
time=2025-11-10T02:09:06.753-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:06.783-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:07.087-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:07.406-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:07.511-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:07.511-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:07.512-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:07.762-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.03 seconds"
[GIN] 2025/11/10 - 02:09:08 | 200 |  2.260377948s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:09 | 200 |  824.196487ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:10 | 200 |  780.784757ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:49 | 200 |  847.225473ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:51 | 200 |  1.777372342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:52 | 200 |  1.251884576s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:54 | 200 |  2.446469394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:55 | 200 |  2.441175232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:00 | 200 |  949.422521ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:01 | 200 |  1.058765491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:17 | 200 |  2.088373384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:20 | 200 |  4.637101517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:22 | 200 |  4.754282249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:25 | 200 |  4.485553565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:27 | 200 |   4.48345229s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:30 | 200 |  5.660470132s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:32 | 200 |  4.200121808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:33 | 200 |  1.398511006s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:12:10.478-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7193 keep=4 new=4096
[GIN] 2025/11/10 - 02:12:11 | 200 |  1.593985126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:16 | 200 |  6.087144878s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:35 | 200 | 24.202060307s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:12:38.015-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7193 keep=4 new=4096
[GIN] 2025/11/10 - 02:12:40 | 200 | 27.955350422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:45 | 200 | 31.910945094s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:48 | 200 | 36.120179226s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:51 | 200 | 37.581990082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:54 | 200 | 40.182871485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:56 | 200 | 42.508259897s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:00 | 200 | 44.702096995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:01 | 500 | 45.045555755s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:01.941-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:01 | 500 | 45.032196639s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:02.714-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:03 | 500 | 45.048057263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:04 | 500 |  44.98135576s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:04.233-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:04.770-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:04 | 500 | 45.050045915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:04 | 500 | 45.605981833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:05 | 500 | 45.233842109s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:06.112-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:06.632-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:07.025-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:11 | 200 |  35.10879844s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:16 | 200 | 35.310345545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:21 | 200 | 36.162653387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:26 | 200 | 37.157301103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:29 | 200 | 38.072540445s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:31 | 200 | 37.603791917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:40 | 200 | 43.797690146s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:43.292-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7296 keep=4 new=4096
[GIN] 2025/11/10 - 02:13:45 | 200 | 45.531916058s |       127.0.0.1 | POST     "/api/generate"
Error #01: write tcp 127.0.0.1:11462->127.0.0.1:36386: write: broken pipe
[GIN] 2025/11/10 - 02:13:46 | 500 | 45.159907526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:47 | 500 | 45.093503991s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:48.360-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:48 | 500 | 45.067962659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:49 | 500 | 45.062761777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:50 | 500 | 45.096561417s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:50.313-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:50 | 500 | 45.048763348s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:51.869-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:52.890-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:53.441-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:53.441-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:55 | 200 | 44.383814964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:01 | 200 | 44.780262047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:06 | 200 | 43.930448393s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:10 | 200 | 44.030119303s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:14 | 200 | 44.741954086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:17 | 500 |  45.06822163s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:17.554-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:21.442-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 25.017357044s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.450-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.454870037s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.452-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |   3.88147418s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.456-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.317648547s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 29.930834615s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 40.303132872s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.457-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  6.495412231s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.459-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:21.459-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 14.921278192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 19.107396019s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.459-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 10.860305081s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  31.47445429s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.463-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:21.464-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.850853202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 30.712052377s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.464-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.926465069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:22 | 200 | 46.256955742s |       127.0.0.1 | POST     "/api/generate"
