time=2025-11-09T23:12:40.668-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:12:40.668-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:12:40.668-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:12:40.668-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-09T23:12:40.669-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:12:40.669-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36467"
time=2025-11-09T23:12:40.852-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34851"
time=2025-11-09T23:12:41.032-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37073"
time=2025-11-09T23:12:41.032-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32981"
time=2025-11-09T23:12:41.253-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:12:45 | 200 |      29.796µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:12:45 | 404 |     236.414µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:12:45 | 200 |      16.591µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:12:46.196-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:12:49.417-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:12:50.636-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:12:51.846-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:12:53.117-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:12:54.357-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:12:55 | 200 |  9.894798636s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |     380.514µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:24:26.599-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44599"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11467/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:24:27.243-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11467/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 34189"
time=2025-11-09T23:24:27.244-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="104.5 GiB" free_swap="0 B"
time=2025-11-09T23:24:27.249-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11467/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:24:27.249-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[20.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:24:27.263-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:24:28.305-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:24:28.305-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:34189"
time=2025-11-09T23:24:28.312-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:24:28.312-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:28.312-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 22369206272 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 21332 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11467/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:24:30.324-07:00 level=INFO source=server.go:1289 msg="llama runner started in 3.08 seconds"
time=2025-11-09T23:24:30.324-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:24:30.324-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:30.324-07:00 level=INFO source=server.go:1289 msg="llama runner started in 3.08 seconds"
[GIN] 2025/11/09 - 23:24:30 | 200 |  3.899573688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:30 | 200 |  3.691926659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:30 | 200 |    3.9690763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:30 | 200 |  498.239332ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  593.070498ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  327.436741ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  182.869159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  234.964541ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  278.258282ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  272.250531ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |   135.62113ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:31 | 200 |  220.676487ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:32 | 200 |   386.51216ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:32 | 200 |  283.307926ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:32 | 200 |  645.818349ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |   324.68501ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  272.424375ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |  187.123802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:48 | 200 |   173.01869ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:49 | 200 |  341.600717ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:50 | 200 |  277.856147ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:55 | 200 |  418.546103ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:56 | 200 |  872.471895ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:57 | 200 |  218.620059ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:58 | 200 |   157.19601ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:58 | 200 |  149.092387ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:01 | 200 |  236.791724ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:01 | 200 |  262.838432ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:01 | 200 |  279.041602ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:05 | 200 |  197.352604ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  239.853844ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  185.735389ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  362.075903ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  379.666094ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:07 | 200 |  250.844013ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  609.606371ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  279.530941ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:19 | 200 |  430.655827ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:19 | 200 |  136.535369ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:19 | 200 |  241.283377ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:23 | 200 |  127.094176ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:23 | 200 |  137.738818ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:35 | 200 |  130.247307ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:35 | 200 |  131.188564ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:35 | 200 |   82.816886ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:43 | 200 |  562.117761ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:44 | 200 |   606.44246ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:00 | 200 |  526.277684ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:00 | 200 |  308.320232ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:01 | 200 |  190.851622ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:12 | 200 |   64.455342ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:12 | 200 |  122.088814ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:14 | 200 |  255.276385ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:14 | 200 |  135.776578ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:15 | 200 |  136.694482ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:16 | 200 |  101.059918ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:17 | 200 |    84.33798ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:17 | 200 |  213.305895ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:17 | 200 |  308.405298ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:24 | 200 |  185.738812ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:25 | 200 |  719.567586ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:29 | 200 |  339.213308ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:29 | 200 |  246.415259ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:29 | 200 |   82.650247ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |  510.369258ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |  131.040159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |    164.9607ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:50 | 200 |  227.025861ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:50 | 200 |  261.865717ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:59 | 200 |  425.362607ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:59 | 200 |  265.075547ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:01 | 200 |  1.140483722s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:05 | 200 |   164.36232ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:05 | 200 |  158.799532ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:08 | 200 |  472.095895ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:08 | 200 |  397.721418ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:16 | 200 |  200.274556ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:17 | 200 |  245.990927ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:27 | 200 |  591.374805ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:28 | 200 |  656.481284ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:28 | 200 |  465.625997ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:28 | 200 |  819.196765ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:29 | 200 |  1.033345775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:29 | 200 |    1.5310114s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:29 | 200 |  923.220346ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:30 | 200 |  370.083606ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:31 | 200 |  714.326582ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:32 | 200 |  446.622731ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:32 | 200 |   280.09743ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:34 | 200 |  332.978879ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:35 | 200 |  506.795925ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:43 | 200 |  305.704723ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:43 | 200 |  375.744522ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:44 | 200 |  474.355478ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:28 | 200 |  444.361219ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:29 | 200 |  430.646842ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:29 | 200 |  153.200808ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:32 | 200 |  147.075855ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:34 | 200 |  1.055705865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:34 | 200 |  966.251962ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  4.913543994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  5.185864346s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |  5.584625103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |   157.55816ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  187.006918ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  525.853747ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |  917.279877ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:45 | 200 |  229.417415ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |   1.15159055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  1.328267103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  1.076752156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |  3.535888437s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |  3.370250381s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |  3.291340155s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |  413.004963ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |  833.967391ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |   733.96094ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:51 | 200 |  414.177714ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:52 | 200 |  206.450926ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:52 | 200 |  254.078322ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  233.070296ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:55 | 200 |  214.137844ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:02 | 200 |  290.240226ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:02 | 200 |   120.91792ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:03 | 200 |   127.17817ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:07 | 200 |  399.371409ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:08 | 200 |  883.540571ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:10 | 200 |   2.10203418s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  323.586598ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  824.504668ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |   849.83519ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  267.401769ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  390.000026ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  245.724515ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  389.519607ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:17 | 200 |  1.084267872s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:23 | 200 |   82.333508ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:23 | 200 |   87.802091ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:29 | 200 |   276.09269ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:29 | 200 |  189.880481ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:29 | 200 |  300.118955ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:41 | 200 |  206.866056ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:41 | 200 |   89.924959ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:42 | 200 |  292.006436ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:44 | 200 |  233.443801ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:44 | 500 |  439.552276ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:38:12.643-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:38:12.644-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:38:12.644-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:38:12.644-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-09T23:38:12.644-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:38:12.644-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45753"
time=2025-11-09T23:38:13.111-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43123"
time=2025-11-09T23:38:13.296-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42759"
time=2025-11-09T23:38:13.296-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34129"
time=2025-11-09T23:38:13.503-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:38:17 | 200 |      54.713µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:38:17 | 404 |     329.168µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:38:17 | 200 |      18.625µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:38:18.152-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:38:21.372-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:38:22.591-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:38:23.817-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:38:25.035-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:38:26 | 200 |  8.608543747s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     684.906µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:49:59.173-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:49:59.174-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:49:59.174-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:49:59.174-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-09T23:49:59.174-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:49:59.175-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46089"
time=2025-11-09T23:49:59.591-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38825"
time=2025-11-09T23:49:59.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44381"
time=2025-11-09T23:49:59.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35373"
time=2025-11-09T23:50:00.018-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:50:04 | 200 |      47.499µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:50:04 | 200 |   46.975853ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |      644.03µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:58:06.347-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:58:06.347-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:58:06.348-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:58:06.348-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-09T23:58:06.348-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:58:06.348-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34645"
time=2025-11-09T23:58:06.552-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34479"
time=2025-11-09T23:58:06.755-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35279"
time=2025-11-09T23:58:06.755-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42305"
time=2025-11-09T23:58:07.196-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:58:11 | 200 |      28.133µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:58:11 | 200 |   46.198564ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     572.586µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:05:25.541-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:05:25.542-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:05:25.542-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:05:25.542-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:05:25.542-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:05:25.543-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46429"
time=2025-11-10T00:05:25.748-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36445"
time=2025-11-10T00:05:26.244-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40705"
time=2025-11-10T00:05:26.244-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37269"
time=2025-11-10T00:05:26.468-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:05:30 | 200 |      29.676µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:05:30 | 200 |   44.711094ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |     465.966µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:11:40.354-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:11:40.354-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:11:40.354-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:11:40.354-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:11:40.355-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:11:40.356-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38089"
time=2025-11-10T00:11:40.578-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34087"
time=2025-11-10T00:11:40.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32869"
time=2025-11-10T00:11:40.792-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46615"
time=2025-11-10T00:11:41.298-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:11:45 | 200 |      40.676µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:11:45 | 200 |   53.801504ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:06 | 200 |      367.43µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:18:18.409-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:18:18.409-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:18:18.410-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:18:18.410-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:18:18.410-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:18:18.411-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38739"
time=2025-11-10T00:18:18.634-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37893"
time=2025-11-10T00:18:19.113-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45227"
time=2025-11-10T00:18:19.113-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46211"
time=2025-11-10T00:18:19.357-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:18:23 | 200 |      60.624µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:18:23 | 200 |   56.041819ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     609.244µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:26:41.699-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:26:41.699-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:26:41.699-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:26:41.700-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:26:41.700-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:26:41.700-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41095"
time=2025-11-10T00:26:42.041-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44811"
time=2025-11-10T00:26:42.257-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37651"
time=2025-11-10T00:26:42.257-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43087"
time=2025-11-10T00:26:42.504-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:26:46 | 200 |      60.374µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:26:46 | 200 |   55.955399ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     547.629µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:37:39.766-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:37:39.767-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:37:39.767-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:37:39.767-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:37:39.767-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:37:39.768-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35959"
time=2025-11-10T00:37:39.963-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39375"
time=2025-11-10T00:37:40.161-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33585"
time=2025-11-10T00:37:40.161-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34753"
time=2025-11-10T00:37:40.392-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:37:44 | 200 |      28.934µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:37:44 | 200 |   48.211345ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     594.447µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:48:14.080-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:48:14.080-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:48:14.080-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:48:14.081-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T00:48:14.081-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:48:14.082-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46059"
time=2025-11-10T00:48:14.281-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40777"
time=2025-11-10T00:48:14.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45013"
time=2025-11-10T00:48:14.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41395"
time=2025-11-10T00:48:14.953-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:48:19 | 200 |      31.299µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:48:19 | 200 |   45.427315ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     296.507µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:08:13.568-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:08:13.569-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:08:13.569-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:08:13.569-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T01:08:13.570-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:08:13.570-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38541"
time=2025-11-10T01:08:13.795-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35549"
time=2025-11-10T01:08:14.242-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36105"
time=2025-11-10T01:08:14.242-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43169"
time=2025-11-10T01:08:14.470-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:08:18 | 200 |      59.922µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:08:18 | 200 |   49.601475ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     493.086µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:15:24.522-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:15:24.523-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:15:24.523-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:15:24.523-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T01:15:24.523-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:15:24.524-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33843"
time=2025-11-10T01:15:24.731-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43379"
time=2025-11-10T01:15:24.941-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33949"
time=2025-11-10T01:15:24.941-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42995"
time=2025-11-10T01:15:25.398-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:15:29 | 200 |      31.719µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:15:29 | 200 |   48.370182ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:50 | 200 |     398.628µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:39:47.608-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:39:47.609-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:39:47.609-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:39:47.609-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T01:39:47.609-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:39:47.610-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39737"
time=2025-11-10T01:39:47.812-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42829"
time=2025-11-10T01:39:48.000-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37657"
time=2025-11-10T01:39:48.000-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39889"
time=2025-11-10T01:39:48.219-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:39:49 | 200 |      36.569µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:39:49 | 200 |   45.594497ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     633.811µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:04.473-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45639"
time=2025-11-10T01:41:04.859-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:04.860-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11467/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 41291"
time=2025-11-10T01:41:04.860-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:04.860-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="94.0 GiB" free_swap="0 B"
time=2025-11-10T01:41:04.860-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="17.8 GiB" free="18.2 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:04.877-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:04.878-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:41291"
time=2025-11-10T01:41:04.885-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:04.940-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:05.078-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:05.833-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:06.213-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:06.213-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:06.213-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:06.213-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:06.214-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:06.214-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:06.717-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.86 seconds"
[GIN] 2025/11/10 - 01:41:07 | 200 |  3.397685446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:08 | 200 |  1.546058402s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:09 | 200 |  1.288868249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:10 | 200 |  859.338353ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:30 | 200 |  4.198248516s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:31.204-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8658 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:32 | 200 |  3.798724153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:37 | 200 |  6.591945451s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:37.847-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8754 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:38 | 200 |  6.377173542s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:39.540-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8769 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:40 | 200 |  1.521336386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:03 | 200 |  2.964963694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:06 | 200 |  2.855462735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:17 | 200 |   4.25524024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:25 | 200 |  7.627302259s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:30 | 200 | 12.447877116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:35 | 200 | 15.929386218s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:38 | 200 | 15.202568619s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:42 | 200 | 10.683502755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:45 | 200 |  8.322822424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:51 | 200 | 12.358893779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:54 | 200 | 10.972888913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:00 | 200 | 14.842655919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:04 | 200 | 14.957088126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:08 | 200 | 12.775474599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:12 | 200 | 11.183084627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:17 | 200 | 12.153056526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:24 | 200 | 17.169483694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:30 | 200 | 17.028032784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:36 | 200 | 23.813068681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:41 | 200 | 22.663671467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:45 | 200 | 19.927544036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:49 | 200 | 17.446821373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:52 | 200 | 15.106372058s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:59 | 200 | 14.953475557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:05 | 200 | 19.371976093s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:10 | 200 | 21.291746895s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:15 | 200 | 24.513138341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:20 | 200 | 27.149466422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:26 | 200 | 26.617307985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:31 | 200 | 26.139278369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:36 | 200 | 25.490023934s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:43 | 200 | 27.749087796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:48 | 200 | 27.222602056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:53 | 200 |  26.19870509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:58 | 200 | 26.889203851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:03 | 200 | 27.241286587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:07 | 200 | 24.067100507s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:12 | 200 | 23.953023599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:16 | 200 | 22.960257078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:18 | 200 | 19.649767715s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:21 | 200 | 17.490031361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:24 | 200 | 16.284919469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:29 | 200 |  16.96855132s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:32 | 200 | 16.044202377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:37 | 200 | 18.202575918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:42 | 200 | 20.668050077s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:49 | 200 | 23.997245888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:54 | 200 | 24.505107272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:00 | 200 | 27.493701765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:04 | 200 | 25.609130605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:08 | 200 | 24.704201691s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:13 | 200 |  23.68885739s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:19 | 200 | 22.866171928s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:23 | 200 | 21.354316179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:26 | 200 | 18.545675686s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:33 | 200 | 18.603570279s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:37 | 200 | 24.214241316s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:42 | 200 | 21.844728314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:49 | 200 | 22.420600554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:54 | 200 | 26.980988958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:59 | 200 |  25.86567367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:00 | 200 | 22.279759703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:03 | 200 | 20.687668007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:07 | 200 | 12.595841254s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:09 | 200 |  2.280288108s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:28 | 200 |  6.943271926s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:33 | 200 | 10.545578011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:39 | 200 | 14.557308344s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:45 | 200 | 20.832103214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:50 | 200 | 24.917951307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:55 | 200 | 26.386500153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:59 | 200 | 25.978749896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:03 | 200 | 24.039264356s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:09 | 200 | 23.924496263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:15 | 200 | 24.509223136s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:19 | 200 | 23.913155487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:23 | 200 |  23.36884327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:29 | 200 | 25.249986951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:35 | 200 | 25.138513832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:39 | 200 | 23.410355328s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:45 | 200 | 25.319519025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:49 | 200 | 24.348229876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:53 | 200 | 21.725100447s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:57 | 200 | 20.645003039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:02 | 200 | 18.940597541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:07 | 200 | 17.140710314s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:11 | 200 |  22.05426918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:17 | 200 | 21.491383157s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:21 | 200 | 18.440704766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:31 | 200 | 24.809740305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:37 | 200 | 27.923392094s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:42 | 200 | 26.130624743s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:46 | 200 | 23.851923357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:50 | 200 | 21.919043268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:56 | 200 | 21.188738525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:01 | 200 |  20.45913876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:03 | 200 | 21.105477202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:07 | 200 |   19.7686154s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:10 | 200 | 16.819590999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:12 | 200 | 12.520692289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:15 | 200 |  9.365744643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:18 | 200 |  5.323992512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:23 | 200 |  10.60296809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:27 | 200 | 11.611122935s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:30 | 200 | 10.365523159s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:34 | 200 |  8.952252478s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:40 | 200 |  9.205575222s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:43 | 200 | 11.684871302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:50 | 200 | 15.079468225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:56 | 200 | 17.544749066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:00 | 200 |  16.61178117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:03 | 200 | 13.884020244s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:06 | 200 | 10.985330611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:09 | 200 |  8.389840938s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:15 | 200 |  14.53977847s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:19 | 200 | 12.305724221s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:21 | 200 | 12.590570206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:25 | 200 |  10.39326244s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:28 | 200 |  7.570663534s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:33 | 200 |  5.981707409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:39 | 200 | 10.016953203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:43 | 200 | 10.107217332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:47 | 200 |  7.498270394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:51 | 200 |  5.103005206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:56 | 200 |    8.4885543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:00 | 200 |  8.376963508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:06 | 200 | 13.529177701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:12 | 200 | 12.272295266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:17 | 200 | 10.639542793s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:22 | 200 |  9.348787916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:29 | 200 | 11.337334793s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:31 | 200 | 11.294494309s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:34 | 200 |  11.84631598s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:36 | 200 |  8.443720483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:42 | 200 | 10.562767008s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:46 | 200 |  11.20972193s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:52 | 200 | 16.174935596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:57 | 200 | 15.871847487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:00 | 200 | 12.736971094s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:04 | 200 |  9.933497528s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:12 | 200 | 12.120215258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:16 | 200 | 14.009755101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:22 | 200 | 11.532063089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:27 | 200 | 14.029437624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:33 | 200 | 16.202368637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:39 | 200 | 22.056105201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:44 | 200 | 21.599107265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:48 | 200 | 18.762318438s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:52 | 200 | 14.746386814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:58 | 200 | 14.100319925s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:04 | 200 | 12.724427385s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:08 | 200 | 10.487231442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:12 | 200 |   7.52309112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:16 | 200 |  8.688480463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:20 | 200 |  8.946896398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:23 | 200 |  4.631197827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:30 | 200 |  5.719964804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:35 | 200 |  8.028764617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:42 | 200 |  8.033915126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:49 | 200 |  9.183392256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:54 | 200 |   5.92010457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:00 | 200 |  5.575251578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:05 | 200 |  5.244878401s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:11 | 200 | 10.786598913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:16 | 200 |   9.16496899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:20 | 200 |   4.61218384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:25 | 200 |  5.462828082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:29 | 200 |  6.783730007s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:35 | 200 |  6.045356676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:40 | 200 | 10.397634693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:45 | 200 | 10.715470863s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:48 | 200 |  5.833818678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:56 | 200 |  7.516163359s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:59 | 200 | 10.675738347s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:03 | 200 |  8.791815391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:07 | 200 |  7.115724442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:11 | 200 |  5.295133504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:17 | 200 |  4.751595157s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:24 | 200 |  5.528814666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:29 | 200 |   5.31603356s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:36 | 200 | 11.835847069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:42 | 200 | 12.438346999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:46 | 200 | 15.494783369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:50 | 200 | 14.064444059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:55 | 200 | 12.116380025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:00 | 200 | 10.709053955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:03 | 200 | 12.168645088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:05 | 200 |  9.184819015s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:07 | 200 |  5.632231864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:15 | 200 |  6.993104747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:19 | 200 |  9.495905014s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:23 | 200 |   7.59718235s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:27 | 200 |  6.310454181s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:34 | 200 |  5.748621335s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:39 | 200 |  5.205048478s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:44 | 200 |  4.053386577s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:55 | 200 |  8.028450297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:00 | 200 |  6.573167502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:02 | 200 |  2.809545373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:08 | 200 |  3.324756438s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:17 | 200 |  8.239249869s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:21 | 200 |  9.122626803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:26 | 200 |   7.28378962s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:31 | 200 |  9.961966316s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:36 | 200 | 11.034001747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:42 | 200 |  9.783873687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:47 | 200 |  9.346250963s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:53 | 200 |  8.514693299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:00 | 200 | 12.630285724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:04 | 200 | 12.971979973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:09 | 200 | 16.668594947s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:14 | 200 | 16.219499339s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:19 | 200 | 14.919374319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:26 | 200 | 21.207098648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:30 | 200 | 20.472800738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:35 | 200 | 18.877381748s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:39 | 200 | 15.068827768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:43 | 200 | 13.565737877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:49 | 200 | 13.890055726s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:54 | 200 | 18.392607378s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:01 | 200 | 21.364593049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:04 | 200 |  21.89614052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:09 | 200 | 25.741514434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:15 | 200 | 25.240559113s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:22 | 200 | 26.890935685s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:28 | 200 | 26.004091502s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:32 | 200 | 24.376915055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:36 | 200 | 23.262434908s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:42 | 200 | 22.355900997s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:46 | 200 | 20.814004663s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:50 | 200 | 17.940633608s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:53 | 200 | 15.401712045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:58 | 200 | 13.598079754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:05 | 200 | 18.358865904s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:10 | 200 | 19.607475636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:16 | 200 | 18.574117631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:20 | 200 | 15.395077494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:22 | 200 | 12.484044914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:27 | 200 | 11.680039213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:31 | 200 |  8.450631445s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:36 | 200 | 13.503759118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:41 | 200 | 11.716549735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:46 | 200 | 14.450842359s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:49 | 200 |  13.85262526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:28 | 200 |  8.774898093s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:33 | 200 | 13.206703165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:37 | 200 | 17.927973633s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:39 | 200 | 20.342033089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:42 | 200 |  22.21457256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:48 | 200 | 19.557806857s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:52 | 200 | 17.841715583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:57 | 200 | 19.729448104s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:03 | 200 | 19.256804711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:07 | 200 | 18.533300824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:13 | 200 | 17.425380324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:18 | 200 | 16.558238713s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:23 | 200 | 16.167747365s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:28 | 200 | 15.356065848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:33 | 200 | 15.322131631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:39 | 200 |  19.57379423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:46 | 200 | 19.635540427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:52 | 200 | 20.130516001s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:55 | 200 | 17.321840116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:58 | 200 |   13.0492512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:03 | 200 | 11.186748629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:07 | 200 | 10.299995527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:12 | 200 | 13.871646491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:17 | 200 | 13.845208728s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:21 | 200 | 13.035069803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:24 | 200 | 15.369096098s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:30 | 200 | 17.542893575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:35 | 200 | 20.432915926s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:38 | 200 | 16.149366275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:40 | 200 | 15.861542413s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:45 | 200 | 18.386874675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:49 | 200 | 16.659108719s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:52 | 200 | 13.111623521s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:55 | 200 |   9.65009377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:59 | 200 |   8.10616375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:03 | 200 |  5.951398645s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:06 | 200 |  2.416874995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:08 | 200 |   2.98837155s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:15 | 200 |  6.145255065s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:22 | 200 |  5.447703165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:28 | 200 |  6.364713884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:33 | 200 | 10.016432321s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:36 | 200 |  7.018340609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:40 | 200 |  5.321566941s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:46 | 200 |  5.172195764s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:53 | 200 |  5.421558338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:58 | 200 |  4.338256377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:05 | 200 |  4.041730428s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:11 | 200 |  4.098150121s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:19 | 200 |  7.788883869s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:23 | 200 |  8.890635306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:27 | 200 |  6.333797729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:36 | 200 |   7.80441567s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:41 | 200 |  7.584178664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:44 | 200 |  5.038071217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:50 | 200 |  3.764449747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:58 | 200 |  5.180336642s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:12.630-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11467 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11467 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:12.630-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:12.631-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:12.631-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11467 (version 0.12.9)"
time=2025-11-10T02:08:12.631-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:12.632-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40309"
time=2025-11-10T02:08:12.880-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42971"
time=2025-11-10T02:08:13.065-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32799"
time=2025-11-10T02:08:13.065-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37843"
time=2025-11-10T02:08:13.285-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:14 | 200 |      59.321µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:14 | 200 |   45.252129ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     585.759µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:50.701-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38123"
time=2025-11-10T02:09:51.219-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:51.219-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11467/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 39527"
time=2025-11-10T02:09:51.225-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:51.225-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="85.7 GiB" free_swap="0 B"
time=2025-11-10T02:09:51.225-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="13.3 GiB" free="13.8 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:51.258-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:51.258-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:39527"
time=2025-11-10T02:09:51.268-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:51.406-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:51.551-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:52.633-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:52.974-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:52.974-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:52.970-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:52.971-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:52.971-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:52.971-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:52.975-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:53.478-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.25 seconds"
[GIN] 2025/11/10 - 02:09:56 | 200 |  6.301815594s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:09:57.104-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8658 keep=4 new=4096
[GIN] 2025/11/10 - 02:09:57 | 200 |  4.185470171s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:00 | 200 |  3.733789083s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:01.051-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8767 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:01 | 200 |  3.813977887s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:02.562-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=8770 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:03 | 200 |  1.434897476s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:12 | 200 |  803.940562ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:14 | 200 |  1.499211773s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:11 | 200 |  663.104225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:12 | 200 |  1.244644694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:13 | 200 |  1.843199287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:15 | 200 |  2.838728847s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:17 | 200 |   2.67808784s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:20 | 200 |  2.692779006s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:24 | 200 |  3.595010399s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:13 | 200 |   2.67145613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:20 | 200 |  8.321912909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:30 | 200 | 17.749505097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:35 | 200 | 22.838576431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:40 | 200 | 28.279760201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:44 | 200 |  31.14663206s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:48 | 200 | 35.335291495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:51 | 200 | 38.645454465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:56 | 200 | 43.118897272s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:12:59.684-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:12:59 | 500 | 45.044891161s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:00.419-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:00 | 500 | 44.980367036s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:00.702-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:00 | 500 | 45.114172486s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:01.858-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:01 | 500 | 45.050371674s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:01 | 200 | 47.429912534s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:02 | 500 | 45.082603554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:03 | 500 | 45.121059958s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:04.387-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:05.105-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:06 | 200 |  40.32533375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:11 | 200 |   40.3640485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:17 | 200 | 42.333660903s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:21 | 200 | 40.416672831s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:24 | 200 | 39.073105468s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:27 | 200 | 37.851969384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:31 | 200 | 39.034935097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:35 | 200 | 38.150103311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:40 | 200 | 40.296236954s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:44 | 200 |  44.40147083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:45 | 500 | 45.044778011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:45 | 500 | 45.044056687s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:46.702-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:47 | 500 | 45.026075383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:48 | 500 | 45.041602596s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:48.492-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:48 | 500 | 45.467151344s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:50.587-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:51.036-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:52 | 500 | 45.117981599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:56 | 200 | 44.216156465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:01 | 200 | 43.566109238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:07 | 200 | 45.756821159s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:09 | 500 | 45.078137769s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:11.327-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:12 | 500 | 45.050633642s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:13.209-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:17 | 200 | 45.953751454s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.468-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 11.630128613s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.469-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 13.841814274s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.469-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.118341063s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.470-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 28.708650685s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.470-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.809799753s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.470-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  8.504944735s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.474-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 36.519531669s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.740513666s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 19.577270631s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  4.179726192s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.049005432s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 24.517064806s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.475-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 41.137440901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 35.075603252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:22 | 200 | 46.230595283s |       127.0.0.1 | POST     "/api/generate"
