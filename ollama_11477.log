time=2025-11-09T23:15:09.785-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:15:09.786-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:15:09.786-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:15:09.786-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-09T23:15:09.786-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:15:09.787-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36183"
time=2025-11-09T23:15:09.978-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40393"
time=2025-11-09T23:15:10.155-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44433"
time=2025-11-09T23:15:10.155-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38499"
time=2025-11-09T23:15:10.373-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:15:14 | 200 |      59.412µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:15:14 | 404 |     205.795µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:15:14 | 200 |      23.184µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:15:15.303-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:15:18.520-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:15:19.744-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:15:20.987-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:15:22.206-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:15:23.469-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:15:24 | 200 |  9.888844789s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |     397.936µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:28:32.786-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37827"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11477/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:28:33.693-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11477/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 46727"
time=2025-11-09T23:28:33.697-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="88.5 GiB" free_swap="0 B"
time=2025-11-09T23:28:33.697-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11477/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:28:33.698-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[8.4 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:28:33.716-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:28:33.992-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:28:33.993-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:46727"
time=2025-11-09T23:28:33.995-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:28:33.995-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:33.995-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 8979611648 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 8563 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11477/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:28:36.254-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.56 seconds"
time=2025-11-09T23:28:36.254-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:28:36.254-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:36.254-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.56 seconds"
[GIN] 2025/11/09 - 23:28:36 | 200 |   3.78690539s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |  2.145642224s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |  727.218481ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |  441.474741ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  402.807654ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:40 | 200 |  502.750634ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:40 | 200 |   254.96056ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:40 | 200 |  252.597215ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |  810.594384ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:42 | 200 |  1.805702112s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  1.137064508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  762.616225ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  488.809193ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |  444.197367ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |  416.233987ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:53 | 200 |  250.211034ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  338.491159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  240.422535ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:54 | 200 |  308.883611ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:55 | 200 |  334.407886ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:55 | 200 |  483.837105ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:07 | 200 |  359.364604ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:08 | 200 |   647.27397ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:08 | 200 |  713.478275ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:10 | 200 |  1.997386153s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  1.982653883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  159.306683ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  178.839773ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  370.834125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  386.759155ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  252.780078ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:19 | 200 |  249.980666ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:20 | 200 |  145.175393ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:20 | 200 |  131.078064ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:32 | 200 |  105.197953ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:32 | 200 |  128.096782ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:35 | 200 |  159.987668ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  253.520258ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  712.290868ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:43 | 200 |  220.352228ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:43 | 200 |   252.06433ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:43 | 200 |  130.948056ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:40:29.425-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:40:29.426-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:40:29.426-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:40:29.426-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-09T23:40:29.426-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:40:29.426-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37235"
time=2025-11-09T23:40:29.636-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40869"
time=2025-11-09T23:40:29.837-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42979"
time=2025-11-09T23:40:29.837-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33951"
time=2025-11-09T23:40:30.078-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:40:34 | 200 |      39.414µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:40:34 | 404 |     240.631µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:40:34 | 200 |      19.436µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:40:34.940-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:40:38.160-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:40:39.388-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:40:40.603-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:40:41.811-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:40:43 | 200 |  8.610560428s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     712.488µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:50:49.814-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:50:49.815-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:50:49.815-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:50:49.815-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-09T23:50:49.815-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:50:49.816-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42579"
time=2025-11-09T23:50:50.015-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34957"
time=2025-11-09T23:50:50.218-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38161"
time=2025-11-09T23:50:50.218-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41807"
time=2025-11-09T23:50:50.737-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:50:54 | 200 |      59.482µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:50:54 | 200 |   44.321123ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     583.526µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:58:57.005-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:58:57.006-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:58:57.006-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:58:57.006-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-09T23:58:57.006-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:58:57.006-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35217"
time=2025-11-09T23:58:57.199-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41065"
time=2025-11-09T23:58:57.392-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42525"
time=2025-11-09T23:58:57.392-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34211"
time=2025-11-09T23:58:57.617-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:59:02 | 200 |       31.54µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:59:02 | 200 |   45.224694ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     655.421µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:06:16.189-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:06:16.189-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:06:16.189-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:06:16.189-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:06:16.190-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:06:16.191-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40323"
time=2025-11-10T00:06:16.408-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33061"
time=2025-11-10T00:06:16.606-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46107"
time=2025-11-10T00:06:16.606-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45699"
time=2025-11-10T00:06:16.843-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:06:21 | 200 |      38.983µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:06:21 | 200 |   51.202805ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |     311.946µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:12:31.035-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:12:31.035-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:12:31.035-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:12:31.036-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:12:31.036-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:12:31.036-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45191"
time=2025-11-10T00:12:31.330-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41819"
time=2025-11-10T00:12:31.523-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43199"
time=2025-11-10T00:12:31.523-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40659"
time=2025-11-10T00:12:31.756-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:12:36 | 200 |      30.818µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:12:36 | 200 |   51.023536ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:07 | 200 |     498.847µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:19:09.170-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:19:09.170-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:19:09.170-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:19:09.170-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:19:09.171-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:19:09.171-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41449"
time=2025-11-10T00:19:09.396-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46383"
time=2025-11-10T00:19:09.612-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44017"
time=2025-11-10T00:19:09.612-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39945"
time=2025-11-10T00:19:09.862-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:19:14 | 200 |      46.647µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:19:14 | 200 |   54.609049ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     490.962µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:27:32.474-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:27:32.474-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:27:32.475-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:27:32.475-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:27:32.475-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:27:32.476-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42549"
time=2025-11-10T00:27:32.697-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43405"
time=2025-11-10T00:27:32.913-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45401"
time=2025-11-10T00:27:32.913-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46199"
time=2025-11-10T00:27:33.176-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:27:37 | 200 |      44.604µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:27:37 | 200 |   59.623006ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |      463.07µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:38:30.427-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:38:30.428-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:38:30.428-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:38:30.428-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:38:30.428-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:38:30.429-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44105"
time=2025-11-10T00:38:30.860-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42259"
time=2025-11-10T00:38:31.050-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43859"
time=2025-11-10T00:38:31.050-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38623"
time=2025-11-10T00:38:31.285-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:38:35 | 200 |      33.182µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:38:35 | 200 |   49.317451ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     408.568µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:49:04.753-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:49:04.754-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:49:04.754-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:49:04.754-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T00:49:04.754-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:49:04.755-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34527"
time=2025-11-10T00:49:04.956-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42987"
time=2025-11-10T00:49:05.155-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41781"
time=2025-11-10T00:49:05.155-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35831"
time=2025-11-10T00:49:05.378-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:49:09 | 200 |      30.257µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:49:09 | 200 |   47.329217ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     346.982µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:09:04.295-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:09:04.295-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:09:04.295-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:09:04.296-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T01:09:04.296-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:09:04.298-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46035"
time=2025-11-10T01:09:04.497-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46177"
time=2025-11-10T01:09:04.706-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40879"
time=2025-11-10T01:09:04.706-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39279"
time=2025-11-10T01:09:04.921-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:09:09 | 200 |      39.184µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:09:09 | 200 |   57.893643ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     612.922µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:16:15.169-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:16:15.169-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:16:15.169-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:16:15.169-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T01:16:15.170-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:16:15.170-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38043"
time=2025-11-10T01:16:15.374-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38553"
time=2025-11-10T01:16:15.570-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45061"
time=2025-11-10T01:16:15.571-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43327"
time=2025-11-10T01:16:15.787-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:16:20 | 200 |      35.116µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:16:20 | 200 |   45.083898ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:51 | 200 |     359.546µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:08.272-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:40:08.273-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:40:08.273-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:40:08.274-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T01:40:08.274-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:40:08.275-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39317"
time=2025-11-10T01:40:08.683-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44999"
time=2025-11-10T01:40:08.882-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42857"
time=2025-11-10T01:40:08.882-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45679"
time=2025-11-10T01:40:09.103-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:40:10 | 200 |      50.936µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:40:10 | 200 |   44.102301ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     515.128µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:29.340-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37761"
time=2025-11-10T01:41:30.403-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:30.403-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11477/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 46879"
time=2025-11-10T01:41:30.408-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:30.408-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="83.5 GiB" free_swap="0 B"
time=2025-11-10T01:41:30.408-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="10.7 GiB" free="11.1 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:30.432-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:30.432-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:46879"
time=2025-11-10T01:41:30.441-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:30.505-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:30.848-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:31.978-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:32.277-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:32.277-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:32.278-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:32.278-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:32.278-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:32.288-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:33.043-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.63 seconds"
[GIN] 2025/11/10 - 01:41:34 | 200 |  5.348744521s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:36 | 200 |  2.272808355s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:38 | 200 |  1.954649423s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:41.523-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=5001 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:42 | 200 |  1.703335735s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:42.815-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=6629 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:43 | 200 |  2.213691072s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:44.020-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=5120 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:44 | 200 |  2.291314637s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:45.075-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=6748 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:45 | 200 |  1.937822147s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:04 | 200 |   2.42757814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:06 | 200 |  1.921654883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:13 | 200 |  3.269083436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:19 | 200 |  7.729608711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:25 | 200 | 10.769761424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:26 | 200 |  12.24427313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:31 | 200 | 14.423227132s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:34 | 200 | 16.821588078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:40 | 200 | 20.625094978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:44 | 200 | 18.813679416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:48 | 200 | 21.036151157s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:51 | 200 | 20.173457216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:55 | 200 | 20.741542533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:00 | 200 | 18.458555144s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:04 | 200 | 20.281929735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:08 | 200 | 18.243742512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:13 | 200 | 16.951466942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:18 | 200 | 22.377716924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:23 | 200 | 22.537854578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:27 | 200 | 22.776042066s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:31 | 200 | 21.821696449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:34 | 200 | 20.588859371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:36 | 200 | 17.949761058s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:40 | 200 | 16.394305883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:43 | 200 | 15.611218484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:46 | 200 | 14.786318462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:50 | 200 | 16.104457073s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:54 | 200 | 17.093959437s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:59 | 200 | 18.727043246s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:02 | 200 | 18.337348083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:05 | 200 | 18.917042601s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:12 | 200 | 20.654948427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:18 | 200 | 23.975009497s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:22 | 200 | 22.055796381s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:27 | 200 | 24.657239846s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:31 | 200 | 26.014047698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:35 | 200 |  22.18492141s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:42 | 200 | 23.726244641s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:46 | 200 | 24.185617149s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:53 | 200 | 25.754249985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:58 | 200 |  26.29034022s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:05 | 200 | 29.291360499s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:08 | 200 | 24.846822491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:10 | 200 | 24.102147964s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:14 | 200 | 20.676700916s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:19 | 200 | 20.598892303s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:24 | 200 | 18.251668584s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:27 | 200 | 19.044603373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:32 | 200 | 20.245639627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:35 | 200 | 21.106311491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:41 | 200 | 21.032245272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:43 | 200 |  19.30180896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:48 | 200 | 21.082919787s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:54 | 200 | 21.910064911s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:00 | 200 |  21.73946742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:05 | 200 | 20.461324247s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:10 | 200 | 19.173890258s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:14 | 200 | 17.571207107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:18 | 200 |  14.92971603s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:22 | 200 | 12.908170839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:28 | 200 | 13.151903475s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:31 | 200 | 15.320145009s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:37 | 200 | 14.615240794s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:42 | 200 | 19.233618992s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:47 | 200 |  17.89587608s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:49 | 200 | 13.585792018s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:53 | 200 | 10.269081192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:32 | 200 |  9.109018605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:37 | 200 |  12.47374198s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:43 | 200 | 18.036931415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:47 | 200 | 22.518908438s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:52 | 200 | 26.499426597s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:56 | 200 | 22.952689261s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:59 | 200 |  21.50965924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:05 | 200 | 21.853818979s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:07 | 200 | 19.795874632s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:13 | 200 | 20.577555033s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:17 | 200 | 21.460626042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:21 | 200 |  21.34784021s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:25 | 200 | 19.386647763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:30 | 200 | 21.729415609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:35 | 200 | 22.018721517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:40 | 200 | 21.874685044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:41 | 200 | 19.946839481s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:46 | 200 | 20.969885969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:50 | 200 | 20.089821723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:55 | 200 | 19.610811622s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:00 | 200 | 20.129879919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:06 | 200 | 21.777625245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:09 | 200 | 18.639302111s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:12 | 200 | 15.032206669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:17 | 200 | 16.338227464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:20 | 200 |  16.48232337s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:24 | 200 | 14.052140932s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:27 | 200 | 10.127495073s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:30 | 200 |  6.118610615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:33 | 200 |  4.636863285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:36 | 200 |   6.36137189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:41 | 200 |  5.417994052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:02 | 200 | 20.473663132s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:07 | 200 | 19.567115354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:11 | 200 | 16.984209723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:16 | 200 | 14.355877046s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:20 | 200 | 12.892432708s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:24 | 200 |  9.920413283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:29 | 200 |  7.931159317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:36 | 200 | 15.490370268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:40 | 200 | 13.100345738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:47 | 200 | 14.051457688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:50 | 200 | 10.424883158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 |  8.914055724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:58 | 200 |  7.733582134s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:03 | 200 | 12.942786519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:07 | 200 | 11.282458636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:10 | 200 |  7.268901161s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:16 | 200 |  9.038936338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:21 | 200 | 11.418042318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:25 | 200 | 14.396216636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:29 | 200 | 14.168496763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:35 | 200 | 13.174844263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:38 | 200 |  9.631703929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:43 | 200 |  9.229747383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:50 | 200 |  14.51259768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:53 | 200 | 12.670765233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:59 | 200 | 14.992044367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:03 | 200 |  16.02975544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:05 | 200 | 11.628059095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:07 | 200 | 13.377053637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:11 | 200 | 11.709243089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:15 | 200 | 14.765587223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:21 | 200 | 17.887990641s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:25 | 200 | 17.317600392s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:27 | 200 | 13.449794361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:33 | 200 | 17.102302819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:35 | 200 | 13.764189122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:42 | 200 |  15.85214935s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:47 | 200 | 19.074628566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:53 | 200 |  23.70625085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:59 | 200 | 22.841982121s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:02 | 200 | 25.751779723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:05 | 200 | 23.330978034s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:10 | 200 | 21.120783289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:15 | 200 | 17.931973042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:19 | 200 | 16.995959163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:23 | 200 | 18.460622954s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:29 | 200 | 23.101939118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:34 | 200 | 23.275675902s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:38 | 200 | 23.051025927s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:42 | 200 | 21.237992958s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:46 | 200 | 22.409826848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:50 | 200 | 20.818094854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:54 | 200 |  19.53941535s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:59 | 200 | 19.829668515s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:03 | 200 | 21.123397059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:08 | 200 | 21.256310101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:12 | 200 | 21.666227443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:16 | 200 |  22.04294631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:20 | 200 | 21.034006599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:22 | 200 |  18.29326676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:29 | 200 | 20.275376866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:35 | 200 | 22.732405912s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:44 | 200 | 27.832937443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:50 | 200 | 29.706335091s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:53 | 200 | 30.514705789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:58 | 200 | 28.530381998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:01 | 200 | 24.674361573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:04 | 200 | 19.063861189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:10 | 200 | 19.539622617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:13 | 200 | 19.477377065s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:20 | 200 | 21.840753121s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:26 | 200 | 24.692564619s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:30 | 200 | 25.826554123s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:36 | 200 | 25.886685666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:42 | 200 | 27.803501846s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:46 | 200 |  25.87365954s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:51 | 200 | 24.442000582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:55 | 200 |  24.10383093s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:58 | 200 | 22.248450217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:03 | 200 | 21.282629031s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:07 | 200 | 19.124754039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:12 | 200 | 20.792097739s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:16 | 200 | 21.160690506s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:20 | 200 | 21.247499519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:26 | 200 | 21.961291204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:29 | 200 | 22.033908224s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:34 | 200 | 22.356273877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:38 | 200 | 22.156438966s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:45 | 200 | 24.159545973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:49 | 200 | 22.786858383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:51 | 200 | 20.985167536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:54 | 200 | 19.532729289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:58 | 200 | 19.388161735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:01 | 200 | 15.670101812s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:05 | 200 | 15.893567601s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:09 | 200 |  17.36367081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:13 | 200 | 18.858986485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:18 | 200 |  19.79298299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:24 | 200 | 22.513452005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:29 | 200 | 23.518311239s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:35 | 200 |  25.99286143s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:42 | 200 | 28.291990373s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:48 | 200 | 29.623882183s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:52 | 200 | 27.882804218s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:59 | 200 | 29.248778247s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:05 | 200 | 28.640705828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:10 | 200 | 27.562800574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:14 | 200 | 24.903096282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:17 | 200 | 24.629852199s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:25 | 200 | 25.504950295s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:32 | 200 | 27.288358121s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:38 | 200 | 27.046647797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:40 | 200 | 25.864154832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:45 | 200 | 26.699152877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:47 | 200 |  22.01476419s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:52 | 200 | 19.864020394s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:58 | 200 | 19.886772539s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:04 | 200 | 24.204160411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:12 | 200 | 27.538285804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:18 | 200 | 29.791613506s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:20 | 200 | 27.455464998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:23 | 200 |   24.4877158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:26 | 200 | 21.004797777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:29 | 200 | 16.490216914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:31 | 200 | 13.616105896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:35 | 200 | 14.216113709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:39 | 200 | 16.231774697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:44 | 200 | 17.547368233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:47 | 200 | 17.734605687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:52 | 200 | 20.044605458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:55 | 200 | 19.957505734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:58 | 200 | 17.951371397s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:02 | 200 | 18.369557111s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:09 | 200 | 21.702123295s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:15 | 200 | 22.850117319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:21 | 200 | 24.821607854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:28 | 200 | 29.401167358s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:32 | 200 | 29.905598188s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:38 | 200 | 28.213803699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:43 | 200 | 27.677313836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:48 | 200 | 26.648605371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:51 | 200 | 23.235215356s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:55 | 200 | 22.162317765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:00 | 200 | 21.796710891s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:07 | 200 |  23.08165329s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:11 | 200 | 22.410970441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:14 | 200 | 23.035300734s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:19 | 200 | 23.309565086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:22 | 200 | 21.764666546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:27 | 200 | 19.618484949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:30 | 200 | 18.122111698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:06 | 200 | 48.823637355s |       127.0.0.1 | POST     "/api/generate"
Error #01: write tcp 127.0.0.1:11477->127.0.0.1:54196: write: broken pipe
[GIN] 2025/11/10 - 02:02:06 | 200 | 42.681830205s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:07 | 200 | 36.501522607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:07 | 200 | 30.208388181s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:08 | 200 |   822.12148ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:08 | 200 |  921.846596ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:33.282-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11477 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11477 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:33.282-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:33.282-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:33.283-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11477 (version 0.12.9)"
time=2025-11-10T02:08:33.283-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:33.283-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44557"
time=2025-11-10T02:08:33.496-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41443"
time=2025-11-10T02:08:33.710-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35667"
time=2025-11-10T02:08:33.710-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35243"
time=2025-11-10T02:08:34.192-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:35 | 200 |      32.361µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:35 | 200 |   47.670141ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     582.243µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:54.476-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34995"
time=2025-11-10T02:09:54.977-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:54.978-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11477/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 40345"
time=2025-11-10T02:09:54.979-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:54.979-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="83.2 GiB" free_swap="0 B"
time=2025-11-10T02:09:54.979-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="10.7 GiB" free="11.2 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:55.040-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:55.040-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:40345"
time=2025-11-10T02:09:55.047-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:55.128-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:55.421-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:56.254-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:56.400-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:56.401-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:56.401-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:56.401-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:56.404-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:56.404-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:56.412-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:56.915-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.94 seconds"
[GIN] 2025/11/10 - 02:09:58 | 200 |  3.874283349s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:59 | 200 |  1.214477512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:00 | 200 |  1.460190043s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:03.600-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=5001 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:04 | 200 |  1.248660692s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:04.540-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=6629 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:05 | 200 |  1.542088972s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:05.293-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=5001 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:05 | 200 |  1.436181575s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:05.936-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=6629 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:06 | 200 |  1.176861875s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:06.391-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=5001 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:06 | 200 |  901.087376ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:06.816-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=6629 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:07 | 200 |  804.956468ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:19 | 200 |  1.093295788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:21 | 200 |  1.410117674s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:23 | 200 |  1.334408223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:24 | 200 |  1.164103808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:12 | 200 |  1.100210462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:14 | 200 |  1.539569974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:17 | 200 |  2.711555666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:28 | 200 |  1.517333417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:31 | 200 |   2.34062151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:28 | 200 | 11.190347884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:35 | 200 |   15.6200505s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:39 | 200 | 19.943749944s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:44 | 200 | 25.299740256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:49 | 200 | 29.130035459s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:53 | 200 | 33.593273062s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:57 | 200 | 32.495699266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:02 | 200 | 35.750323371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:09 | 200 | 42.714692246s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:15.051-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:15 | 500 | 44.840203258s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:16.295-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:16 | 500 | 45.049691759s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:16.295-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:16 | 500 | 45.062035003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:16 | 200 | 47.199264237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:22 | 200 | 46.504937043s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:26 | 200 | 46.478434123s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:30 | 200 |  45.33565236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:34 | 200 | 45.042966248s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:38 | 200 | 45.081133537s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:42 | 200 | 44.078316678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:46 | 200 | 42.715178429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:48 | 200 | 42.001886326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:52 | 200 | 43.156765102s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:56.411-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:56 | 500 | 45.045235265s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:57 | 200 | 47.085441837s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:59 | 500 | 45.049866144s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:59.994-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:00 | 500 | 45.028865522s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:01 | 500 |  45.04785129s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:01 | 500 | 45.042384242s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:02.365-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:03.061-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:03.837-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:07 | 200 | 46.578760292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:12 | 200 | 37.604123688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:14 | 200 | 31.514145315s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:18 | 200 | 32.451459611s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.445-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 12.170328914s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 31.989164054s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.458-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  23.09506849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 27.880917443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 |  4.711134471s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:22.221-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:22.337-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
