time=2025-11-09T23:15:24.704-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:15:24.704-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:15:24.704-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:15:24.704-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-09T23:15:24.705-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:15:24.705-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43305"
time=2025-11-09T23:15:25.158-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35551"
time=2025-11-09T23:15:25.335-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35929"
time=2025-11-09T23:15:25.335-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46869"
time=2025-11-09T23:15:25.544-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:15:29 | 200 |      30.227µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:15:29 | 404 |      143.88µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:15:29 | 200 |       16.43µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:15:30.228-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:15:33.453-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:15:34.664-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:15:35.913-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:15:37.140-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:15:38.367-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:15:39 | 200 |  9.874100075s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |      474.29µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:28:32.164-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46509"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11478/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:28:32.938-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11478/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 38851"
time=2025-11-09T23:28:32.939-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="89.0 GiB" free_swap="0 B"
time=2025-11-09T23:28:32.940-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11478/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:28:32.941-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[8.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:28:32.992-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:28:33.511-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:28:33.512-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:38851"
time=2025-11-09T23:28:33.515-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:33.515-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:28:33.518-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 8999403520 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 8582 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11478/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:28:35.526-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.59 seconds"
time=2025-11-09T23:28:35.527-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:28:35.527-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:35.527-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.59 seconds"
[GIN] 2025/11/09 - 23:28:36 | 200 |  3.812847155s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |  2.860666109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:37 | 200 |    5.7730377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  2.045952777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  746.974511ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  949.596514ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  233.562641ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  204.699432ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  218.748189ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  246.334361ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:39 | 200 |  151.299853ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:41 | 200 |  381.390782ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:43 | 200 |  1.439971864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:44 | 200 |  1.320303529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:46 | 200 |  3.159893558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:46 | 200 |  2.451239407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  2.432342322s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  277.151973ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  429.376655ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:47 | 200 |  393.808101ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:48 | 200 |  884.805576ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:48 | 200 |  956.436953ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:49 | 200 |   800.02862ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:55 | 200 |  595.192566ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  658.028677ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  149.461419ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  122.484339ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  219.067767ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  157.847569ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:58 | 200 |  134.840339ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:58 | 200 |  182.548778ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:01 | 200 |  211.340436ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:01 | 200 |  277.890338ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:02 | 200 |  305.534601ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:02 | 200 |  132.148188ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:02 | 200 |  158.422238ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:10 | 200 |  441.656935ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:10 | 200 |  376.840155ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  797.505323ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  837.498173ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:11 | 200 |  832.332348ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  813.729555ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |  602.317159ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:12 | 200 |   796.72489ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  931.345742ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:13 | 200 |  863.364623ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  1.073655136s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  1.186875583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  219.009436ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:14 | 200 |  233.768986ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  185.846365ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  188.482071ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:35 | 200 |  198.636758ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  359.205788ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  157.209955ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  277.551573ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  176.106252ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  421.047698ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:37 | 200 |  440.261761ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:40:43.067-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:40:43.067-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:40:43.067-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:40:43.067-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-09T23:40:43.068-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:40:43.068-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36023"
time=2025-11-09T23:40:43.276-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44411"
time=2025-11-09T23:40:43.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38507"
time=2025-11-09T23:40:43.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39583"
time=2025-11-09T23:40:43.968-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:40:48 | 200 |      75.522µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:40:48 | 404 |     331.663µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:40:48 | 200 |      23.464µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:40:48.649-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:40:51.889-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:40:53.120-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:40:54.349-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:40:55.569-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:40:56 | 200 |  8.725897139s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     531.658µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:50:54.877-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:50:54.878-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:50:54.878-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:50:54.878-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-09T23:50:54.878-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:50:54.879-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34277"
time=2025-11-09T23:50:55.079-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38693"
time=2025-11-09T23:50:55.292-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45667"
time=2025-11-09T23:50:55.292-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36105"
time=2025-11-09T23:50:55.519-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:50:59 | 200 |      34.084µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:50:59 | 200 |   48.758067ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     557.808µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:59:02.065-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:59:02.066-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:59:02.066-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:59:02.066-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-09T23:59:02.067-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:59:02.067-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44445"
time=2025-11-09T23:59:02.339-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46711"
time=2025-11-09T23:59:02.550-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38541"
time=2025-11-09T23:59:02.550-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33211"
time=2025-11-09T23:59:02.772-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:59:07 | 200 |      37.971µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:59:07 | 200 |   44.664872ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     586.742µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:06:21.258-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:06:21.258-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:06:21.259-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:06:21.259-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:06:21.259-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:06:21.260-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46031"
time=2025-11-10T00:06:21.548-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40243"
time=2025-11-10T00:06:21.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44127"
time=2025-11-10T00:06:21.746-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46759"
time=2025-11-10T00:06:21.971-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:06:26 | 200 |      42.761µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:06:26 | 200 |   45.684301ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |     605.307µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:12:36.103-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:12:36.104-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:12:36.105-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:12:36.105-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:12:36.105-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:12:36.105-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36767"
time=2025-11-10T00:12:36.560-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44275"
time=2025-11-10T00:12:36.742-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36243"
time=2025-11-10T00:12:36.742-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38315"
time=2025-11-10T00:12:36.961-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:12:41 | 200 |      39.174µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:12:41 | 200 |   49.085286ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:07 | 200 |     508.324µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:19:14.242-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:19:14.243-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:19:14.243-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:19:14.243-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:19:14.244-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:19:14.244-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41833"
time=2025-11-10T00:19:14.655-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44231"
time=2025-11-10T00:19:14.870-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46473"
time=2025-11-10T00:19:14.870-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40733"
time=2025-11-10T00:19:15.098-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:19:19 | 200 |      55.635µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:19:19 | 200 |   58.032688ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     551.867µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:27:37.552-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:27:37.553-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:27:37.553-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:27:37.553-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:27:37.553-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:27:37.554-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40385"
time=2025-11-10T00:27:37.789-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40815"
time=2025-11-10T00:27:38.018-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45579"
time=2025-11-10T00:27:38.018-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38649"
time=2025-11-10T00:27:38.264-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:27:42 | 200 |      47.489µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:27:42 | 200 |   50.799372ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     340.069µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:38:35.496-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:38:35.497-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:38:35.497-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:38:35.497-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:38:35.497-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:38:35.498-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43289"
time=2025-11-10T00:38:35.702-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35543"
time=2025-11-10T00:38:36.156-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41423"
time=2025-11-10T00:38:36.156-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39141"
time=2025-11-10T00:38:36.388-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:38:40 | 200 |      31.229µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:38:40 | 200 |   47.631724ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     538.602µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:49:09.819-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:49:09.819-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:49:09.819-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:49:09.820-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T00:49:09.820-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:49:09.821-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35561"
time=2025-11-10T00:49:10.035-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40469"
time=2025-11-10T00:49:10.235-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39695"
time=2025-11-10T00:49:10.235-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46859"
time=2025-11-10T00:49:10.459-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:49:14 | 200 |      44.053µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:49:14 | 200 |   48.688159ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     311.546µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:09:09.373-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:09:09.373-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:09:09.374-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:09:09.374-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T01:09:09.374-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:09:09.375-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41253"
time=2025-11-10T01:09:09.676-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36627"
time=2025-11-10T01:09:09.902-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35403"
time=2025-11-10T01:09:09.902-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41495"
time=2025-11-10T01:09:10.152-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:09:14 | 200 |      36.238µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:09:14 | 200 |   45.216033ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     583.336µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:16:20.229-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:16:20.230-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:16:20.230-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:16:20.230-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T01:16:20.230-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:16:20.231-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36501"
time=2025-11-10T01:16:20.487-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40487"
time=2025-11-10T01:16:20.678-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38761"
time=2025-11-10T01:16:20.678-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41821"
time=2025-11-10T01:16:20.898-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:16:25 | 200 |      52.759µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:16:25 | 200 |   45.642668ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:51 | 200 |     563.338µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:10.334-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:40:10.335-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:40:10.335-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:40:10.335-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T01:40:10.335-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:40:10.335-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34515"
time=2025-11-10T01:40:10.542-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40949"
time=2025-11-10T01:40:10.734-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36551"
time=2025-11-10T01:40:10.734-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36243"
time=2025-11-10T01:40:11.218-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:40:12 | 200 |      59.432µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:40:12 | 200 |   45.718939ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     588.465µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:51.681-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38569"
time=2025-11-10T01:40:52.025-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:40:52.025-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11478/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 43669"
time=2025-11-10T01:40:52.026-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:40:52.026-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="104.3 GiB" free_swap="0 B"
time=2025-11-10T01:40:52.026-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="22.2 GiB" free="22.6 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:40:52.044-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:40:52.044-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:43669"
time=2025-11-10T01:40:52.049-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:52.105-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:40:52.568-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:40:53.832-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:40:54.080-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:40:54.080-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:40:54.081-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:40:54.589-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.56 seconds"
[GIN] 2025/11/10 - 01:40:55 | 200 |  4.049498627s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:40:57 | 200 |  1.972173587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:05 | 200 |  587.974802ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:06 | 200 |  911.998266ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:07 | 200 |  1.243777013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:00 | 200 |  2.766139566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:02 | 200 |  2.226139456s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:05 | 200 |  1.908492171s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:07 | 200 |  2.318485469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:14 | 200 |  3.861880733s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:21 | 200 |   7.86876665s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:24 | 200 | 10.079367587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:30 | 200 | 14.994798114s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:33 | 200 | 17.817740828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:40 | 200 | 23.434176326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:45 | 200 | 23.458021103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:48 | 200 | 24.082688302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:52 | 200 | 21.729355305s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:57 | 200 | 22.834894625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:01 | 200 |  21.29142863s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:08 | 200 | 22.464663977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:11 | 200 | 22.257050154s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:15 | 200 | 22.627883082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:21 | 200 | 24.171297083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:26 | 200 | 23.857424503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:30 | 200 | 22.106334179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:34 | 200 | 22.446704806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:38 | 200 | 23.248644968s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:43 | 200 | 21.536073825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:46 | 200 | 19.744827656s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:50 | 200 | 19.622168723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:55 | 200 | 20.413368924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:58 | 200 | 19.403564101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:02 | 200 | 18.482844286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:06 | 200 | 19.105742682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:10 | 200 | 19.710045355s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:13 | 200 | 18.291735848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:17 | 200 | 18.676526249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:22 | 200 | 20.499672517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:26 | 200 | 20.163441487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:33 | 200 | 22.295268817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:35 | 200 | 21.758860695s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 | 23.441633453s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:48 | 200 | 25.539884313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:52 | 200 |  25.57479529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:57 | 200 | 23.352123237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:01 | 200 | 24.803294029s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:05 | 200 | 24.162089816s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:10 | 200 |  20.63849977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:15 | 200 | 21.335088237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:20 | 200 |  22.55073972s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:23 | 200 | 21.963068203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:26 | 200 | 20.935248569s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:31 | 200 | 20.454375501s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:34 | 200 | 19.316966862s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:37 | 200 | 16.921239407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:41 | 200 | 17.637202517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:46 | 200 | 18.986497431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:49 | 200 | 18.222667672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:53 | 200 | 18.822327523s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:58 | 200 |  19.81793041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:03 | 200 | 21.156311864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:08 | 200 | 22.082445059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:13 | 200 | 22.882418506s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:15 | 200 | 21.450994429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:20 | 200 | 21.694448145s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:26 | 200 | 23.034543636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:30 | 200 |  20.63714724s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:34 | 200 | 18.431178555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:38 | 200 | 22.465927665s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:43 | 200 | 22.905528338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:45 | 200 | 18.973427531s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:48 | 200 | 17.765008096s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:53 | 200 | 17.635607642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:56 | 200 | 17.424379555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:00 | 200 | 16.182386536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:02 | 200 | 13.603859955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:04 | 200 |  7.163323631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:06 | 200 |  6.881307884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:30 | 200 |  7.860716846s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:33 | 200 |  9.547635983s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:38 | 200 | 13.424995269s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:45 | 200 | 19.801364618s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:50 | 200 | 25.087903593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:54 | 200 |  23.43420755s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:59 | 200 | 24.840987103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:02 | 200 | 24.092487272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:07 | 200 | 22.168125637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:12 | 200 | 21.619943973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:16 | 200 | 21.412521242s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:20 | 200 | 21.121972393s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:25 | 200 | 21.660781257s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:28 | 200 | 20.289644028s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:32 | 200 | 20.219784566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:36 | 200 | 20.342047106s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:40 | 200 | 18.790077667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:43 | 200 |  18.08737451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:47 | 200 | 18.771415473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:50 | 200 | 17.527714451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:55 | 200 | 18.270510503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:00 | 200 | 19.941436898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:04 | 200 | 20.949163369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:10 | 200 | 22.507225125s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:13 | 200 | 22.939552268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:18 | 200 | 22.794401177s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:21 | 200 | 20.605072886s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:24 | 200 | 19.980170819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:29 | 200 | 18.191196374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:32 | 200 | 18.271940454s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:35 | 200 | 16.803826819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:39 | 200 | 18.777482417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:43 | 200 |  17.93611224s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:49 | 200 | 19.164461817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:53 | 200 | 17.590610711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:55 | 200 |  19.42663289s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:58 | 200 | 15.904755035s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:04 | 200 | 20.268838614s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:07 | 200 | 17.468825216s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:09 | 200 | 15.213716984s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:14 | 200 | 18.544675262s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:21 | 200 |  19.08141522s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:25 | 200 | 17.109541815s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:30 | 200 | 21.239159063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:34 | 200 | 24.257676076s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:39 | 200 | 24.115241821s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:49 | 200 |  27.49948697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 | 27.222360326s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:59 | 200 | 25.990540674s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:02 | 200 | 22.540159989s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:05 | 200 | 20.071517939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:09 | 200 |  19.57091161s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:14 | 200 | 20.104088324s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:20 | 200 |  21.09380312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:23 | 200 | 20.397976697s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:28 | 200 | 22.360284306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:33 | 200 | 23.101763263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:40 | 200 | 25.060101639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:44 | 200 | 23.781988617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:48 | 200 | 25.374938699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:53 | 200 | 24.610769299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:59 | 200 | 25.472976653s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:04 | 200 | 23.495693065s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:08 | 200 | 23.199338969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:11 | 200 | 22.754705455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:18 | 200 | 25.512090513s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:22 | 200 |  23.36255921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:25 | 200 | 21.667107997s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:30 | 200 | 21.995654037s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:34 | 200 | 22.434696747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:38 | 200 | 19.222318783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:42 | 200 | 19.069986913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:47 | 200 |  21.54923858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:52 | 200 | 21.658168036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:56 | 200 | 21.004228027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:01 | 200 |  22.51592884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:07 | 200 | 24.614387087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:12 | 200 | 24.348742786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:17 | 200 | 24.023335818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:21 | 200 | 24.169835131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:25 | 200 | 23.757282238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:29 | 200 | 22.470722888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:32 | 200 | 19.768696558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:36 | 200 | 19.599265201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:42 | 200 | 20.948493673s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:49 | 200 |  23.85530633s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:54 | 200 | 24.663378145s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:58 | 200 | 26.229186392s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:02 | 200 | 25.427588929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:08 | 200 |  25.49266681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:12 | 200 | 21.798914807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:15 | 200 | 20.196797865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:20 | 200 |  21.34074423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:24 | 200 | 22.021685768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:31 | 200 | 23.223939147s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:36 | 200 | 24.037466648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:42 | 200 |  26.81945443s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:47 | 200 | 26.664872701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:51 | 200 | 26.314851528s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:55 | 200 | 22.429201012s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:59 | 200 | 22.659403364s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:03 | 200 | 20.371412744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:09 | 200 | 22.393908831s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:14 | 200 | 22.740905361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:19 | 200 | 24.273561854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:24 | 200 | 24.017131669s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:29 | 200 | 25.735045605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:31 | 200 | 21.078473948s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:33 | 200 | 18.625396645s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:37 | 200 |  16.87237514s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:42 | 200 | 18.106771642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:46 | 200 |  16.55098174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:49 | 200 | 18.222736505s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:52 | 200 | 18.746571659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:57 | 200 | 20.495429217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:02 | 200 | 19.929054434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:09 | 200 | 23.441264766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:15 | 200 | 25.089557602s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:19 | 200 | 26.715300432s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:24 | 200 | 26.021294622s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:27 | 200 | 24.582656633s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:32 | 200 | 21.906229411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:38 | 200 | 22.814863815s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:41 | 200 |   21.4335823s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:45 | 200 | 20.907249813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:49 | 200 | 21.281934082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:53 | 200 | 21.479064411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:00 | 200 | 21.709707637s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:07 | 200 | 25.147259172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:14 | 200 | 28.710407578s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:19 | 200 | 29.901783087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:23 | 200 | 29.949922998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:28 | 200 | 27.549130388s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:30 | 200 | 23.117470909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:37 | 200 | 22.714861212s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:41 | 200 | 20.806178543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:45 | 200 | 21.319039128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:50 | 200 | 22.181620458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:55 | 200 | 24.489753099s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:02 | 200 | 24.809158115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:06 | 200 |   24.4297899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:08 | 200 | 22.556044907s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:12 | 200 | 21.835467819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:17 | 200 | 21.622088693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:22 | 200 | 19.924041731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:25 | 200 | 18.795393647s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:30 | 200 | 21.220393186s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:34 | 200 | 21.030608282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:38 | 200 | 20.438459229s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:42 | 200 | 19.497342879s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:46 | 200 | 20.989318626s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:51 | 200 | 20.837306387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:56 | 200 | 21.534665703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:00 | 200 | 21.519083421s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:05 | 200 | 22.277941809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:09 | 200 | 22.498262486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:16 | 200 | 24.937277583s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:22 | 200 | 25.776220193s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:28 | 200 | 27.661096287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:34 | 200 | 28.541498138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:39 | 200 | 28.896982547s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:44 | 200 | 27.026581947s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:48 | 200 | 25.511459884s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:54 | 200 | 25.801504078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:00 | 200 | 25.892479675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:04 | 200 |  24.71232972s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:06 | 200 |  22.31561643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:09 | 200 | 20.519390937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:13 | 200 | 19.279693119s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:20 | 200 | 19.967056811s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:27 | 200 | 22.512403285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:31 | 200 | 24.461636967s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:34 | 200 | 24.526394243s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:39 | 200 | 25.193434374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:44 | 200 | 23.685332226s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:47 | 200 | 19.612451736s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:49 | 200 | 17.014904312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:53 | 200 | 18.758246677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:57 | 200 | 17.752044711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:01 | 200 | 15.298482487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:06 | 200 | 19.061368595s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:09 | 200 | 19.436433296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:13 | 200 | 19.436966042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:17 | 200 | 20.572403825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:24 | 200 | 22.448797711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:28 | 200 | 21.540556135s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:34 | 200 | 24.083471653s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:37 | 200 | 24.553963924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:42 | 200 | 24.165816423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:46 | 200 | 21.651868286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:49 | 200 | 18.385163749s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:51 | 200 | 13.513654115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:54 | 200 |  4.908228441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:29 | 200 | 11.411377468s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:34 | 200 | 16.127163051s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:38 | 200 | 18.440561382s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:44 | 200 | 25.455680922s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:50 | 200 | 30.236389192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:54 | 200 | 24.336033703s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:58 | 200 | 23.215875041s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:01 | 200 | 22.669147727s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:05 | 200 | 20.644503299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:10 | 200 | 19.818643463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:13 | 200 |  19.02981434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:20 | 200 | 21.587183677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:24 | 200 | 21.947641682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:28 | 200 | 23.197451821s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:32 | 200 | 21.972904665s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:37 | 200 | 22.529978055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:41 | 200 | 20.128754573s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:49 | 200 | 24.837754546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:54 | 200 |  25.00269607s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:59 | 200 | 27.001572831s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:04 | 200 | 26.730589554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:10 | 200 | 27.806017954s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:14 | 200 | 24.204891596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:19 | 200 | 24.735515663s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:23 | 200 | 22.832568056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:27 | 200 | 23.445520446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:31 | 200 | 21.362515655s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:35 | 200 | 21.177784843s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:41 | 200 | 22.401141128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:46 | 200 | 22.918135279s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:50 | 200 | 22.207589901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:54 | 200 | 22.656260097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:57 | 200 | 21.157490737s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:00 | 200 | 18.765921056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:05 | 200 | 18.256586987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:10 | 200 | 18.904723977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:14 | 200 | 19.448544173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:20 | 200 | 22.423869866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:25 | 200 | 23.868106995s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:28 | 200 | 23.283542327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:32 | 200 | 22.294710543s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:36 | 200 | 21.326537961s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:39 | 200 | 19.099011088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:44 | 200 | 18.597382262s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:48 | 200 | 19.039668182s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:52 | 200 | 19.288943319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:57 | 200 | 20.744036836s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:03 | 200 | 23.230986471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:07 | 200 | 22.894623177s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:12 | 200 | 23.862473773s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:16 | 200 | 23.901049486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:20 | 200 |  23.15246789s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:25 | 200 | 21.539311288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:31 | 200 | 23.427579352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:36 | 200 | 22.847635392s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:41 | 200 | 24.597703684s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:47 | 200 | 26.043875486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:52 | 200 |  27.01190986s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:58 | 200 | 25.806326736s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.019-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:07:00 | 500 | 12.494554473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 500 | 23.531982798s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.028-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:07:00 | 500 |  6.990399974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 500 | 18.393375318s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.127-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:08:35.344-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11478 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11478 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:35.344-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:35.344-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:35.344-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11478 (version 0.12.9)"
time=2025-11-10T02:08:35.345-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:35.345-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40075"
time=2025-11-10T02:08:35.556-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42455"
time=2025-11-10T02:08:35.760-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41143"
time=2025-11-10T02:08:35.760-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46417"
time=2025-11-10T02:08:35.981-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:37 | 200 |      60.724µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:37 | 200 |   45.112911ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     502.473µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:17.719-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33845"
time=2025-11-10T02:09:18.171-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:18.171-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11478/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 42029"
time=2025-11-10T02:09:18.171-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:18.171-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="104.3 GiB" free_swap="0 B"
time=2025-11-10T02:09:18.172-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="22.1 GiB" free="22.6 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:18.187-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:18.187-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:42029"
time=2025-11-10T02:09:18.196-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:18.246-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:18.941-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:19.750-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:20.088-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:20.088-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:20.590-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.42 seconds"
[GIN] 2025/11/10 - 02:09:22 | 200 |  4.509245596s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:24 | 200 |  2.092327485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:30 | 200 |  497.374644ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:31 | 200 |  805.421327ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:33 | 200 |  1.139286403s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:14 | 200 |   1.15135579s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:17 | 200 |  1.960861683s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:21 | 200 |  1.089140122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:23 | 200 |   1.19048743s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:15 | 200 |   1.70691462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:17 | 200 |  2.396023668s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:20 | 200 |  2.509054864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:21 | 200 |  2.802604469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:24 | 200 |  2.406557148s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:27 | 200 | 12.953168478s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:34 | 200 |  18.51332374s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:38 | 200 | 22.762820214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:43 | 200 | 26.560594616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:48 | 200 | 31.201955156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:52 | 200 |  32.79163166s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:55 | 200 | 34.998636557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:59 | 200 |   38.0769788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:03 | 200 | 42.437671541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 44.609368225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:07 | 500 |  45.03492983s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:08.794-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:10 | 500 | 45.058697252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:12 | 500 | 45.047664884s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:13.110-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:14 | 500 | 45.040155234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.161327867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.064322798s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:16.558-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:17.548-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:20 | 200 | 45.523310107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:22 | 200 | 42.917393925s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:26 | 200 | 42.497466163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:30 | 200 | 41.949270754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:35 | 200 | 42.388782796s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:39 | 200 | 43.240627524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:42 | 200 | 43.091956211s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:47 | 200 | 43.578736465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:51 | 500 | 45.047757955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:53 | 500 | 45.050612222s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:55.328-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:57 | 200 | 34.775656847s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:02 | 200 | 36.205901769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:09 | 200 | 38.311007894s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:15 | 200 | 40.143145956s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 200 | 42.139340306s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.441-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 12.334353667s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 |  38.18191884s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 22.859099008s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 29.671099293s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 30.438910534s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.261503338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 |  4.828598774s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:22.602-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:22.632-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
