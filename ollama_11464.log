time=2025-11-09T23:11:55.879-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:11:55.879-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:11:55.879-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:11:55.879-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-09T23:11:55.879-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:11:55.880-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40019"
time=2025-11-09T23:11:56.063-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35397"
time=2025-11-09T23:11:56.260-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35009"
time=2025-11-09T23:11:56.260-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46783"
time=2025-11-09T23:11:56.469-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:12:00 | 200 |      31.099µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:12:00 | 404 |     304.262µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:12:00 | 200 |      18.535µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:12:01.378-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:12:04.597-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:12:05.815-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:12:07.044-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:12:08.270-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:12:09.488-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:12:10 | 200 |  9.829767848s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |     278.152µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:24:27.028-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41659"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11464/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:24:28.688-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11464/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 41567"
time=2025-11-09T23:24:28.689-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="102.1 GiB" free_swap="0 B"
time=2025-11-09T23:24:28.690-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11464/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:24:28.690-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[20.8 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:24:28.709-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:24:29.705-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:24:29.705-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:41567"
time=2025-11-09T23:24:29.712-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:24:29.712-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:29.713-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 21437743104 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 20444 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11464/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:24:34.479-07:00 level=INFO source=server.go:1289 msg="llama runner started in 5.79 seconds"
time=2025-11-09T23:24:34.479-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:24:34.479-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:24:34.479-07:00 level=INFO source=server.go:1289 msg="llama runner started in 5.79 seconds"
[GIN] 2025/11/09 - 23:24:34 | 200 |  7.781415538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:34 | 200 |  7.832374659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:34 | 200 |   7.25701037s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:35 | 200 |  865.504557ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  1.254626079s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  1.068032091s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  422.482949ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  194.967409ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:36 | 200 |  764.378524ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:37 | 200 |  1.031759316s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:38 | 200 |  1.824491677s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:38 | 200 |  1.410392406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:38 | 200 |  1.160917709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:38 | 200 |  378.426491ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |   575.81313ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |  966.669422ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:39 | 200 |  979.045621ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |  215.598437ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:24:40 | 200 |   210.38161ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:00 | 200 |  289.017858ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:00 | 200 |  130.399808ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:00 | 200 |  122.984556ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:05 | 200 |  337.171466ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  133.590126ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:06 | 200 |  738.426606ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:07 | 200 |  233.506596ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:07 | 200 |   190.32983ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  328.727563ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |  130.710236ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:13 | 200 |   79.309489ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:25 | 200 |  307.317747ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:25 | 200 |  204.201627ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:25 | 200 |  158.258408ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:38 | 200 |  265.085762ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:25:38 | 200 |  183.806209ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:01 | 200 |  312.146634ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:01 | 200 |  213.030868ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:02 | 200 |   249.81074ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:10 | 200 |  146.761378ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:10 | 200 |   77.547758ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:10 | 200 |   74.258607ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:13 | 200 |  174.052939ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:14 | 200 |  383.029578ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:15 | 200 |  1.056117739s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:18 | 200 |  128.034914ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:19 | 200 |  203.796102ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:19 | 200 |  149.390355ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:26 | 200 |   78.639711ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:26 | 200 |   264.28603ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:27 | 200 |  794.442376ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:35 | 200 |  366.718126ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:36 | 200 |  377.554038ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:39 | 200 |  113.845456ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:39 | 200 |  140.082017ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:42 | 200 |  578.283928ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:42 | 200 |  121.628301ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:42 | 200 |  113.531177ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:54 | 200 |   52.671017ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:26:54 | 200 |   45.510069ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:14 | 200 |  363.041626ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |  182.036401ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |  159.543362ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |   71.506125ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:15 | 200 |   98.370558ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:27:24 | 200 |  372.185283ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 500 |          1m0s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  56.69482554s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 | 56.581763462s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  169.879759ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  126.909103ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |   177.03738ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  219.420523ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  213.818489ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  310.472552ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  291.669785ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  370.815921ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:24 | 200 |  226.772781ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:25 | 200 |  343.383257ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:25 | 200 |  550.525277ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:25 | 200 |  848.716071ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:37:31.691-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:37:31.692-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:37:31.692-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:37:31.692-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-09T23:37:31.692-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:37:31.693-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38627"
time=2025-11-09T23:37:31.884-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45931"
time=2025-11-09T23:37:32.105-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45789"
time=2025-11-09T23:37:32.105-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42557"
time=2025-11-09T23:37:32.544-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:37:36 | 200 |      35.446µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:37:36 | 404 |     203.202µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:37:36 | 200 |       26.43µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:37:37.210-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:37:40.439-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:37:41.662-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:37:42.876-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:37:44.094-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:37:45 | 200 |  8.623908008s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     376.156µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:49:43.981-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:49:43.982-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:49:43.982-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:49:43.982-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-09T23:49:43.982-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:49:43.982-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45011"
time=2025-11-09T23:49:44.194-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34713"
time=2025-11-09T23:49:44.391-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34769"
time=2025-11-09T23:49:44.391-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45607"
time=2025-11-09T23:49:44.611-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:49:48 | 200 |     214.773µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:49:49 | 200 |   48.074988ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     605.949µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:57:51.146-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:57:51.147-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:57:51.147-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:57:51.147-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-09T23:57:51.148-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:57:51.148-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45377"
time=2025-11-09T23:57:51.372-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39999"
time=2025-11-09T23:57:51.569-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41381"
time=2025-11-09T23:57:51.569-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44121"
time=2025-11-09T23:57:51.776-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:57:56 | 200 |      30.177µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:57:56 | 200 |   54.405649ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     580.941µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:05:10.343-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:05:10.344-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:05:10.344-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:05:10.344-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:05:10.345-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:05:10.345-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37641"
time=2025-11-10T00:05:10.551-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39891"
time=2025-11-10T00:05:10.740-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46197"
time=2025-11-10T00:05:10.740-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39573"
time=2025-11-10T00:05:10.967-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:05:15 | 200 |      59.421µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:05:15 | 200 |   46.999231ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |      628.14µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:11:25.146-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:11:25.146-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:11:25.146-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:11:25.147-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:11:25.147-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:11:25.148-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40675"
time=2025-11-10T00:11:25.430-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36839"
time=2025-11-10T00:11:25.621-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41459"
time=2025-11-10T00:11:25.621-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37565"
time=2025-11-10T00:11:25.853-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:11:30 | 200 |      28.864µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:11:30 | 200 |    52.87339ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:06 | 200 |      575.12µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:18:03.178-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:18:03.179-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:18:03.179-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:18:03.179-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:18:03.179-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:18:03.180-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42243"
time=2025-11-10T00:18:03.401-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35719"
time=2025-11-10T00:18:03.616-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38085"
time=2025-11-10T00:18:03.616-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33579"
time=2025-11-10T00:18:03.864-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:18:08 | 200 |      54.773µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:18:08 | 200 |   57.668674ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     348.495µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:26:26.469-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:26:26.469-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:26:26.469-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:26:26.470-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:26:26.470-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:26:26.471-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40057"
time=2025-11-10T00:26:26.690-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33473"
time=2025-11-10T00:26:26.907-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39893"
time=2025-11-10T00:26:26.907-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46015"
time=2025-11-10T00:26:27.150-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:26:31 | 200 |      61.015µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:26:31 | 200 |    56.24192ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     454.063µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:37:24.568-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:37:24.569-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:37:24.569-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:37:24.569-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:37:24.569-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:37:24.569-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35521"
time=2025-11-10T00:37:24.772-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35267"
time=2025-11-10T00:37:25.211-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36481"
time=2025-11-10T00:37:25.211-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43683"
time=2025-11-10T00:37:25.436-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:37:29 | 200 |      46.758µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:37:29 | 200 |   48.808818ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |      383.46µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:47:58.865-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:47:58.866-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:47:58.866-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:47:58.866-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T00:47:58.866-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:47:58.867-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40875"
time=2025-11-10T00:47:59.064-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35415"
time=2025-11-10T00:47:59.256-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37109"
time=2025-11-10T00:47:59.256-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38237"
time=2025-11-10T00:47:59.484-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:48:03 | 200 |      32.822µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:48:03 | 200 |   46.304562ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |       388.4µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:07:58.369-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:07:58.370-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:07:58.370-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:07:58.370-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T01:07:58.370-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:07:58.371-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33553"
time=2025-11-10T01:07:58.572-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46141"
time=2025-11-10T01:07:58.760-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35089"
time=2025-11-10T01:07:58.760-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42859"
time=2025-11-10T01:07:58.976-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:08:03 | 200 |      44.123µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:08:03 | 200 |   45.456628ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     735.902µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:15:09.308-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:15:09.309-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:15:09.310-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:15:09.310-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T01:15:09.310-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:15:09.310-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35311"
time=2025-11-10T01:15:09.569-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45511"
time=2025-11-10T01:15:09.767-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42295"
time=2025-11-10T01:15:09.767-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35019"
time=2025-11-10T01:15:10.000-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:15:14 | 200 |      34.825µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:15:14 | 200 |   49.296262ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:50 | 200 |     467.668µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:39:41.413-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:39:41.413-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:39:41.414-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:39:41.414-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T01:39:41.414-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:39:41.414-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44813"
time=2025-11-10T01:39:41.616-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41203"
time=2025-11-10T01:39:42.068-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40393"
time=2025-11-10T01:39:42.068-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34953"
time=2025-11-10T01:39:42.280-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:39:43 | 200 |      37.982µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:39:43 | 200 |   46.332213ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     437.432µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:07.796-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44161"
time=2025-11-10T01:41:08.618-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:08.623-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11464/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 42387"
time=2025-11-10T01:41:08.628-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:08.628-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="90.0 GiB" free_swap="0 B"
time=2025-11-10T01:41:08.628-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="13.3 GiB" free="13.8 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:08.644-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:08.646-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:42387"
time=2025-11-10T01:41:08.650-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:08.719-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:08.848-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:09.705-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:09.824-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:09.825-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:09.825-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:09.834-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:10.336-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.71 seconds"
[GIN] 2025/11/10 - 01:41:11 | 200 |  3.635053473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:11 | 200 |  765.296995ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:32 | 200 |  1.697869484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:34 | 200 |  1.998647431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:36 | 200 |  1.963865313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:38 | 200 |  2.110795867s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:39 | 200 |  2.805836925s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:41 | 200 |  1.411859406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:57 | 200 |  1.514365862s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:01 | 200 |  3.848205338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:12 | 200 |  2.818490354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:18 | 200 |  6.414667635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:22 | 200 |  6.327476118s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:25 | 200 |  6.059322407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:28 | 200 |  5.840376937s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:33 | 200 |  8.272571909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:38 | 200 |  9.568824766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:43 | 200 | 12.035557775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:46 | 200 |   9.64915416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:50 | 200 |  7.969808011s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:57 | 200 |  9.941751378s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:00 | 200 | 11.962272149s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:04 | 200 |  9.299081236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:07 | 200 |  7.044665405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:13 | 200 | 12.544321762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:17 | 200 | 12.890172377s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:19 | 200 | 12.631024181s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:25 | 200 | 16.727919587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:28 | 200 | 15.012269772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:31 | 200 | 13.095147563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:37 | 200 | 17.625351479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:42 | 200 | 17.173273618s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:46 | 200 | 15.529397999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:51 | 200 | 18.790892657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:55 | 200 | 17.888895416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:57 | 200 | 13.704004581s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:01 | 200 | 14.440049838s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:05 | 200 |  14.94821085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:09 | 200 | 14.281520138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:13 | 200 | 16.460378019s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:19 | 200 | 21.387640813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:23 | 200 | 20.077861169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:27 | 200 |  17.91686517s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:32 | 200 | 16.362299771s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:36 | 200 | 13.385711103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 | 16.715058563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:45 | 200 | 15.881882371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:49 | 200 | 14.330232819s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:53 | 200 | 11.569888084s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:57 | 200 |  9.280929349s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:00 | 200 | 10.222942626s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:04 | 200 |  9.669422276s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:11 | 200 | 17.281039817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:15 | 200 | 14.759843701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:19 | 200 | 13.278211999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:22 | 200 | 10.670377117s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:28 | 200 | 11.760128859s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:30 | 200 | 10.643331929s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:35 | 200 | 16.088402826s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:41 | 200 | 16.651830544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:47 | 200 | 16.633895135s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:50 | 200 | 12.626776108s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:54 | 200 | 11.072358694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:58 | 200 |  8.422815079s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:05 | 200 | 10.138850221s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:10 | 200 |  16.04333179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:15 | 200 | 14.320006527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:20 | 200 | 12.088738146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:22 | 200 |  9.468529818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:26 | 200 |   6.24019203s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:34 | 200 | 10.996337411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:37 | 200 | 10.770837398s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:42 | 200 |  8.157031624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:46 | 200 |  5.215406189s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:52 | 200 |  4.910956195s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:30 | 200 |  5.624094058s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:34 | 200 |   8.62657145s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:39 | 200 | 14.570207319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:46 | 200 | 20.633835699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:52 | 200 | 26.788847849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:55 | 200 | 20.501531945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:01 | 200 | 19.476932306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:06 | 200 | 20.041680693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:11 | 200 |  22.50862491s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:16 | 200 | 21.187301127s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:20 | 200 | 18.642992274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:24 | 200 | 16.476117701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:29 | 200 | 14.646116808s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:30 | 200 | 11.299788178s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:32 | 200 |  7.731963277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:36 | 200 |   5.75567818s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:42 | 200 | 11.223508483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:47 | 200 |  10.45494025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:52 | 200 |   9.21324342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:56 | 200 |  7.839566414s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:02 | 200 | 13.484814193s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:05 | 200 |  9.323347532s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:10 | 200 | 15.473175343s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:15 | 200 | 12.897401756s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:20 | 200 | 11.614832768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:22 | 200 | 11.592236126s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:26 | 200 |  10.62480723s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:30 | 200 | 14.642754411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:33 | 200 | 10.214226401s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:40 | 200 | 11.458704922s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:45 | 200 | 11.868143599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:51 | 200 | 16.626992368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:56 | 200 | 16.683335825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:58 | 200 |  12.23777375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:02 | 200 |  9.399311788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:08 | 200 |   9.69754409s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:11 | 200 | 11.326766064s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:15 | 200 |  9.122733751s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:19 | 200 |  7.947908042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:23 | 200 | 11.190316009s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:28 | 200 |  8.854819643s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:32 | 200 |  6.930879353s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:37 | 200 |  6.872106542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:41 | 200 |  3.924887232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:50 | 200 |  6.990095089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:55 | 200 |  5.704044762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:00 | 200 |  4.437069045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:06 | 200 |  5.450545673s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:12 | 200 |  5.919790525s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:15 | 200 |   7.35667323s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:18 | 200 |  4.688513982s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:24 | 200 |  3.512234334s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:29 | 200 |  4.710434898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:32 | 200 |  5.188297803s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:37 | 200 |  4.584348109s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:45 | 200 |  6.012568214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:49 | 200 |  3.069140053s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:54 | 200 |  5.071671725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:58 | 200 |  5.986697905s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:04 | 200 |  5.190986341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:10 | 200 |   4.87813308s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:14 | 200 |  7.795186508s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:17 | 200 |  4.673028665s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:24 | 200 | 10.376870071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:27 | 200 |  7.942274786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:33 | 200 |  8.454002945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:39 | 200 | 11.067589018s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:43 | 200 | 15.428227416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:48 | 200 | 14.278976096s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:50 | 200 |  9.798877679s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:55 | 200 |  8.038576087s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:58 | 200 |  7.363247076s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:03 | 200 |  8.547034358s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:09 | 200 |  7.063347226s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:12 | 200 |  8.847990875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:18 | 200 |  9.526450381s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:26 | 200 | 10.052964222s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:30 | 200 |  7.543279828s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:39 | 200 |  9.325965154s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:47 | 200 | 15.894802182s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:52 | 200 | 15.880096348s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:56 | 200 | 12.646928835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:04 | 200 | 13.738616545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:11 | 200 | 15.734175217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:15 | 200 | 17.487317061s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:21 | 200 | 16.930696465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:30 | 200 | 18.343167038s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:38 | 200 | 23.005979631s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:43 | 200 | 24.715307151s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:49 | 200 | 22.115289524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:56 | 200 | 25.419227169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:00 | 200 | 25.779112006s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:03 | 200 | 23.665999489s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:08 | 200 | 20.129745666s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:13 | 200 | 23.924672675s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:18 | 200 | 21.992826124s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:22 | 200 | 22.723187378s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:26 | 200 | 18.900150601s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:31 | 200 | 15.904004426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:35 | 200 | 13.708767574s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:40 | 200 | 17.924951068s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:45 | 200 | 18.782923173s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:48 | 200 | 19.042699342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:52 | 200 | 17.418545951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:56 | 200 | 21.170317101s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:59 | 200 | 18.133282782s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:02 | 200 | 14.106997213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:07 | 200 | 18.857189899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:11 | 200 | 17.107048423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:16 | 200 | 16.903937436s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:22 | 200 | 22.311902626s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:28 | 200 | 25.314661177s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:32 | 200 | 24.335178158s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:35 | 200 | 24.085932678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:39 | 200 | 21.656597295s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:43 | 200 | 19.636643683s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:47 | 200 | 17.847478256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:52 | 200 | 19.120752082s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:54 | 200 | 17.998720228s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:59 | 200 |  19.37682355s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:01 | 200 | 17.872878839s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:07 | 200 | 19.660503317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:12 | 200 | 19.920463036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:16 | 200 | 21.629918034s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:19 | 200 | 19.724767268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:23 | 200 | 21.637959536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:28 | 200 | 20.112283416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:35 | 200 | 22.709549467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:40 | 200 | 23.427543634s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:45 | 200 | 25.581944757s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:49 | 200 | 25.811559953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:51 | 200 | 22.982785862s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:54 | 200 | 19.121364238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:57 | 200 |  16.70333372s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:01 | 200 | 15.759953455s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:05 | 200 | 14.923914527s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:09 | 200 | 17.668733742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:12 | 200 | 17.325694232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:15 | 200 | 17.430237698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:17 | 200 | 15.521852681s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:21 | 200 | 15.340691461s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:24 | 200 |  15.32187795s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:29 | 200 |    15.999367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:34 | 200 | 18.610962805s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:40 | 200 | 23.495910694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:47 | 200 | 26.263076598s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:51 | 200 | 26.455565333s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:57 | 200 | 28.178643524s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:03 | 200 | 28.747235759s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:07 | 200 | 24.874960746s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:13 | 200 | 25.826043437s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:18 | 200 | 26.134795272s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:21 | 200 |  23.31772611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:26 | 200 | 23.052615457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:30 | 200 | 22.754669299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:32 | 200 | 19.011961693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:35 | 200 | 17.321098073s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:40 | 200 | 18.868428741s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:43 | 200 | 16.477281565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:48 | 200 | 18.106054871s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:52 | 200 | 19.536479664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:57 | 200 | 21.640092485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:04 | 200 | 23.172821875s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:09 | 200 |  25.43314966s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:14 | 200 | 25.123182465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:21 | 200 | 28.731813515s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:27 | 200 | 29.244637482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:30 | 200 | 25.393142417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:36 | 200 | 26.347351049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:41 | 200 | 26.702738496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:46 | 200 | 24.305678557s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:50 | 200 | 23.209678591s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:56 | 200 | 26.068180575s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:01 | 200 | 24.535364389s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:05 | 200 | 24.065748089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:11 | 200 | 25.167222642s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:16 | 200 | 25.818003115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:22 | 200 | 25.895350099s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:29 | 200 | 27.930039344s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:33 | 200 | 27.029286919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:38 | 200 |  26.80238361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:44 | 200 | 27.165436137s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:47 | 200 | 24.850751341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:51 | 200 | 20.816689338s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:52 | 200 | 18.510514234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:54 | 200 | 16.017080423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:55 | 200 | 11.237317931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:57 | 200 |  9.007504423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:58 | 200 |  2.808156593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:27 | 200 |  7.992742049s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:31 | 200 | 11.139031474s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:36 | 200 | 16.071964095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:38 | 200 | 18.096740257s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:43 | 200 | 22.924000788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:47 | 200 | 17.743536025s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:53 | 200 |    16.696149s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:56 | 200 | 13.617857994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:03 | 200 | 14.167632542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:07 | 200 | 19.504105987s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:11 | 200 | 14.918654989s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:15 | 200 | 17.934538661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:18 | 200 |  16.63366045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:22 | 200 | 14.697858545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:26 | 200 | 13.774966496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:30 | 200 | 12.300733542s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:35 | 200 | 16.563715448s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:39 | 200 | 13.191529532s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:46 | 200 | 14.273330318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:52 | 200 | 14.676503944s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:57 | 200 |  17.50612308s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:01 | 200 | 16.945968624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:06 | 200 | 15.682494787s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:10 | 200 | 13.118290318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:12 | 200 | 10.955810997s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:16 | 200 | 13.602940626s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:21 | 200 | 12.541168322s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:26 | 200 | 12.276372362s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:31 | 200 |  14.69242029s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:35 | 200 | 14.695802391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:41 | 200 | 14.363344313s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:44 | 200 | 12.237241496s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:49 | 200 |  9.659143829s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:54 | 200 |  8.089830391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:58 | 200 | 11.424562959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:02 | 200 | 11.538033428s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:08 | 200 | 11.905934721s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:14 | 200 | 10.961183742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:21 | 200 | 17.910691234s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:24 | 200 |  15.26172099s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:29 | 200 | 12.984387479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:34 | 200 | 12.772636169s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:37 | 200 | 15.384277881s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:39 | 200 | 10.537239957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:44 | 200 |  9.890850826s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:51 | 200 | 10.028444353s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:56 | 200 | 15.830247201s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:00 | 200 | 13.589106472s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:05 | 200 | 11.130827503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:06 | 200 |  5.162422444s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:10 | 200 |  4.314893707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:13 | 200 |  6.183316521s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:19 | 200 |  8.458187772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:24 | 200 | 10.116349056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:29 | 200 |  8.118505898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:32 | 200 |    5.2697245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:38 | 200 |  5.750093623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:45 | 200 |  5.922005582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:49 | 200 |  2.767085115s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:53 | 200 |  4.361855074s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:56 | 200 |  4.853636678s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:06.437-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11464 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11464 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:06.437-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:06.437-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:06.438-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11464 (version 0.12.9)"
time=2025-11-10T02:08:06.438-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:06.439-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44099"
time=2025-11-10T02:08:06.635-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41393"
time=2025-11-10T02:08:06.821-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43375"
time=2025-11-10T02:08:06.821-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37977"
time=2025-11-10T02:08:07.039-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:08 | 200 |      49.062µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:08 | 200 |   44.368791ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     583.575µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:29.991-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42041"
time=2025-11-10T02:09:30.418-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:30.418-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11464/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 36179"
time=2025-11-10T02:09:30.420-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:30.420-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="95.5 GiB" free_swap="0 B"
time=2025-11-10T02:09:30.420-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 library=CUDA available="16.9 GiB" free="17.3 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:30.435-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:30.435-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:36179"
time=2025-11-10T02:09:30.442-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:30.487-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:30.641-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:31.183-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:31.332-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:31.332-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:31.836-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.42 seconds"
[GIN] 2025/11/10 - 02:09:32 | 200 |  2.647650807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:33 | 200 |  932.272413ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:59 | 200 |  1.282598824s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:01 | 200 |  1.524378785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:02 | 200 |  1.209955164s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:18 | 200 |  2.395540545s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:20 | 200 |  1.669132311s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:12 | 200 |  1.696876636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:25 | 200 | 11.802591268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:32 | 200 | 15.752899606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:36 | 200 | 19.037822623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:41 | 200 | 24.210603251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:46 | 200 | 27.213266807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:51 | 200 | 27.428673027s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:55 | 200 | 31.307856918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:00 | 200 | 34.581743563s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 39.718424858s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:12 | 200 |  43.05224704s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:14 | 500 | 44.989840494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:14 | 500 | 45.035541387s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.101824501s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:15.541-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:17 | 500 | 45.048334382s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:18.573-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:19.098-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:21.053-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:22 | 200 | 45.204296559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:27 | 200 | 45.118978654s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:31 | 200 | 43.870504983s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:35 | 200 |  43.11123348s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:39 | 200 | 43.798965296s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:44 | 200 | 43.894631482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:49 | 200 | 43.425067383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:52 | 200 | 45.558004048s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:53 | 500 | 45.063347044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:55 | 500 | 45.072882351s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:58 | 500 | 45.147624975s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:59 | 500 | 45.058896334s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:59 | 500 | 45.045660401s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:02.051-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:04 | 200 | 33.295165404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:10 | 200 | 34.805325955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:17 | 200 | 36.958510666s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.447-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 31.732511701s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.454-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 30.150507335s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.459-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  7.042883116s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.459-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 14.474944788s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 200 | 36.982048199s |       127.0.0.1 | POST     "/api/generate"
