time=2025-11-09T23:14:39.954-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:14:39.954-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:14:39.954-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:14:39.954-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-09T23:14:39.955-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:14:39.955-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35347"
time=2025-11-09T23:14:40.147-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46171"
time=2025-11-09T23:14:40.604-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44241"
time=2025-11-09T23:14:40.604-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43749"
time=2025-11-09T23:14:40.816-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:14:44 | 200 |      30.297µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:14:44 | 404 |     152.075µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:14:44 | 200 |      18.675µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:14:45.528-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:14:48.749-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:14:49.994-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:14:51.221-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:14:52.438-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:14:53.648-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:14:54 | 200 |  9.899534169s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |     393.318µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:28:28.251-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43449"
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11475/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 1
print_info: model type       = ?B
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
llama_model_load: vocab only - skipping tensors
time=2025-11-09T23:28:29.178-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --model /home/dan/.ollama-11475/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 --port 41437"
time=2025-11-09T23:28:29.179-07:00 level=INFO source=server.go:470 msg="system memory" total="123.6 GiB" free="90.4 GiB" free_swap="0 B"
time=2025-11-09T23:28:29.179-07:00 level=INFO source=memory.go:37 msg="new model will fit in available VRAM across minimum required GPUs, loading" model=/home/dan/.ollama-11475/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 library=CUDA parallel=1 required="910.5 MiB" gpus=1
time=2025-11-09T23:28:29.179-07:00 level=INFO source=server.go:522 msg=offload library=CUDA layers.requested=-1 layers.model=31 layers.offload=31 layers.split=[31] memory.available="[11.9 GiB]" memory.gpu_overhead="0 B" memory.required.full="910.5 MiB" memory.required.partial="910.5 MiB" memory.required.kv="90.0 MiB" memory.required.allocations="[910.5 MiB]" memory.weights.total="256.6 MiB" memory.weights.repeating="202.6 MiB" memory.weights.nonrepeating="54.0 MiB" memory.graph.full="97.1 MiB" memory.graph.partial="120.4 MiB"
time=2025-11-09T23:28:29.233-07:00 level=INFO source=runner.go:910 msg="starting go runner"
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-874c91aa-3bb2-54a0-534f-860241e77353
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-09T23:28:29.450-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-09T23:28:29.450-07:00 level=INFO source=runner.go:946 msg="Server listening on 127.0.0.1:41437"
time=2025-11-09T23:28:29.459-07:00 level=INFO source=runner.go:845 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:false KvSize:4096 KvCacheType: NumThreads:8 GPULayers:31[ID:GPU-874c91aa-3bb2-54a0-534f-860241e77353 Layers:31(0..30)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:true}"
time=2025-11-09T23:28:29.465-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:29.466-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
ggml_backend_cuda_device_get_memory device GPU-874c91aa-3bb2-54a0-534f-860241e77353 utilizing NVML memory reporting free: 12813336576 total: 25757220864
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:09:00.0) - 12219 MiB free
llama_model_loader: loaded meta data with 33 key-value pairs and 272 tensors from /home/dan/.ollama-11475/blobs/sha256-f535f83ec568d040f88ddc04a199fa6da90923bbb41d4dcaed02caa924d6ef57 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Smollm2 135M 8k Lc100K Mix1 Ep2
llama_model_loader: - kv   3:                       general.organization str              = HuggingFaceTB
llama_model_loader: - kv   4:                           general.finetune str              = 8k-lc100k-mix1-ep2
llama_model_loader: - kv   5:                           general.basename str              = smollm2
llama_model_loader: - kv   6:                         general.size_label str              = 135M
llama_model_loader: - kv   7:                            general.license str              = apache-2.0
llama_model_loader: - kv   8:                          general.languages arr[str,1]       = ["en"]
llama_model_loader: - kv   9:                          llama.block_count u32              = 30
llama_model_loader: - kv  10:                       llama.context_length u32              = 8192
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 576
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 1536
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 9
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 3
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 100000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  20:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  22:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  23:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  24:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  26:                      tokenizer.ggml.merges arr[str,48900]   = ["Ġ t", "Ġ a", "i n", "h e", "Ġ Ġ...
llama_model_loader: - kv  27:                tokenizer.ggml.bos_token_id u32              = 1
llama_model_loader: - kv  28:                tokenizer.ggml.eos_token_id u32              = 2
llama_model_loader: - kv  29:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  30:            tokenizer.ggml.padding_token_id u32              = 2
llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...
llama_model_loader: - kv  32:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:   61 tensors
llama_model_loader: - type  f16:  211 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 256.63 MiB (16.00 BPW) 
load: printing all EOG tokens:
load:   - 0 ('<|endoftext|>')
load:   - 2 ('<|im_end|>')
load:   - 4 ('<reponame>')
load: special tokens cache size = 17
load: token to piece cache size = 0.3170 MB
print_info: arch             = llama
print_info: vocab_only       = 0
print_info: n_ctx_train      = 8192
print_info: n_embd           = 576
print_info: n_layer          = 30
print_info: n_head           = 9
print_info: n_head_kv        = 3
print_info: n_rot            = 64
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 64
print_info: n_embd_head_v    = 64
print_info: n_gqa            = 3
print_info: n_embd_k_gqa     = 192
print_info: n_embd_v_gqa     = 192
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-05
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 1536
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 0
print_info: rope scaling     = linear
print_info: freq_base_train  = 100000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 8192
print_info: rope_finetuned   = unknown
print_info: model type       = 256M
print_info: model params     = 134.52 M
print_info: general.name     = Smollm2 135M 8k Lc100K Mix1 Ep2
print_info: vocab type       = BPE
print_info: n_vocab          = 49152
print_info: n_merges         = 48900
print_info: BOS token        = 1 '<|im_start|>'
print_info: EOS token        = 2 '<|im_end|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: PAD token        = 2 '<|im_end|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM REP token    = 4 '<reponame>'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: EOG token        = 2 '<|im_end|>'
print_info: EOG token        = 4 '<reponame>'
print_info: max token length = 162
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: offloading 30 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 31/31 layers to GPU
load_tensors:        CUDA0 model buffer size =   256.63 MiB
load_tensors:   CPU_Mapped model buffer size =    54.00 MiB
llama_init_from_model: model default pooling_type is [0], but [-1] was specified
llama_context: constructing llama_context
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 4096
llama_context: n_ctx_per_seq = 4096
llama_context: n_batch       = 512
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = disabled
llama_context: kv_unified    = false
llama_context: freq_base     = 100000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     0.19 MiB
llama_kv_cache:      CUDA0 KV buffer size =    90.00 MiB
llama_kv_cache: size =   90.00 MiB (  4096 cells,  30 layers,  1/1 seqs), K (f16):   45.00 MiB, V (f16):   45.00 MiB
llama_context:      CUDA0 compute buffer size =    97.12 MiB
llama_context:  CUDA_Host compute buffer size =     9.88 MiB
llama_context: graph nodes  = 1086
llama_context: graph splits = 2
time=2025-11-09T23:28:32.492-07:00 level=INFO source=server.go:1289 msg="llama runner started in 3.31 seconds"
time=2025-11-09T23:28:32.492-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-09T23:28:32.492-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-09T23:28:32.493-07:00 level=INFO source=server.go:1289 msg="llama runner started in 3.31 seconds"
[GIN] 2025/11/09 - 23:28:32 | 200 |   4.71111235s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:32 | 200 |  4.522673892s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:33 | 200 |  619.382725ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:33 | 200 |  590.970153ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:33 | 200 |  621.278053ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:34 | 200 |  731.535359ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:34 | 200 |  249.055214ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:38 | 200 |  1.291048876s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:45 | 200 |  6.720336718s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:48 | 200 |  247.193168ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:49 | 200 |  201.546489ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:49 | 200 |  444.396295ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |  592.144789ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:50 | 200 |   473.04263ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:56 | 200 |  234.629302ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:57 | 200 |  181.993095ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:57 | 200 |  161.376752ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:59 | 200 |  346.348609ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:28:59 | 200 |  104.207147ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:00 | 200 |  145.868287ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:04 | 200 |  353.309167ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:05 | 200 |  690.259142ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:05 | 200 |  455.805074ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:06 | 200 |  466.299039ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:07 | 200 |  1.717311649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  256.752243ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  150.176478ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:15 | 200 |  163.837878ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:21 | 200 |  171.230405ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:22 | 200 |  307.788043ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:22 | 200 |  136.085633ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:34 | 200 |  324.914315ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:34 | 200 |  209.836538ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:34 | 200 |  155.366926ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |  145.010739ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:36 | 200 |   88.042035ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:38 | 200 |  539.568951ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:40 | 200 |  1.205216451s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:40 | 200 |  163.854548ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:40 | 200 |  133.633235ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/09 - 23:29:41 | 200 |  154.304614ms |       127.0.0.1 | POST     "/api/generate"
time=2025-11-09T23:40:02.007-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:40:02.007-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:40:02.008-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:40:02.008-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-09T23:40:02.008-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:40:02.009-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38203"
time=2025-11-09T23:40:02.216-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35089"
time=2025-11-09T23:40:02.410-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44305"
time=2025-11-09T23:40:02.410-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35591"
time=2025-11-09T23:40:02.638-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:40:07 | 200 |   10.461282ms |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:40:07 | 404 |     345.339µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:40:07 | 200 |      18.816µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:40:07.536-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:40:10.745-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:40:11.974-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:40:13.196-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:40:14.418-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:40:15 | 200 |  8.625369478s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |      782.75µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:50:39.687-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:50:39.687-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:50:39.687-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:50:39.687-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-09T23:50:39.688-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:50:39.688-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40623"
time=2025-11-09T23:50:39.895-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44109"
time=2025-11-09T23:50:40.324-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40679"
time=2025-11-09T23:50:40.324-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42109"
time=2025-11-09T23:50:40.547-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:50:44 | 200 |      49.834µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:50:44 | 200 |   45.343414ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     518.645µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:58:46.874-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:58:46.874-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:58:46.875-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:58:46.875-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-09T23:58:46.875-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:58:46.876-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34921"
time=2025-11-09T23:58:47.080-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39589"
time=2025-11-09T23:58:47.277-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33563"
time=2025-11-09T23:58:47.277-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45289"
time=2025-11-09T23:58:47.515-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:58:51 | 200 |       32.24µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:58:51 | 200 |   44.537763ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     288.101µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:06:06.059-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:06:06.060-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:06:06.060-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:06:06.060-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:06:06.060-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:06:06.061-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40465"
time=2025-11-10T00:06:06.273-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46105"
time=2025-11-10T00:06:06.483-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43773"
time=2025-11-10T00:06:06.483-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40341"
time=2025-11-10T00:06:06.705-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:06:11 | 200 |      40.436µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:06:11 | 200 |   47.701089ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |     769.075µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:12:20.902-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:12:20.903-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:12:20.903-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:12:20.903-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:12:20.903-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:12:20.904-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42189"
time=2025-11-10T00:12:21.117-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 32879"
time=2025-11-10T00:12:21.330-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43055"
time=2025-11-10T00:12:21.330-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39497"
time=2025-11-10T00:12:21.565-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:12:25 | 200 |      33.763µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:12:25 | 200 |   51.750381ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:07 | 200 |     308.659µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:18:59.004-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:18:59.004-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:18:59.004-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:18:59.004-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:18:59.005-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:18:59.005-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35511"
time=2025-11-10T00:18:59.229-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38665"
time=2025-11-10T00:18:59.475-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37159"
time=2025-11-10T00:18:59.475-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44489"
time=2025-11-10T00:18:59.734-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:19:04 | 200 |      75.913µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:19:04 | 200 |   56.286729ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     467.979µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:27:22.322-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:27:22.322-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:27:22.322-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:27:22.323-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:27:22.323-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:27:22.324-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40813"
time=2025-11-10T00:27:22.545-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42569"
time=2025-11-10T00:27:23.020-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40271"
time=2025-11-10T00:27:23.020-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43633"
time=2025-11-10T00:27:23.267-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:27:27 | 200 |      73.338µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:27:27 | 200 |    58.01222ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |      505.84µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:38:20.297-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:38:20.297-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:38:20.298-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:38:20.298-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:38:20.298-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:38:20.299-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44327"
time=2025-11-10T00:38:20.512-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41991"
time=2025-11-10T00:38:20.720-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33137"
time=2025-11-10T00:38:20.720-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41119"
time=2025-11-10T00:38:20.958-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:38:25 | 200 |      28.203µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:38:25 | 200 |    46.71373ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     489.991µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:48:54.621-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:48:54.622-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:48:54.622-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:48:54.622-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T00:48:54.623-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:48:54.623-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34933"
time=2025-11-10T00:48:54.832-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39187"
time=2025-11-10T00:48:55.067-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39669"
time=2025-11-10T00:48:55.067-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36227"
time=2025-11-10T00:48:55.576-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:48:59 | 200 |      59.522µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:48:59 | 200 |   46.803078ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     524.405µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:08:54.163-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:08:54.164-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:08:54.164-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:08:54.164-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T01:08:54.164-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:08:54.164-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46835"
time=2025-11-10T01:08:54.383-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38883"
time=2025-11-10T01:08:54.593-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43533"
time=2025-11-10T01:08:54.593-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46507"
time=2025-11-10T01:08:54.826-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:08:59 | 200 |      36.148µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:08:59 | 200 |   50.108697ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     431.741µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:16:05.040-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:16:05.040-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:16:05.041-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:16:05.041-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T01:16:05.041-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:16:05.041-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45761"
time=2025-11-10T01:16:05.244-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35569"
time=2025-11-10T01:16:05.437-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44407"
time=2025-11-10T01:16:05.437-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44137"
time=2025-11-10T01:16:05.660-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:16:10 | 200 |       32.16µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:16:10 | 200 |    45.73958ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:51 | 200 |     599.416µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:04.145-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:40:04.145-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:40:04.145-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:40:04.145-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T01:40:04.146-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:40:04.146-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39415"
time=2025-11-10T01:40:04.636-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36715"
time=2025-11-10T01:40:04.830-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 34543"
time=2025-11-10T01:40:04.831-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45923"
time=2025-11-10T01:40:05.052-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:40:06 | 200 |      54.973µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:40:06 | 200 |   46.706025ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     322.356µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:23.252-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41207"
time=2025-11-10T01:41:23.547-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:23.547-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11475/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 35499"
time=2025-11-10T01:41:23.547-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:23.548-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="86.3 GiB" free_swap="0 B"
time=2025-11-10T01:41:23.548-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="13.3 GiB" free="13.8 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:23.564-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:23.565-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:35499"
time=2025-11-10T01:41:23.570-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:23.621-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:23.971-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:24.390-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:24.530-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:24.533-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:24.534-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:24.531-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:24.531-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:24.531-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:24.536-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:25.045-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.50 seconds"
[GIN] 2025/11/10 - 01:41:26 | 200 |  3.397357827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:28 | 200 |  1.422220977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:36 | 200 |  2.190180516s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:02 | 200 |  2.505881255s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:05 | 200 |  2.580220685s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:22 | 200 |  7.506816369s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:26 | 200 |   9.54117914s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:42:27.953-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4116 keep=4 new=4096
[GIN] 2025/11/10 - 01:42:30 | 200 | 14.140252977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:33 | 200 | 15.076902081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:36 | 200 |  17.32127619s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:39 | 200 | 15.497507167s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:42:41.032-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4211 keep=4 new=4096
[GIN] 2025/11/10 - 01:42:45 | 200 | 14.429167032s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:49 | 200 | 17.246523562s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:55 | 200 | 17.412012991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:59 | 200 |  15.09617724s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:43:01.790-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4235 keep=4 new=4096
[GIN] 2025/11/10 - 01:43:05 | 200 | 19.523202501s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:09 | 200 | 19.045957615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:13 | 200 | 22.818335742s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:16 | 200 | 20.789375469s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:20 | 200 | 18.416422057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:23 | 200 | 15.035786312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:28 | 200 | 14.594166211s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:32 | 200 | 13.287213849s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:36 | 200 | 12.769826662s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:42 | 200 | 16.542660765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:46 | 200 | 14.877776644s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:53 | 200 | 19.998514569s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:58 | 200 | 20.010262866s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:04 | 200 | 19.232194678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:10 | 200 |  23.12503822s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:14 | 200 | 22.323525541s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:21 | 200 | 22.909189558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:24 | 200 |  20.23091699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:30 | 200 | 19.419404484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:33 | 200 | 15.467178924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:36 | 200 | 12.411980287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:41 | 200 | 11.514159245s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:48 | 200 |  11.31768518s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:51 | 200 | 14.546377698s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:56 | 200 | 12.738749122s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:00 | 200 | 11.544678533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:03 | 200 |  8.056900073s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:07 | 200 |  6.298191342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:12 | 200 | 11.294722779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:16 | 200 | 12.197625896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:20 | 200 | 12.229624088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:24 | 200 | 10.709754868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:28 | 200 |  7.620866273s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:32 | 200 |  7.565603936s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:37 | 200 | 11.389705672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:43 | 200 | 15.353370599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:49 | 200 | 16.944759888s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:53 | 200 | 15.504542204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:58 | 200 | 13.638335261s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:02 | 200 | 10.405519779s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:05 | 200 |  8.582253473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:08 | 200 |  5.612199635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:13 | 200 | 10.642325426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:20 | 200 | 14.366906129s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:25 | 200 | 16.571910391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:29 | 200 | 20.123255131s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:35 | 200 | 19.418579202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:39 | 200 | 17.500899735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:43 | 200 | 15.225941582s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:50 | 200 |  20.96279444s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:53 | 200 | 17.016745915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:55 | 200 | 19.609348632s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:58 | 200 | 15.189977924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:01 | 200 |  3.077364128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:31 | 200 |  7.599055067s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:34 | 200 |  8.832136288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:39 | 200 | 13.965445426s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:45 | 200 |  20.14575699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:51 | 200 | 25.457134463s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:54 | 200 | 23.006867649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:58 | 200 | 23.792259951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:01 | 200 | 21.428424269s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:04 | 200 |  18.68395217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:05 | 200 | 14.543024795s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:11 | 200 | 16.757545067s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:15 | 200 | 17.172017407s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:19 | 200 | 16.103272948s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:22 | 200 | 13.196953299s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:28 | 200 | 12.957393701s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:34 | 200 | 15.502818473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:39 | 200 | 19.224301887s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:44 | 200 | 21.259789957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:49 | 200 | 21.611377464s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:53 | 200 | 21.445712097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:57 | 200 | 19.848594976s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:02 | 200 | 18.702395646s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:07 | 200 | 17.705476533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:12 | 200 | 15.763097821s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:17 | 200 | 12.937046844s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:22 | 200 | 13.912364775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:27 | 200 | 16.770273221s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:31 | 200 | 14.348523899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:38 | 200 | 21.133318774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:42 | 200 | 19.256076769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:47 | 200 | 19.704406558s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:55 | 200 | 24.808085288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:00 | 200 | 25.114452274s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:05 | 200 | 22.734185538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:08 | 200 | 20.504745842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:14 | 200 | 19.821740618s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:20 | 200 | 18.956134785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:25 | 200 | 17.992382197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:28 | 200 | 14.394854507s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:34 | 200 | 13.632458256s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:40 | 200 |  13.64470182s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:44 | 200 | 11.644337615s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:47 | 200 |  8.542971871s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:51 | 200 |  6.353523363s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 |  9.901240893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:00 | 200 | 10.008775337s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:05 | 200 |  9.529282353s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:09 | 200 |  6.677798817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:15 | 200 |   9.19678689s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:18 | 200 |  8.148913468s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:21 | 200 | 11.406262404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:24 | 200 |  9.246193097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:29 | 200 |  7.767971095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:36 | 200 | 14.550708853s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:41 | 200 | 16.117652791s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:46 | 200 | 18.413377538s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:51 | 200 | 21.643232854s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:53 | 200 | 18.676775404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:56 | 200 | 16.235112566s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:00 | 200 | 12.908267726s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:04 | 200 | 10.639687401s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:14 | 200 | 19.339164359s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:20 | 200 | 23.363099045s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:25 | 200 | 24.110260404s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:28 | 200 | 20.675476366s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:32 | 200 | 17.993155898s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:40 | 200 | 25.828954297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:46 | 200 | 25.655487915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:52 | 200 | 27.297399408s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:56 | 200 | 27.006528556s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:01 | 200 | 28.349727567s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:07 | 200 | 26.110564913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:13 | 200 | 27.478414555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:19 | 200 | 26.651598032s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:22 | 200 |   24.8013417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:30 | 200 | 28.335157893s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:34 | 200 | 27.222464606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:40 | 200 |  25.89497416s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:45 | 200 | 26.293578306s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:48 | 200 | 26.127794594s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:53 | 200 | 22.516342442s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:59 | 200 | 24.642713452s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:03 | 200 | 22.936178977s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:07 | 200 | 20.876537841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:13 | 200 | 24.344121225s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:20 | 200 | 26.530318357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:27 | 200 | 27.195276672s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:31 | 200 | 28.008544909s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:33 | 200 |  25.70059717s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:37 | 200 |  23.55660973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:41 | 200 | 20.386129196s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:47 | 200 | 19.611875694s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:53 | 200 | 21.228069501s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:58 | 200 |  22.95747843s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:01 | 200 | 19.878330292s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:06 | 200 | 17.578621217s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:11 | 200 | 15.849366917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:14 | 200 | 13.285562448s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:21 | 200 | 19.659754918s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:28 | 200 | 18.452482092s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:34 | 200 | 22.764304342s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:38 | 200 | 23.384125325s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:42 | 200 | 26.055472024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:48 | 200 | 24.269739441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:53 | 200 |  23.29383077s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:56 | 200 | 20.455652729s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:02 | 200 | 20.029496841s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:08 | 200 | 25.428935339s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:13 | 200 | 23.913991586s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:16 | 200 | 22.901453383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:22 | 200 | 25.517986424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:28 | 200 | 24.942118904s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:32 | 200 | 22.937695593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:35 | 200 | 21.800090244s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:38 | 200 | 21.482841503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:44 | 200 | 21.257250901s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:47 | 200 | 19.222385013s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:52 | 200 | 19.598252457s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:55 | 200 | 20.062299939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:58 | 200 | 19.118028832s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:03 | 200 | 19.179925259s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:09 | 200 | 21.553881156s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:14 | 200 | 21.511464386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:21 | 200 | 24.859144279s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:25 | 200 | 27.523248519s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:28 | 200 | 24.986913375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:32 | 200 | 22.473692639s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:35 | 200 | 20.210894307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:38 | 200 | 17.183171777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:42 | 200 | 16.862970252s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:47 | 200 | 18.301377595s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:53 | 200 | 20.782123483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:56 | 200 |  21.10923738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:02 | 200 |  23.24261549s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:05 | 200 | 22.278834427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:09 | 200 | 20.958028503s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:11 | 200 | 18.226149949s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:16 | 200 | 19.023757711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:18 | 200 | 15.930001035s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:22 | 200 | 15.464236097s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:28 | 200 | 15.816218051s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:33 | 200 | 17.230456913s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:38 | 200 | 18.088099384s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:46 | 200 | 19.919280754s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:52 | 200 |    18.800651s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:59 | 200 | 25.307971116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:05 | 200 | 26.929218985s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:10 | 200 | 31.137071367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:15 | 200 | 28.330603285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:20 | 200 | 28.081220688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:25 | 200 | 26.399224458s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:31 | 200 | 25.562010661s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:34 | 200 | 23.456543065s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:38 | 200 | 23.102523559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:44 | 200 | 22.949740179s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:46 | 200 | 20.507239623s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:48 | 200 | 17.676367241s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:52 | 200 | 16.501719214s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:59 | 200 | 15.586548744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:06 | 200 | 21.803141968s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:11 | 200 |  20.84104213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:16 | 200 | 20.171906616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:24 | 200 |  21.45135243s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:29 | 200 | 20.279622105s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:35 | 200 | 20.217088223s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:39 | 200 | 18.349207601s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:42 | 200 | 16.005261284s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:47 | 200 | 17.188350063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:49 | 200 | 16.366426597s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:53 | 200 | 14.838758868s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:57 | 200 | 12.438406973s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:02 | 200 | 10.562285335s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:06 | 200 | 12.575831509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:10 | 200 | 11.949878059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:15 | 200 | 16.877433883s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:20 | 200 | 17.808907763s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:25 | 200 | 20.699110412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:30 | 200 | 19.901785809s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:34 | 200 |  19.53648291s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:38 | 200 | 21.068327293s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:41 | 200 | 17.988147592s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:46 | 200 | 19.496520277s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:49 | 200 | 18.352208184s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:52 | 200 | 16.197394116s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:55 | 200 | 16.450518364s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:58 | 200 | 15.626160529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:26 | 200 |  8.838713676s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:29 | 200 | 11.274068423s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:33 | 200 | 14.849199862s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:38 | 200 | 19.073217267s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:42 | 200 | 22.190797033s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:44 | 200 | 18.191346089s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:47 | 200 | 18.401093103s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:49 | 200 | 15.578538354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:52 | 200 | 13.403359334s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:56 | 200 | 13.140404853s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:01 | 200 | 16.306157608s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:05 | 200 | 17.224344772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:09 | 200 | 19.192376012s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:13 | 200 | 20.774678232s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:17 | 200 | 20.957794402s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:22 | 200 | 20.206565488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:26 | 200 | 21.120898429s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:30 | 200 | 20.299540504s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:33 | 200 | 19.001431367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:36 | 200 | 14.969512877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:41 | 200 | 13.099644375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:44 | 200 | 11.526451865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:48 | 200 | 14.615177762s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:55 | 200 | 16.074839371s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:00 | 200 | 13.873232998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:04 | 200 | 11.938938861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:10 | 200 | 12.014891802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:12 | 200 |  7.745945014s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:17 | 200 | 12.297834688s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:21 | 200 | 11.496629678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:27 | 200 | 13.967815564s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:32 | 200 | 16.803088769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:39 | 200 | 21.869726483s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:45 | 200 | 23.043952776s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:49 | 200 | 26.585367288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:53 | 200 | 25.017163766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:56 | 200 | 21.825528817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:59 | 200 | 19.369307017s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:04 | 200 |  17.68909928s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:09 | 200 | 15.987679485s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:13 | 200 | 17.085674813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:19 | 200 | 20.634856783s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:23 | 200 | 18.885270725s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:25 | 200 | 15.212255243s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:28 | 200 | 10.267714393s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:32 | 200 |  9.065615567s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:36 | 200 |  7.893408957s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:41 | 200 | 11.405176282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:46 | 200 | 11.035033536s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:51 | 200 |  8.044951877s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:53 | 200 |  3.708966611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:59 | 200 |  3.797533526s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:02 | 200 |  6.842157613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:06 | 200 |  3.985673693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:17 | 200 |  7.912639129s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:21 | 200 |  4.985962991s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:26 | 200 |  4.046306845s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:33 | 200 |  4.903172449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:38 | 200 |  4.576961486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:41 | 200 |  6.984272368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:45 | 200 |  5.036662405s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:50 | 200 |  3.417419692s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 200 |  6.964406737s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:29.156-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11475 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11475 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:29.157-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:29.157-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:29.157-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11475 (version 0.12.9)"
time=2025-11-10T02:08:29.158-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:29.160-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36605"
time=2025-11-10T02:08:29.367-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45341"
time=2025-11-10T02:08:29.572-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43501"
time=2025-11-10T02:08:29.572-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35441"
time=2025-11-10T02:08:30.065-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:31 | 200 |      46.057µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:31 | 200 |   46.231029ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     323.608µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:53.920-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36651"
time=2025-11-10T02:09:54.621-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:54.621-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11475/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 44281"
time=2025-11-10T02:09:54.623-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:54.623-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="83.5 GiB" free_swap="0 B"
time=2025-11-10T02:09:54.623-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="10.7 GiB" free="11.2 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:54.652-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:54.652-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:44281"
time=2025-11-10T02:09:54.657-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:54.727-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:55.004-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:56.032-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:56.185-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:56.185-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:56.185-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:56.185-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:56.186-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:56.186-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:56.194-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:56.695-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.07 seconds"
[GIN] 2025/11/10 - 02:09:57 | 200 |   3.94540227s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:58 | 200 |  1.130276367s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:17 | 200 |  2.233264312s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:19 | 200 |  1.497756136s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:24.605-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4116 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:25 | 200 |   1.39428631s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:25.956-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4116 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:26 | 200 |  1.009775375s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:27.094-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=4211 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:27 | 200 |   1.25499804s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:21 | 200 |  2.072970785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:23 | 200 |  3.123439917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:24 | 200 |  3.066127486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:26 | 200 |   3.32899086s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:28 | 200 |   3.82301707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:31 | 200 |  4.353901279s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:11 | 200 |  1.456032396s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:23 | 200 | 10.766369616s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:32 | 200 | 17.630314492s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:36 | 200 | 18.849263146s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:40 | 200 | 21.178680815s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:44 | 200 | 24.864977476s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:48 | 200 | 28.680783529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:52 | 200 | 32.702893391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:55 | 200 | 35.398270846s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:01 | 200 | 39.882263509s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 40.823293561s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:12 | 200 | 43.167941828s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:14.602-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:14 | 500 | 45.087811178s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 46.344883533s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.045426579s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:17 | 500 | 45.051411826s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:18.612-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:18.690-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:23 | 200 | 46.275395521s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:26 | 500 | 45.048164128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:30 | 200 | 45.490160599s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:35 | 200 | 46.352784307s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:40 | 200 | 46.993143749s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:41 | 500 | 45.058086461s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:42.342-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:46 | 200 | 44.410789549s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:50 | 200 | 43.902058286s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:52 | 500 |  45.10530933s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:52.645-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:55 | 200 | 44.946714682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:57 | 500 | 45.048933202s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:58.852-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:00 | 500 | 45.057604029s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:04 | 200 | 18.104715411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:08 | 200 | 16.658425003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:11 | 200 | 15.241850777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:16 | 200 | 17.989039851s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:19 | 200 | 10.847557469s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.484-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  1.890503694s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.492-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  4.910115782s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:22 | 200 |  6.723275842s |       127.0.0.1 | POST     "/api/generate"
