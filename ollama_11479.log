time=2025-11-09T23:15:39.607-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:1 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:15:39.607-07:00 level=INFO source=images.go:522 msg="total blobs: 0"
time=2025-11-09T23:15:39.607-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:15:39.607-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-09T23:15:39.608-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:15:39.608-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40729"
time=2025-11-09T23:15:39.794-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36703"
time=2025-11-09T23:15:39.970-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35875"
time=2025-11-09T23:15:39.970-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43865"
time=2025-11-09T23:15:40.193-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-874c91aa-3bb2-54a0-534f-860241e77353 filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:09:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:15:44 | 200 |      35.867µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:15:44 | 404 |     169.226µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:15:44 | 200 |      18.284µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:15:45.126-07:00 level=INFO source=download.go:177 msg="downloading f535f83ec568 in 3 100 MB part(s)"
time=2025-11-09T23:15:48.344-07:00 level=INFO source=download.go:177 msg="downloading fbacade46b4d in 1 68 B part(s)"
time=2025-11-09T23:15:49.572-07:00 level=INFO source=download.go:177 msg="downloading d502d55c1d60 in 1 675 B part(s)"
time=2025-11-09T23:15:50.789-07:00 level=INFO source=download.go:177 msg="downloading 58d1e17ffe51 in 1 11 KB part(s)"
time=2025-11-09T23:15:52.033-07:00 level=INFO source=download.go:177 msg="downloading f02dd72bb242 in 1 59 B part(s)"
time=2025-11-09T23:15:53.285-07:00 level=INFO source=download.go:177 msg="downloading b0f58c4c1a3c in 1 561 B part(s)"
[GIN] 2025/11/09 - 23:15:54 | 200 |  9.887617605s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:24:07 | 200 |     465.794µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:40:56.823-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:40:56.823-07:00 level=INFO source=images.go:522 msg="total blobs: 6"
time=2025-11-09T23:40:56.823-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:40:56.824-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-09T23:40:56.824-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:40:56.824-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37117"
time=2025-11-09T23:40:57.017-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40659"
time=2025-11-09T23:40:57.200-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35427"
time=2025-11-09T23:40:57.200-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43753"
time=2025-11-09T23:40:57.412-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:41:01 | 200 |      38.502µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:41:01 | 404 |     371.307µs |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:41:01 | 200 |      17.954µs |       127.0.0.1 | HEAD     "/"
time=2025-11-09T23:41:02.327-07:00 level=INFO source=download.go:177 msg="downloading 735af2139dc6 in 3 100 MB part(s)"
time=2025-11-09T23:41:05.544-07:00 level=INFO source=download.go:177 msg="downloading 4b19ac7dd2fb in 1 476 B part(s)"
time=2025-11-09T23:41:06.767-07:00 level=INFO source=download.go:177 msg="downloading 3e2c24001f9e in 1 8.4 KB part(s)"
time=2025-11-09T23:41:07.984-07:00 level=INFO source=download.go:177 msg="downloading 339e884a40f6 in 1 61 B part(s)"
time=2025-11-09T23:41:09.238-07:00 level=INFO source=download.go:177 msg="downloading 74156d92caf6 in 1 490 B part(s)"
[GIN] 2025/11/09 - 23:41:10 | 200 |   8.64150835s |       127.0.0.1 | POST     "/api/pull"
[GIN] 2025/11/09 - 23:41:50 | 200 |     600.918µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:50:59.942-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:50:59.943-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:50:59.943-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:50:59.943-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-09T23:50:59.943-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:50:59.944-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40277"
time=2025-11-09T23:51:00.149-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38613"
time=2025-11-09T23:51:00.346-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37901"
time=2025-11-09T23:51:00.346-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45673"
time=2025-11-09T23:51:00.567-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:51:04 | 200 |      33.773µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:51:04 | 200 |   48.100943ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:51:25 | 200 |     665.801µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-09T23:59:07.135-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-09T23:59:07.136-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-09T23:59:07.136-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-09T23:59:07.136-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-09T23:59:07.137-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-09T23:59:07.138-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42319"
time=2025-11-09T23:59:07.577-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44367"
time=2025-11-09T23:59:07.765-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39563"
time=2025-11-09T23:59:07.765-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35445"
time=2025-11-09T23:59:07.985-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/09 - 23:59:12 | 200 |      54.122µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/09 - 23:59:12 | 200 |   45.106942ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/09 - 23:59:32 | 200 |     466.797µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:06:26.321-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:06:26.321-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:06:26.321-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:06:26.321-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:06:26.322-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:06:26.322-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42661"
time=2025-11-10T00:06:26.782-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46587"
time=2025-11-10T00:06:26.979-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39945"
time=2025-11-10T00:06:26.979-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35517"
time=2025-11-10T00:06:27.218-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:06:31 | 200 |       53.14µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:06:31 | 200 |   47.308862ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:06:52 | 200 |     538.711µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:12:41.169-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:12:41.170-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:12:41.170-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:12:41.170-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:12:41.171-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:12:41.171-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39633"
time=2025-11-10T00:12:41.382-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46821"
time=2025-11-10T00:12:41.844-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44987"
time=2025-11-10T00:12:41.844-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 46145"
time=2025-11-10T00:12:42.067-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:12:46 | 200 |      40.847µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:12:46 | 200 |   50.480497ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:13:07 | 200 |      335.61µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:19:19.316-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:19:19.317-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:19:19.317-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:19:19.317-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:19:19.317-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:19:19.318-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41251"
time=2025-11-10T00:19:19.712-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45379"
time=2025-11-10T00:19:19.978-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44211"
time=2025-11-10T00:19:19.978-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44897"
time=2025-11-10T00:19:20.226-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:19:24 | 200 |      72.326µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:19:24 | 200 |   55.075414ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:19:45 | 200 |     582.404µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:27:42.619-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:27:42.619-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:27:42.619-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:27:42.619-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:27:42.620-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:27:42.620-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39349"
time=2025-11-10T00:27:42.861-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40441"
time=2025-11-10T00:27:43.076-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 42789"
time=2025-11-10T00:27:43.076-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36279"
time=2025-11-10T00:27:43.320-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:27:47 | 200 |      52.288µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:27:47 | 200 |   54.846498ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:28:08 | 200 |     688.173µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:38:40.562-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:38:40.562-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:38:40.562-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:38:40.562-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:38:40.563-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:38:40.563-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39625"
time=2025-11-10T00:38:40.763-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45379"
time=2025-11-10T00:38:40.960-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 39251"
time=2025-11-10T00:38:40.960-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37285"
time=2025-11-10T00:38:41.469-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:38:45 | 200 |      57.498µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:38:45 | 200 |   50.607124ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:39:06 | 200 |     408.037µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T00:49:14.878-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T00:49:14.879-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T00:49:14.879-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T00:49:14.879-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T00:49:14.879-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T00:49:14.880-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38843"
time=2025-11-10T00:49:15.186-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36261"
time=2025-11-10T00:49:15.398-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38233"
time=2025-11-10T00:49:15.398-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33071"
time=2025-11-10T00:49:15.615-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 00:49:19 | 200 |      44.303µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 00:49:19 | 200 |   46.273644ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 00:49:40 | 200 |     593.034µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:09:14.438-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:09:14.439-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:09:14.439-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:09:14.439-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T01:09:14.439-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:09:14.440-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35049"
time=2025-11-10T01:09:14.896-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45155"
time=2025-11-10T01:09:15.075-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38019"
time=2025-11-10T01:09:15.075-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35601"
time=2025-11-10T01:09:15.284-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:09:19 | 200 |       37.66µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:09:19 | 200 |   46.250057ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:09:40 | 200 |     528.353µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:16:25.298-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:16:25.298-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:16:25.298-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:16:25.299-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T01:16:25.299-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:16:25.299-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33871"
time=2025-11-10T01:16:25.725-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 38449"
time=2025-11-10T01:16:25.907-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 37597"
time=2025-11-10T01:16:25.907-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35953"
time=2025-11-10T01:16:26.123-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:16:30 | 200 |      35.477µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:16:30 | 200 |   46.390853ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:16:51 | 200 |     656.754µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:40:12.395-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T01:40:12.395-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T01:40:12.395-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T01:40:12.396-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T01:40:12.396-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T01:40:12.398-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36999"
time=2025-11-10T01:40:12.657-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 44535"
time=2025-11-10T01:40:12.853-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 40233"
time=2025-11-10T01:40:12.853-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 45485"
time=2025-11-10T01:40:13.077-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 01:40:14 | 200 |      48.521µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 01:40:14 | 200 |   48.538708ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 01:40:24 | 200 |     310.955µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T01:41:04.591-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 35585"
time=2025-11-10T01:41:04.886-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T01:41:04.886-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11479/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 45393"
time=2025-11-10T01:41:04.887-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T01:41:04.887-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="94.0 GiB" free_swap="0 B"
time=2025-11-10T01:41:04.887-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="17.8 GiB" free="18.2 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T01:41:04.902-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T01:41:04.903-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:45393"
time=2025-11-10T01:41:04.913-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:04.966-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T01:41:05.134-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T01:41:05.863-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:06.286-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T01:41:06.287-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T01:41:06.287-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T01:41:06.293-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T01:41:06.799-07:00 level=INFO source=server.go:1289 msg="llama runner started in 1.91 seconds"
[GIN] 2025/11/10 - 01:41:07 | 200 |  3.276735915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:08 | 200 |  779.099853ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:09 | 200 |  1.164881593s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:13 | 200 |  978.271151ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:15 | 200 |  1.349834412s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:41:16 | 200 |  1.030402857s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:56.536-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7587 keep=4 new=4096
[GIN] 2025/11/10 - 01:41:57 | 200 |  1.497688666s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:41:58.936-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7705 keep=4 new=4096
[GIN] 2025/11/10 - 01:42:01 | 200 |  3.318356257s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:42:02.756-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7682 keep=4 new=4096
[GIN] 2025/11/10 - 01:42:04 | 200 |  2.853763035s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:05 | 200 |  2.931768085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:08 | 200 |  2.438247091s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:12 | 200 |  3.935460165s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:15 | 200 |  4.303387163s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:22 | 200 |  6.871067005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:25 | 200 |  9.443124236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:31 | 200 | 14.473304167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:38 | 200 | 20.148752471s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:41 | 200 | 22.112382806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:47 | 200 | 22.766545352s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:50 | 200 | 17.461231959s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:54 | 200 | 16.381126174s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:42:58 | 200 |   16.4445705s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:01 | 200 |   17.1514419s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:04 | 200 | 13.826793388s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:10 | 200 | 13.748523078s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:14 | 200 | 11.874962774s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:18 | 200 |  9.200296411s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:22 | 200 |  7.938099772s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:27 | 200 |  6.893698069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:36 | 200 |  8.227140974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:41 | 200 |  8.388780264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:45 | 200 |  5.088607933s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:49 | 200 |  6.906503213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:52 | 200 |  6.675566512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:43:59 | 200 |  6.252136738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:02 | 200 |  3.725275243s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:08 | 200 |  5.780543923s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:12 | 200 |   7.19199653s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:16 | 200 |  5.426786532s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:20 | 200 |   8.31079341s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:24 | 200 |  6.501783802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:30 | 200 |  6.133213595s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:36 | 200 |  5.433552565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:42 | 200 |  5.462700907s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:46 | 200 |  9.414831807s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:49 | 200 |  5.331970107s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:44:56 | 200 |  6.380245106s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:03 | 200 |  6.379433332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:07 | 200 |  5.195084609s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:13 | 200 | 10.338734921s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:19 | 200 | 11.630743006s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:24 | 200 | 16.126825785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:30 | 200 | 16.106623166s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:34 | 200 | 13.205483328s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:40 | 200 | 13.876394368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:46 | 200 | 12.891770266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:51 | 200 | 12.654991487s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:54 | 200 |  9.812555249s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:45:59 | 200 |  8.363446597s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:05 | 200 | 10.482070926s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:11 | 200 | 13.945103005s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:15 | 200 | 11.755702611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:19 | 200 |  8.882137747s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:23 | 200 |  6.867241744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:29 | 200 |  9.942896043s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:32 | 200 |  9.833352644s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:36 | 200 |  6.994813653s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:40 | 200 |  7.393968777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:43 | 200 |  6.523019942s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:46 | 200 | 10.209454874s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:50 | 200 |  7.701554786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:46:56 | 200 | 13.589419228s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:34 | 200 |  9.131498406s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:40 | 200 | 17.071895069s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:49 | 200 | 23.753510714s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:53 | 200 | 28.718720994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:56 | 200 | 31.383093535s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:47:59 | 200 | 24.505062354s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:04 | 200 | 23.478420164s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:09 | 200 | 19.865725511s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:13 | 200 | 19.522227969s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:17 | 200 | 20.429907047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:22 | 200 | 22.602926903s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:26 | 200 | 21.398597835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:33 | 200 | 23.269450648s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:36 | 200 |  23.00946865s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:41 | 200 | 24.222395026s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:48 | 200 | 25.244739966s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:48:54 | 200 | 27.968362168s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:00 | 200 | 26.400394024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:04 | 200 |  27.29365088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:09 | 200 |  27.04196861s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:13 | 200 | 24.658173678s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:19 | 200 | 24.357343731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:24 | 200 |  23.78793266s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:29 | 200 | 24.494328157s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:35 | 200 | 25.728499564s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:39 | 200 | 25.507191636s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:44 | 200 | 24.900409288s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:47 | 200 | 23.080489133s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:51 | 200 | 21.700481386s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:54 | 200 | 18.799033738s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:49:57 | 200 | 17.467381624s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:00 | 200 |  16.45787036s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:05 | 200 | 17.702120213s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:10 | 200 |  18.97257572s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:13 | 200 | 18.659486486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:19 | 200 | 21.335843709s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:25 | 200 |  23.86850024s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:32 | 200 | 26.111837238s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:37 | 200 | 27.426462682s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:39 | 200 | 25.641670765s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:44 | 200 | 24.945085856s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:50 | 200 |  24.41625437s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:54 | 200 | 21.873367785s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:50:57 | 200 | 19.174911835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:00 | 200 | 20.426036494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:04 | 200 | 18.648820061s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:08 | 200 | 17.984375211s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:12 | 200 | 17.708981635s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:17 | 200 | 18.824556833s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:20 | 200 | 19.809618728s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:23 | 200 | 19.291725656s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:27 | 200 | 19.125124329s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:34 | 200 | 22.008461931s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:38 | 200 | 20.747574276s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:42 | 200 | 21.854580301s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:54 | 200 | 29.599735486s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:51:59 | 200 | 31.169419806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:04 | 200 | 29.644950896s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:08 | 200 | 29.428578052s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:12 | 200 | 29.493954348s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:19 | 200 | 24.188479919s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:26 | 200 |  26.64405467s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:28 | 200 | 22.888949167s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:33 | 200 | 24.364064683s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:37 | 200 | 24.716903081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:42 | 200 | 22.655896263s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:49 | 200 | 22.725126095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:54 | 200 | 25.540504797s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:52:59 | 200 | 26.306854813s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:04 | 200 | 26.158726621s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:08 | 200 | 26.402945555s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:14 | 200 | 25.030955055s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:19 | 200 | 24.282135598s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:22 | 200 | 22.852970285s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:25 | 200 | 20.412470479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:31 | 200 | 22.779163012s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:53:36 | 200 | 21.340615554s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:54:08.047-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 01:54:08 | 500 | 45.051966657s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:54:11.194-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 01:54:11 | 500 | 45.059127917s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:54:17.640-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 01:54:17 | 500 | 45.049443851s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T01:54:21.815-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 01:54:21 | 500 | 45.087728422s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:25 | 200 |          1m5s |       127.0.0.1 | POST     "/api/generate"
Error #01: write tcp 127.0.0.1:11479->127.0.0.1:50274: write: broken pipe
[GIN] 2025/11/10 - 01:54:28 | 200 |  23.02385273s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:32 | 200 | 24.104644106s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:38 | 200 |  27.20446705s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:43 | 200 | 25.606546494s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:48 | 200 | 25.745045424s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:52 | 200 | 24.125428044s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:54:57 | 200 | 24.052972282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:00 | 200 | 21.266851812s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:04 | 200 | 20.650763735s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:07 | 200 | 18.806440659s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:12 | 200 | 18.577381128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:14 | 200 | 17.119802056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:17 | 200 | 16.211483515s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:19 | 200 | 14.184904059s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:22 | 200 | 14.621654864s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:26 | 200 | 14.643094375s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:33 | 200 |  18.36493473s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:36 | 200 | 19.723305415s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:39 | 200 | 20.468284585s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:44 | 200 | 20.395386003s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:51 | 200 | 20.749108817s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:54 | 200 | 17.867259629s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:55:59 | 200 | 21.942748466s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:03 | 200 | 23.511072825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:07 | 200 | 21.609685882s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:10 | 200 | 19.103252204s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:15 | 200 | 20.252733598s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:21 | 200 | 21.417353441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:24 | 200 | 20.346318039s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:30 | 200 | 22.282271026s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:36 | 200 | 25.854218999s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:40 | 200 | 24.644683379s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:44 | 200 | 22.249958383s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:48 | 200 | 23.871956775s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:56:55 | 200 | 25.051894687s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:02 | 200 | 25.494744843s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:06 | 200 | 25.595493924s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:10 | 200 | 25.949701302s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:14 | 200 | 24.792781589s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:17 | 200 | 21.094007241s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:22 | 200 | 19.388777993s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:27 | 200 | 20.123795295s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:31 | 200 | 20.205528664s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:37 | 200 | 22.669732336s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:43 | 200 |  25.14754744s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:48 | 200 | 25.945243955s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:54 | 200 | 26.217778523s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:57:58 | 200 | 26.843996287s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:02 | 200 | 24.502452433s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:08 | 200 | 23.736809172s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:13 | 200 | 24.478707657s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:19 | 200 | 24.269752711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:22 | 200 | 24.088778199s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:26 | 200 | 23.449081446s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:30 | 200 | 21.545227758s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:33 | 200 | 19.400418529s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:37 | 200 | 18.175243135s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:43 | 200 |  20.69955431s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:49 | 200 |  23.50135786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:53 | 200 |  23.36289176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:58:59 | 200 | 25.503315105s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:05 | 200 | 27.298870855s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:08 | 200 |  24.35575461s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:12 | 200 | 22.222497821s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:14 | 200 | 19.872070952s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:15 | 200 | 15.808074327s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:20 | 200 | 14.944945018s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:23 | 200 | 11.355531162s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:27 | 200 | 15.412014318s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:34 | 200 | 18.312229128s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:38 | 200 | 19.678605081s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:44 | 200 | 21.061664444s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:49 | 200 | 24.600447812s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:53 | 200 | 21.221691488s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 01:59:57 | 200 | 19.787004825s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:02 | 200 | 18.058408736s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:06 | 200 | 15.171222941s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:09 | 200 | 12.873100088s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:12 | 200 |   8.47146939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:17 | 200 |  6.779946899s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:22 | 200 |  7.718947777s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:32 | 200 | 13.851303197s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:35 | 200 | 13.691824236s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:41 | 200 | 13.922999776s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:44 | 200 | 10.784555063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:49 | 200 |  9.083691587s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:55 | 200 |  10.80948449s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:00:59 | 200 |  12.91748544s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:04 | 200 | 12.043895978s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:08 | 200 |  9.617666769s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:14 | 200 | 14.578531261s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:19 | 200 | 14.123447047s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:27 | 200 | 16.359537915s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:34 | 200 | 19.458905093s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:39 | 200 | 21.961586731s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:44 | 200 |  24.69411514s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:46 | 200 | 21.211232042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:50 | 200 | 19.369573945s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:52 | 200 | 15.112007943s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:01:54 | 200 |  8.130501848s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:24 | 200 |  7.544826331s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:28 | 200 |  9.591269138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:34 | 200 | 15.797840434s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:40 | 200 |  20.44399699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:45 | 200 | 26.121975083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:51 | 200 | 26.638028157s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:02:55 | 200 | 25.860872391s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:01 | 200 | 25.605040565s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:06 | 200 | 25.379181192s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:09 | 200 | 23.374860806s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:14 | 200 | 22.599838319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:19 | 200 | 24.608645281s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:25 | 200 | 23.548535063s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:28 | 200 | 21.606635251s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:32 | 200 | 22.896443267s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:35 | 200 | 20.560738376s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:38 | 200 | 18.052718135s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:43 | 200 | 17.864117332s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:47 | 200 |  18.46429297s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:53 | 200 | 20.666495786s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:03:58 | 200 | 23.044843148s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:03 | 200 | 24.453531552s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:08 | 200 | 24.248323138s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:12 | 200 | 24.966306482s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:16 | 200 |  22.87539319s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:20 | 200 | 21.422826953s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:24 | 200 | 21.174588309s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:28 | 200 | 19.959693441s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:34 | 200 | 20.847645998s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:40 | 200 | 23.480403218s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:47 | 200 |  26.97139711s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:52 | 200 | 27.921066268s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:04:56 | 200 | 27.502003479s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:00 | 200 | 25.856889026s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:06 | 200 | 25.370046695s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:10 | 200 | 22.447373233s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:13 | 200 | 20.543297085s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:20 | 200 | 23.709371802s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:22 | 200 | 21.713867835s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:26 | 200 | 19.039750625s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:30 | 200 | 19.183987939s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:34 | 200 |  20.51837614s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:43 | 200 | 22.656210071s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:49 | 200 | 26.748556814s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:05:55 | 200 | 28.473795649s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:00 | 200 | 29.793896617s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:06 | 200 | 30.851094095s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:11 | 200 | 27.352514842s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:17 | 200 | 27.214599427s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:22 | 200 | 26.980543262s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:27 | 200 | 26.091072202s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:32 | 200 | 25.565827448s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:36 | 200 | 24.482167053s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:41 | 200 | 23.374167707s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:44 | 200 | 21.089596728s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:47 | 200 | 18.477523361s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:50 | 200 | 18.066864768s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:06:55 | 200 | 19.046825525s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.953-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 |   3.53451002s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:06:59.994-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:06:59 | 500 | 15.341012044s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.020-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:07:00 | 500 | 12.260256813s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:07:00.027-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:07:00 | 500 |  8.686424766s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:07:00 | 200 | 19.299939158s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:08:37.408-07:00 level=INFO source=routes.go:1524 msg="server config" env="map[CUDA_VISIBLE_DEVICES:2 GGML_VK_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_CONTEXT_LENGTH:4096 OLLAMA_DEBUG:INFO OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11479 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/dan/.ollama-11479 OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://* vscode-file://*] OLLAMA_REMOTES:[ollama.com] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-11-10T02:08:37.409-07:00 level=INFO source=images.go:522 msg="total blobs: 11"
time=2025-11-10T02:08:37.409-07:00 level=INFO source=images.go:529 msg="total unused blobs removed: 0"
time=2025-11-10T02:08:37.409-07:00 level=INFO source=routes.go:1577 msg="Listening on [::]:11479 (version 0.12.9)"
time=2025-11-10T02:08:37.409-07:00 level=INFO source=runner.go:76 msg="discovering available GPUs..."
time=2025-11-10T02:08:37.410-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 41445"
time=2025-11-10T02:08:37.609-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 33895"
time=2025-11-10T02:08:38.067-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 36629"
time=2025-11-10T02:08:38.067-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43917"
time=2025-11-10T02:08:38.297-07:00 level=INFO source=types.go:42 msg="inference compute" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f filtered_id="" library=CUDA compute=8.9 name=CUDA0 description="NVIDIA GeForce RTX 4090" libdirs=ollama,cuda_v13 driver=13.0 pci_id=0000:0d:00.0 type=discrete total="24.0 GiB" available="23.5 GiB"
[GIN] 2025/11/10 - 02:08:39 | 200 |       47.82µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/11/10 - 02:08:39 | 200 |   49.634538ms |       127.0.0.1 | POST     "/api/show"
[GIN] 2025/11/10 - 02:08:49 | 200 |     585.579µs |       127.0.0.1 | GET      "/api/tags"
time=2025-11-10T02:09:30.349-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --port 43321"
time=2025-11-10T02:09:30.766-07:00 level=INFO source=server.go:215 msg="enabling flash attention"
time=2025-11-10T02:09:30.766-07:00 level=INFO source=server.go:400 msg="starting runner" cmd="/usr/local/bin/ollama runner --ollama-engine --model /home/dan/.ollama-11479/blobs/sha256-735af2139dc652bf01112746474883d79a52fa1c19038265d363e3d42556f7a2 --port 43413"
time=2025-11-10T02:09:30.768-07:00 level=INFO source=server.go:653 msg="loading model" "model layers"=19 requested=-1
time=2025-11-10T02:09:30.768-07:00 level=INFO source=server.go:658 msg="system memory" total="123.6 GiB" free="95.2 GiB" free_swap="0 B"
time=2025-11-10T02:09:30.768-07:00 level=INFO source=server.go:665 msg="gpu memory" id=GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f library=CUDA available="17.8 GiB" free="18.2 GiB" minimum="457.0 MiB" overhead="0 B"
time=2025-11-10T02:09:30.784-07:00 level=INFO source=runner.go:1349 msg="starting ollama engine"
time=2025-11-10T02:09:30.784-07:00 level=INFO source=runner.go:1384 msg="Server listening on 127.0.0.1:43413"
time=2025-11-10T02:09:30.792-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:fit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:30.846-07:00 level=INFO source=ggml.go:136 msg="" architecture=gemma3 file_type=Q8_0 name="" description="" num_tensors=236 num_key_values=37
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes, ID: GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v13/libggml-cuda.so
time=2025-11-10T02:09:31.160-07:00 level=INFO source=ggml.go:104 msg=system CPU.0.SSE3=1 CPU.0.SSSE3=1 CPU.0.AVX=1 CPU.0.AVX2=1 CPU.0.F16C=1 CPU.0.FMA=1 CPU.0.BMI2=1 CPU.0.AVX512=1 CPU.0.AVX512_VBMI=1 CPU.0.AVX512_VNNI=1 CPU.0.LLAMAFILE=1 CPU.1.LLAMAFILE=1 CUDA.0.ARCHS=750,800,860,870,890,900,1000,1030,1100,1200,1210 CUDA.0.USE_GRAPHS=1 CUDA.0.PEER_MAX_BATCH_SIZE=128 compiler=cgo(gcc)
time=2025-11-10T02:09:32.362-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:alloc LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:32.652-07:00 level=INFO source=runner.go:1222 msg=load request="{Operation:commit LoraPath:[] Parallel:1 BatchSize:512 FlashAttention:true KvSize:4096 KvCacheType: NumThreads:8 GPULayers:19[ID:GPU-6cab3899-f78d-7304-2676-e0fbf97a3f8f Layers:19(0..18)] MultiUserCache:false ProjectorPath: MainGPU:0 UseMmap:false}"
time=2025-11-10T02:09:32.653-07:00 level=INFO source=ggml.go:482 msg="offloading 18 repeating layers to GPU"
time=2025-11-10T02:09:32.655-07:00 level=INFO source=device.go:212 msg="model weights" device=CUDA0 size="271.9 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=device.go:217 msg="model weights" device=CPU size="170.0 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=device.go:223 msg="kv cache" device=CUDA0 size="27.0 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=device.go:234 msg="compute graph" device=CUDA0 size="155.5 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=device.go:239 msg="compute graph" device=CPU size="1.2 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=device.go:244 msg="total memory" size="625.6 MiB"
time=2025-11-10T02:09:32.658-07:00 level=INFO source=sched.go:493 msg="loaded runners" count=1
time=2025-11-10T02:09:32.658-07:00 level=INFO source=server.go:1251 msg="waiting for llama runner to start responding"
time=2025-11-10T02:09:32.653-07:00 level=INFO source=ggml.go:489 msg="offloading output layer to GPU"
time=2025-11-10T02:09:32.653-07:00 level=INFO source=ggml.go:494 msg="offloaded 19/19 layers to GPU"
time=2025-11-10T02:09:32.660-07:00 level=INFO source=server.go:1285 msg="waiting for server to become available" status="llm server loading model"
time=2025-11-10T02:09:33.164-07:00 level=INFO source=server.go:1289 msg="llama runner started in 2.40 seconds"
[GIN] 2025/11/10 - 02:09:33 | 200 |  3.650765176s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:34 | 200 |  883.773483ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:35 | 200 |   1.03027283s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:38 | 200 |  1.148438727s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:39 | 200 |  983.494824ms |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:09:40 | 200 |    1.3914611s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:16.971-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7587 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:17 | 200 |  2.224814423s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:18.862-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7684 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:19 | 200 |  1.686571413s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:20 | 200 |  1.698247194s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:10:21.252-07:00 level=WARN source=runner.go:171 msg="truncating input prompt" limit=4096 prompt=7682 keep=4 new=4096
[GIN] 2025/11/10 - 02:10:22 | 200 |  2.591372561s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:23 | 200 |  2.638047484s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:10:24 | 200 |  1.224469275s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:17 | 200 |  2.084451951s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:21 | 200 |   3.48847042s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:39 | 200 | 21.008758056s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:40 | 200 | 19.154908611s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:41 | 200 | 17.677455564s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:42 | 200 |  8.548878606s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:45 | 200 |  6.274734264s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:45 | 200 |   4.62630613s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:46 | 200 |  4.066742417s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:11:48 | 200 |  3.226609601s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:12 | 200 |  1.392037905s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:24 | 200 | 11.283759801s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:32 | 200 | 17.899003331s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:36 | 200 | 21.710567546s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:41 | 200 | 26.408331356s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:44 | 200 |  28.39489282s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:50 | 200 | 32.698737083s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:12:54 | 200 | 38.188137693s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:00 | 200 | 43.901459463s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:05.953-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:05 | 500 | 45.011793559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:06 | 200 | 46.015024605s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:07 | 500 | 45.983406368s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:08 | 500 | 45.039723203s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:09.399-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:10 | 500 | 45.596718483s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:10.769-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:10 | 500 | 45.043862704s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:11 | 500 |  45.09403378s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:11.237-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:11.763-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:13:13.128-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:15 | 500 | 45.046196465s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:17 | 500 | 45.053583686s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:13:17.741-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:13:21 | 200 | 43.815286344s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:24 | 200 | 42.436090917s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:27 | 200 | 42.332170994s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:32 | 200 |  41.53537357s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:35 | 200 | 40.074025152s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:39 | 200 | 38.673438512s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:44 | 200 |  38.27051748s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:47 | 200 | 41.237104237s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:51 | 200 | 43.875660559s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:13:56 | 200 | 34.768926057s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:01 | 200 | 36.710162495s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:03 | 200 | 35.807695699s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:07 | 200 | 34.739425827s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:11 | 200 | 35.208440823s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:16 | 200 | 36.299650414s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:20 | 200 | 36.248546679s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.449-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 31.866180294s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.450-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  29.63415108s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.453-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  33.33875317s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 | 13.746672947s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.456-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.407697388s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.456-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.831108974s |       127.0.0.1 | POST     "/api/generate"
[GIN] 2025/11/10 - 02:14:21 | 500 |  5.359834118s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 32.827788639s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 33.136510947s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 24.480864346s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 28.932396525s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 17.404656139s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 |  9.977513287s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:21.460-07:00 level=INFO source=server.go:1433 msg="aborting completion request due to client closing the connection"
[GIN] 2025/11/10 - 02:14:21 | 500 | 19.969847148s |       127.0.0.1 | POST     "/api/generate"
time=2025-11-10T02:14:22.557-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
time=2025-11-10T02:14:22.655-07:00 level=INFO source=runner.go:868 msg="aborting completion request due to client closing the connection"
